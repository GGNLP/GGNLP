{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost\n",
    "\n",
    "Use Xgboost as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_files = [\n",
    "    'Abhishek_features',\n",
    "    'magic_feature',\n",
    "    'magic_feature_v1',\n",
    "#     'lystdo_correctwords_lstm',\n",
    "    'Howard_feature',\n",
    "#     'lystdo_1234_loss017',\n",
    "    'HubertLin_features_raw',\n",
    "    'HubertLin_features_simple_tokenizer',\n",
    "    'HubertLin_features_word_corrected',\n",
    "    'pagerank',\n",
    "    'word_match_share',\n",
    "    'fasttext_distance',\n",
    "#     'modified_count',\n",
    "    'magic_v25_qid',\n",
    "    'k_scrore',\n",
    "    'pos_dist',\n",
    "    'dep_dist',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "take_feature = {\n",
    "    'lystdo_origin_loss018': [\n",
    "        'lystdo_original_c',\n",
    "#         'lystdo_original_p',\n",
    "#         'lystdo_original_k',\n",
    "#         'lystdo_original_b'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def read_csv(file_name, base=''):\n",
    "    df = pd.read_csv(base + file_name + '.csv')\n",
    "    if file_name in take_feature:\n",
    "        df = df[take_feature[file_name]]\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frames = [read_csv(file, base='features_from_model/train/') for file in feature_files]\n",
    "df_train = pd.concat(data_frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('../dataset/quora-question-pairs/train.csv')['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get original val set idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# val_pos_pairs = pickle.load(open('../dataset/augmented/validation_positive_qid_pairs.pkl','rb'))\n",
    "# val_neg_pairs = pickle.load(open('../dataset/augmented/validation_negative_qid_pairs.pkl','rb'))\n",
    "\n",
    "# df = pd.read_csv('../dataset/quora-question-pairs/train.csv')\n",
    "\n",
    "# def get_idx(qid_pair, label):\n",
    "#     res = df[np.bitwise_and(df['qid1']==qid_pair[0],df['qid2']==qid_pair[1])]\n",
    "    \n",
    "#     # since we do augmentation in val set also, there might be some samples not exist in original train set\n",
    "#     if len(res)==0:\n",
    "#         return np.nan\n",
    "#     elif res['is_duplicate'].tolist()[0]!=label:\n",
    "#         return np.nan\n",
    "#     return res.index[0]\n",
    "\n",
    "# pos_idxes = np.unique([get_idx(pair, 1) for pair in val_pos_pairs])\n",
    "# neg_idxes = np.unique([get_idx(pair, 0) for pair in val_neg_pairs])\n",
    "\n",
    "# # # workaround\n",
    "# pos_idxes = [idx for idx in pos_idxes if not np.isnan(idx)]\n",
    "# neg_idxes = [idx for idx in neg_idxes if not np.isnan(idx)]\n",
    "\n",
    "# all_idxes = np.concatenate([pos_idxes,neg_idxes])\n",
    "\n",
    "# del df\n",
    "# del val_pos_pairs\n",
    "# del val_neg_pairs\n",
    "\n",
    "# val_idxes = np.concatenate([pos_idxes, neg_idxes]).astype('int')\n",
    "# y_val = np.concatenate([np.ones(len(pos_idxes)),np.zeros(len(neg_idxes))]).astype('int')\n",
    "\n",
    "# pickle.dump([val_idxes, y_val], open('./val/val_data.pkl','wb'))\n",
    "# pickle.dump(all_idxes, open('./val/val_idxes.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_idxes, y_val = pickle.load(open('./val/val_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inf_nan_to_zero(arr):\n",
    "    nan = np.isnan(arr)\n",
    "    inf = np.isinf(arr)\n",
    "    arr[nan] = 0\n",
    "    arr[inf] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_val = inf_nan_to_zero(df_train.ix[val_idxes])\n",
    "df_train = inf_nan_to_zero(df_train.drop(df_train.index[val_idxes]))\n",
    "y_train = y_train.drop(y_train.index[val_idxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the class weights to simulate the class disribution of testing samples\n",
    "#\n",
    "# These numbers and formula is got from here: \n",
    "#     https://www.kaggle.com/lystdo/quora-question-pairs/lstm-with-word2vec-embeddings#175198\n",
    "\n",
    "def get_weights(gt):\n",
    "    test_set_pos_label_ratio = 0.1746\n",
    "    training_pos_ratio = sum(gt)/len(gt)\n",
    "\n",
    "    same_question_ratio = 0 # a training hyper paramter to add testcases like [A,A] \n",
    "    random_negative_samples_ratio = 0\n",
    "\n",
    "    weights = {\n",
    "        0: (1-test_set_pos_label_ratio) / (1-training_pos_ratio),\n",
    "        1: test_set_pos_label_ratio/training_pos_ratio\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "weights = get_weights(y_train)\n",
    "y_train_weights = np.array([weights[0] if y==0 else weights[1] for y in y_train])\n",
    "\n",
    "weights = get_weights(y_val)\n",
    "y_val_weights = np.array([weights[0] if y==0 else weights[1] for y in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ss = StandardScaler()\n",
    "# ss.fit(np.vstack([df_train,df_test]))\n",
    "\n",
    "# df_train = ss.transform(df_train)\n",
    "# df_val = ss.transform(df_val)\n",
    "# df_test = ss.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.454963\tvalid-logloss:0.455608\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-logloss:0.257184\tvalid-logloss:0.26828\n",
      "[100]\ttrain-logloss:0.207992\tvalid-logloss:0.221891\n",
      "[150]\ttrain-logloss:0.190306\tvalid-logloss:0.205805\n",
      "[200]\ttrain-logloss:0.182328\tvalid-logloss:0.198787\n",
      "[250]\ttrain-logloss:0.177601\tvalid-logloss:0.194956\n",
      "[300]\ttrain-logloss:0.173787\tvalid-logloss:0.192064\n",
      "[350]\ttrain-logloss:0.170618\tvalid-logloss:0.189909\n",
      "[400]\ttrain-logloss:0.167894\tvalid-logloss:0.188351\n",
      "[450]\ttrain-logloss:0.165501\tvalid-logloss:0.186982\n",
      "[500]\ttrain-logloss:0.163567\tvalid-logloss:0.186041\n",
      "[550]\ttrain-logloss:0.161795\tvalid-logloss:0.185331\n",
      "[600]\ttrain-logloss:0.16013\tvalid-logloss:0.184651\n",
      "[650]\ttrain-logloss:0.158537\tvalid-logloss:0.184088\n",
      "[700]\ttrain-logloss:0.157103\tvalid-logloss:0.183528\n",
      "[750]\ttrain-logloss:0.155634\tvalid-logloss:0.183069\n",
      "[800]\ttrain-logloss:0.154299\tvalid-logloss:0.182629\n",
      "[850]\ttrain-logloss:0.153039\tvalid-logloss:0.182328\n",
      "[900]\ttrain-logloss:0.151794\tvalid-logloss:0.18192\n",
      "[950]\ttrain-logloss:0.1506\tvalid-logloss:0.181511\n",
      "[1000]\ttrain-logloss:0.149491\tvalid-logloss:0.181214\n",
      "[1050]\ttrain-logloss:0.148259\tvalid-logloss:0.180981\n",
      "[1100]\ttrain-logloss:0.147013\tvalid-logloss:0.180761\n",
      "[1150]\ttrain-logloss:0.145754\tvalid-logloss:0.180461\n",
      "[1200]\ttrain-logloss:0.144592\tvalid-logloss:0.1803\n",
      "[1250]\ttrain-logloss:0.143535\tvalid-logloss:0.18013\n",
      "[1300]\ttrain-logloss:0.142458\tvalid-logloss:0.179945\n",
      "[1350]\ttrain-logloss:0.14136\tvalid-logloss:0.179753\n",
      "[1400]\ttrain-logloss:0.140351\tvalid-logloss:0.179517\n",
      "[1450]\ttrain-logloss:0.139334\tvalid-logloss:0.179389\n",
      "[1500]\ttrain-logloss:0.138361\tvalid-logloss:0.179277\n",
      "[1550]\ttrain-logloss:0.137413\tvalid-logloss:0.17922\n",
      "[1600]\ttrain-logloss:0.136473\tvalid-logloss:0.179158\n",
      "[1650]\ttrain-logloss:0.135468\tvalid-logloss:0.179056\n",
      "[1700]\ttrain-logloss:0.134503\tvalid-logloss:0.178996\n",
      "[1750]\ttrain-logloss:0.133564\tvalid-logloss:0.178876\n",
      "[1800]\ttrain-logloss:0.13269\tvalid-logloss:0.178773\n",
      "[1850]\ttrain-logloss:0.131766\tvalid-logloss:0.178656\n",
      "[1900]\ttrain-logloss:0.130844\tvalid-logloss:0.178582\n",
      "[1950]\ttrain-logloss:0.129982\tvalid-logloss:0.178581\n",
      "[2000]\ttrain-logloss:0.129086\tvalid-logloss:0.178564\n",
      "[2050]\ttrain-logloss:0.128169\tvalid-logloss:0.178481\n",
      "[2100]\ttrain-logloss:0.127339\tvalid-logloss:0.178384\n",
      "[2150]\ttrain-logloss:0.12648\tvalid-logloss:0.178327\n",
      "[2200]\ttrain-logloss:0.125609\tvalid-logloss:0.178248\n",
      "[2250]\ttrain-logloss:0.124734\tvalid-logloss:0.178144\n",
      "[2300]\ttrain-logloss:0.123898\tvalid-logloss:0.178105\n",
      "[2350]\ttrain-logloss:0.123054\tvalid-logloss:0.178041\n",
      "[2400]\ttrain-logloss:0.122197\tvalid-logloss:0.17798\n",
      "[2450]\ttrain-logloss:0.121408\tvalid-logloss:0.177953\n",
      "[2500]\ttrain-logloss:0.120614\tvalid-logloss:0.177889\n",
      "[2550]\ttrain-logloss:0.119786\tvalid-logloss:0.177823\n",
      "[2600]\ttrain-logloss:0.11903\tvalid-logloss:0.177834\n",
      "[2650]\ttrain-logloss:0.118273\tvalid-logloss:0.177787\n",
      "[2700]\ttrain-logloss:0.117565\tvalid-logloss:0.177826\n",
      "[2750]\ttrain-logloss:0.116813\tvalid-logloss:0.177771\n",
      "Stopping. Best iteration:\n",
      "[2660]\ttrain-logloss:0.118137\tvalid-logloss:0.177749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    " \n",
    "# params['eta'] = 0.1\n",
    "# params['max_depth'] = 5\n",
    "# params['subsample'] = 0.7\n",
    "# params['colsample_bytree'] = 0.5\n",
    "# params['colsample_bylevel'] = 0.5\n",
    "# params['min_child_weight'] = 10\n",
    "\n",
    "# params['base_score'] = 0.1\n",
    "\n",
    "\n",
    "d_train = xgb.DMatrix(df_train, label=y_train, weight=y_train_weights)\n",
    "d_valid = xgb.DMatrix(df_val, label=y_val, weight=y_val_weights)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 100000, watchlist, early_stopping_rounds=100, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(df_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in d_test.feature_names:\n",
    "    if c not in list(df_train.columns):\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_test.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_frames = [read_csv(file, base='features_from_model/test/') for file in feature_files]\n",
    "df_test = pd.concat(test_frames, axis=1)\n",
    "df_test = inf_nan_to_zero(df_test)\n",
    "del test_frames\n",
    "\n",
    "d_test = xgb.DMatrix(df_test)\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_name = 'xgboost_all_features.csv'\n",
    "\n",
    "# # process train\n",
    "\n",
    "# d_test = xgb.DMatrix(df_train)\n",
    "# pred = bst.predict(d_test)\n",
    "\n",
    "# sub = pd.DataFrame()\n",
    "# sub['is_duplicate'] = pred\n",
    "# sub.to_csv('./model_predictions/train/' + output_name, index=False)\n",
    "\n",
    "# process test\n",
    "\n",
    "pred = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['is_duplicate'] = pred\n",
    "sub.to_csv('./model_predictions/train/' + output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_name = 'prediction/xgb_loss0141.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(df_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = np.arange(2345796)\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv(output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
