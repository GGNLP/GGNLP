{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some special features\n",
    "\n",
    "Just tried some special features, but not working very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpaCy `en_core_web_md` corpus...\n"
     ]
    }
   ],
   "source": [
    "# Import spacy corpus, glove embeddings.\n",
    "import spacy\n",
    "import textacy\n",
    "\n",
    "print('Loading SpaCy `en_core_web_md` corpus...')\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1_train_corrected, q2_train_corrected, y_train = pickle.load(open('./lystdo_kernel/train_text_processed.pkl','rb'))\n",
    "q1_test_corrected, q2_test_corrected, y_test = pickle.load(open('./lystdo_kernel/test_text_processed.pkl','rb'))\n",
    "\n",
    "q1_train_simple, q2_train_simple, y_train = pickle.load(open('./lystdo_kernel/train_text_without_process.pkl','rb'))\n",
    "q1_test_simple, q2_test_simple, y_test = pickle.load(open('./lystdo_kernel/test_text_without_process.pkl','rb'))\n",
    "\n",
    "df_train = pd.read_csv('../dataset/quora-question-pairs/train.csv')\n",
    "df_test = pd.read_csv('../dataset/quora-question-pairs/test.csv')\n",
    "q1_train_original = df_train['question1']\n",
    "q2_train_original = df_train['question2']\n",
    "q1_test_original = df_test['question1']\n",
    "q2_test_original = df_test['question2']\n",
    "\n",
    "del df_train\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions\n",
    "'''\n",
    "\n",
    "def get_function_name(f):\n",
    "    return str(f).split(' ')[1]\n",
    "\n",
    "def get_regex_number(regex, text):\n",
    "    m = re.findall(regex, text)\n",
    "    if m==None:\n",
    "        return 0\n",
    "    return len(m)\n",
    "\n",
    "'''\n",
    "Features\n",
    "'''\n",
    "\n",
    "def get_non_ascii(text):\n",
    "    return get_regex_number('[^\\x00-\\x7F]', text)\n",
    "    \n",
    "def get_number(text):\n",
    "    return get_regex_number('[0-9]+[\\.\\,]*[0-9]*', text)\n",
    "    \n",
    "def get_puncts(text):\n",
    "    return get_regex_number('[\\!\\?！？\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\\\(\\)\\[\\]\\{\\}\\<\\>\\'\\\"’`“…é\\$\\%\\&]', text)\n",
    "\n",
    "def get_brackets(text):\n",
    "    return get_regex_number('[\\(\\)\\[\\]\\{\\}\\<\\>\\'\\\"]', text)\n",
    "\n",
    "def get_dashes(text):\n",
    "    return get_regex_number('\\-', text)\n",
    "\n",
    "def get_dots(text):\n",
    "    return get_regex_number('\\.', text)\n",
    "\n",
    "def get_end_of_sent(text):\n",
    "    return get_regex_number('[\\.\\!\\?！？]', text)\n",
    "\n",
    "def get_commas(text):\n",
    "    return get_regex_number('\\,', text)\n",
    "\n",
    "def get_spaces(text):\n",
    "    return get_regex_number('[\\s\\t\\n]+', text)\n",
    "\n",
    "def get_entity_count(text):\n",
    "    doc = nlp(text)\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.ent_type_!='':\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def get_OOB(text):\n",
    "    doc = nlp(text)\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if not token.has_vector:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen features (takes a little long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "features_before_clean = [\n",
    "    get_non_ascii,\n",
    "    get_number,\n",
    "    get_puncts,\n",
    "    get_dashes,\n",
    "    get_dots,\n",
    "    get_end_of_sent,\n",
    "    get_commas,\n",
    "    get_spaces,\n",
    "    get_entity_count,\n",
    "    get_OOB,\n",
    "    get_brackets,\n",
    "]\n",
    "\n",
    "features_after_clean = [\n",
    "    get_non_ascii,\n",
    "    get_number,\n",
    "    get_puncts,\n",
    "    get_dashes,\n",
    "    get_dots,\n",
    "    get_end_of_sent,\n",
    "    get_commas,\n",
    "    get_entity_count,\n",
    "    get_OOB,\n",
    "    get_brackets,\n",
    "]\n",
    "\n",
    "def extract_features(texts, functions, run_name, q):\n",
    "    \n",
    "    fields = [run_name+'_'+q+'_'+get_function_name(f) for f in functions]\n",
    "    \n",
    "    sample_len = len(texts)\n",
    "    print('At '+run_name, sample_len, ' samples')\n",
    "    \n",
    "    ret = []\n",
    "    for i,text in enumerate(texts):\n",
    "        if type(text)!=str:\n",
    "            text = ''\n",
    "        features = [func(text) for func in functions]\n",
    "        ret.append(features)\n",
    "        \n",
    "        if i%100000==0:\n",
    "            print(i,'/',sample_len)\n",
    "        \n",
    "    return pd.DataFrame(data=ret, columns=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At raw 2345796  samples\n",
      "0 / 2345796\n",
      "100000 / 2345796\n",
      "200000 / 2345796\n",
      "300000 / 2345796\n",
      "400000 / 2345796\n",
      "500000 / 2345796\n",
      "600000 / 2345796\n",
      "700000 / 2345796\n",
      "800000 / 2345796\n",
      "900000 / 2345796\n",
      "1000000 / 2345796\n",
      "1100000 / 2345796\n",
      "1200000 / 2345796\n",
      "1300000 / 2345796\n",
      "1400000 / 2345796\n",
      "1500000 / 2345796\n",
      "1600000 / 2345796\n",
      "1700000 / 2345796\n",
      "1800000 / 2345796\n",
      "1900000 / 2345796\n",
      "2000000 / 2345796\n",
      "2100000 / 2345796\n",
      "2200000 / 2345796\n",
      "2300000 / 2345796\n",
      "At raw 2345796  samples\n",
      "0 / 2345796\n",
      "100000 / 2345796\n",
      "200000 / 2345796\n",
      "300000 / 2345796\n",
      "400000 / 2345796\n",
      "500000 / 2345796\n",
      "600000 / 2345796\n",
      "700000 / 2345796\n",
      "800000 / 2345796\n",
      "900000 / 2345796\n",
      "1000000 / 2345796\n",
      "1100000 / 2345796\n",
      "1200000 / 2345796\n",
      "1300000 / 2345796\n",
      "1400000 / 2345796\n",
      "1500000 / 2345796\n",
      "1600000 / 2345796\n",
      "1700000 / 2345796\n",
      "1800000 / 2345796\n",
      "1900000 / 2345796\n",
      "2000000 / 2345796\n",
      "2100000 / 2345796\n",
      "2200000 / 2345796\n",
      "2300000 / 2345796\n"
     ]
    }
   ],
   "source": [
    "run_name = 'word_corrected'\n",
    "\n",
    "q1 = extract_features(q1_train_corrected, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_train_corrected, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('./features_from_model/train/HubertLin_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "q1 = extract_features(q1_test_corrected, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_test_corrected, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('./features_from_model/test/HubertLin_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "#######################\n",
    "\n",
    "run_name = 'simple_tokenizer'\n",
    "\n",
    "q1 = extract_features(q1_train_simple, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_train_simple, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('./features_from_model/train/HubertLin_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "q1 = extract_features(q1_test_simple, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_test_simple, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('./features_from_model/test/HubertLin_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "#######################\n",
    "\n",
    "run_name = 'raw'\n",
    "\n",
    "q1 = extract_features(q1_train_original, features_before_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_train_original, features_before_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('./features_from_model/train/HubertLin_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "q1 = extract_features(q1_test_original, features_before_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_test_original, features_before_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('./features_from_model/test/HubertLin_features_'+run_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
