{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN + leak features\n",
    "\n",
    "Since RNN alone can't yield a great result (since this data set as many special features and distributions), we need to geed some of extracted features as leakage into the RNN model. This boosted log-loss from 0.3 to 0.153, almost 50% boost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import RMSprop, Nadam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "set directories and parameters\n",
    "'''\n",
    "EMBEDDING_FILE = '../dataset/glove/glove.6B.300d.txt'\n",
    "TRAIN_DATA_FILE = '../dataset/quora-question-pairs/train.csv'\n",
    "TEST_DATA_FILE = '../dataset/quora-question-pairs/test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process texts in datasets (Cached)\n",
    "\n",
    "<font color='red'>**Following codes are commented since their results are cached**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Processing text dataset')\n",
    "\n",
    "# # The function \"text_to_wordlist\" is from\n",
    "# # https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "# def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "#     # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "#     # Convert words to lower case and split them\n",
    "#     text = text.lower().split()\n",
    "\n",
    "#     # Optionally, remove stop words\n",
    "#     if remove_stopwords:\n",
    "#         stops = set(stopwords.words(\"english\"))\n",
    "#         text = [w for w in text if not w in stops]\n",
    "    \n",
    "#     text = \" \".join(text)\n",
    "\n",
    "#     # Clean the text\n",
    "    \n",
    "#     text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "#     text = re.sub(r\"what's\", \"what is \", text)\n",
    "#     text = re.sub(r\"\\'s\", \" \", text)\n",
    "#     text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "#     text = re.sub(r\"can't\", \"cannot \", text)\n",
    "#     text = re.sub(r\"won't\", \" will not \", text)\n",
    "#     text = re.sub(r\"n't\", \" not \", text)\n",
    "#     text = re.sub(r\"i'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"I'm\", \"i am \", text)\n",
    "#     text = re.sub(r\"\\'re\", \" are \", text)\n",
    "#     text = re.sub(r\"\\'d\", \" would \", text)\n",
    "#     text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "#     text = re.sub(r\",\", \" \", text)\n",
    "#     text = re.sub(r\"\\.\", \" \", text)\n",
    "#     text = re.sub(r\"!\", \" ! \", text)\n",
    "#     text = re.sub(r\"\\/\", \" \", text)\n",
    "#     text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "#     text = re.sub(r\"\\+\", \" + \", text)\n",
    "#     text = re.sub(r\"\\-\", \" - \", text)\n",
    "#     text = re.sub(r\"\\=\", \" = \", text)\n",
    "#     text = re.sub(r\"'\", \" \", text)\n",
    "#     text = re.sub(r\":\", \" : \", text)\n",
    "#     text = re.sub(r\" e g \", \" eg \", text)\n",
    "#     text = re.sub(r\" b g \", \" bg \", text)\n",
    "#     text = re.sub(r\" u s \", \" american \", text)\n",
    "#     text = re.sub(r\"\\0s\", \"0\", text)\n",
    "#     text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "#     text = re.sub(r\"e - mail\", \"email\", text)\n",
    "#     text = re.sub(r\"j k\", \"jk\", text)\n",
    "#     text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "#     # Optionally, shorten words to their stems\n",
    "#     if stem_words:\n",
    "#         text = text.split()\n",
    "#         stemmer = SnowballStemmer('english')\n",
    "#         stemmed_words = [stemmer.stem(word) for word in text]\n",
    "#         text = \" \".join(stemmed_words)\n",
    "    \n",
    "#     # Return a list of words\n",
    "#     return(text)\n",
    "\n",
    "# texts_1 = [] \n",
    "# texts_2 = []\n",
    "# labels = []\n",
    "# with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     header = next(reader)\n",
    "#     for values in reader:\n",
    "#         texts_1.append(text_to_wordlist(values[3]))\n",
    "#         texts_2.append(text_to_wordlist(values[4]))\n",
    "#         labels.append(int(values[5]))\n",
    "# print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "# test_texts_1 = []\n",
    "# test_texts_2 = []\n",
    "# test_ids = []\n",
    "# with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     header = next(reader)\n",
    "#     for values in reader:\n",
    "#         test_texts_1.append(text_to_wordlist(values[1]))\n",
    "#         test_texts_2.append(text_to_wordlist(values[2]))\n",
    "#         test_ids.append(values[0])\n",
    "# print('Found %s texts in test.csv' % len(test_texts_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checker with Glove (Cached)\n",
    "<font color='red'>**Following codes are commented since their results are cached**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Import spacy corpus, glove embeddings.\n",
    "# import spacy\n",
    "# import textacy\n",
    "\n",
    "# print('Loading SpaCy `en_core_web_md` corpus...')\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# def word_prob(word):\n",
    "#     \"Probability of word.\"\n",
    "#     return nlp.vocab[word].prob\n",
    "\n",
    "# def correction(word):\n",
    "#     \"Most probable spelling correction for word.\"\n",
    "#     if nlp.vocab[word].prob > -15.0: \n",
    "#         return word, False\n",
    "#     else:\n",
    "#         return max(candidates(word), key=word_prob), True\n",
    "\n",
    "# def candidates(word):\n",
    "#     \"Generate possible spelling corrections for word.\"\n",
    "#     return set([word] + known([word]) + known(edits1(word)) + known(edits2(word)))\n",
    "\n",
    "# def known(words):\n",
    "#     \"The subset of `words` that appear in the vocabulary.\"\n",
    "#     return [w for w in words if w in nlp.vocab]\n",
    "\n",
    "# def edits1(word):\n",
    "#     \"All edits that are one edit away from `word`.\"\n",
    "#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "#     deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "#     return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# def edits2(word):\n",
    "#     \"All edits that are two edits away from `word`.\"\n",
    "#     return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "# abbr_dict = {\n",
    "#     \"\\'s\":\" is\",\n",
    "#     \"\\'re\":\" are\",\n",
    "\n",
    "#     \"i'm\":\"i am\",\n",
    "#     \"it's\":\"it is\",\n",
    "#     \"\\'ve\":\" have\",\n",
    "\n",
    "#     \"\\'ll\":\" will\",\n",
    "\n",
    "#     \"won't\":\"will not\",\n",
    "#     \"can't\":\"can not\",\n",
    "#     \"\\'t\":\" not\",\n",
    "    \n",
    "# }\n",
    "\n",
    "# def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    \n",
    "#     if type(text)!=str:\n",
    "#         text = ''\n",
    "    \n",
    "#     # Remove spaces.\n",
    "#     text = textacy.preprocess_text(text, lowercase=True)\n",
    "\n",
    "#     # Handle Abbreviation.\n",
    "#     for k, v in abbr_dict.items():\n",
    "#         text = re.sub(r\"{}\".format(k), v, text)\n",
    "\n",
    "#     # Spelling correction\n",
    "#     corrected_words = []\n",
    "#     modified_count = 0\n",
    "#     for doc in nlp(text):\n",
    "#         # Don't correct special words\n",
    "#         if doc.ent_type_ is not '' or doc.pos_ in ['PROPN', 'PUNCT']:\n",
    "#             corrected_words.append(doc.text)\n",
    "#         else:\n",
    "#             corrected, is_modified = correction(doc.text)\n",
    "#             corrected_words.append(corrected)\n",
    "#             if is_modified:\n",
    "#                 modified_count += 1\n",
    "\n",
    "# #     # Re-merge corrected words\n",
    "# #     text = \" \".join(corrected_words)\n",
    "\n",
    "# #     # Text normalization\n",
    "# #     text = re.sub('[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\\\(\\)\\[\\]\\{\\}\\<\\>\\'\\\"]', ' ', text)\n",
    "\n",
    "# #     # Convert to lower case, remove punctuations and further text normalization.\n",
    "# #     text = textacy.preprocess_text(text, lowercase=True, no_punct=True, no_numbers=True, no_currency_symbols=True)\n",
    "\n",
    "#     return text, modified_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# ## Implement multi thread\n",
    "# ########################################\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "# from math import ceil\n",
    "\n",
    "# TOTAL_SPLITS = 64\n",
    "\n",
    "# def batch_process(batch_data, is_train=False):\n",
    "#     res1 = [] \n",
    "#     res2 = [] \n",
    "#     labels = []\n",
    "#     mod_count1 = []\n",
    "#     mod_count2 = []\n",
    "    \n",
    "#     for i,series in batch_data.iterrows():\n",
    "#         q1_modified, q1_modified_count = text_to_wordlist(series['question1'])\n",
    "#         q2_modified, q2_modified_count = text_to_wordlist(series['question2'])\n",
    "#         res1.append(q1_modified)\n",
    "#         res2.append(q2_modified)\n",
    "#         mod_count1.append(q1_modified_count)\n",
    "#         mod_count2.append(q2_modified_count)\n",
    "#         if is_train:\n",
    "#             labels.append(int(series['is_duplicate']))\n",
    "#         else:\n",
    "#             labels.append(int(series['test_id']))\n",
    "    \n",
    "#     return res1, res2, mod_count1, mod_count2, labels\n",
    "    \n",
    "# def join_jobs_result(results, is_train=False):\n",
    "         \n",
    "#     res1 = np.concatenate([results[i][0] for i in range(TOTAL_SPLITS)]).tolist()\n",
    "#     res2 = np.concatenate([results[i][1] for i in range(TOTAL_SPLITS)]).tolist()\n",
    "    \n",
    "#     mod1 = np.concatenate([results[i][2] for i in range(TOTAL_SPLITS)]).tolist()\n",
    "#     mod2 = np.concatenate([results[i][3] for i in range(TOTAL_SPLITS)]).tolist()\n",
    "    \n",
    "#     if is_train:\n",
    "#         labels = np.concatenate([results[i][-1] for i in range(TOTAL_SPLITS)]).tolist()\n",
    "#     else:\n",
    "#         ids = np.concatenate([results[i][-1] for i in range(TOTAL_SPLITS)]).tolist()\n",
    "    \n",
    "#     if is_train:\n",
    "#         return res1, res2, mod1, mod2, labels\n",
    "#     else:\n",
    "#         return res1, res2, mod1, mod2, ids\n",
    "\n",
    "# ########################################\n",
    "# ## process texts in datasets\n",
    "# ########################################\n",
    "# print('Processing text dataset')\n",
    "\n",
    "# # df = pd.read_csv(TRAIN_DATA_FILE, encoding='utf-8')\n",
    "# # SPLIT_SIZE = int(ceil(len(df)/TOTAL_SPLITS))\n",
    "# # res = Parallel(n_jobs=-1, verbose=11)(delayed(batch_process)(df[i*SPLIT_SIZE:(i+1)*SPLIT_SIZE],is_train=True) for i in range(TOTAL_SPLITS))\n",
    "# # texts_1, texts_2, mod1, mod2, labels = join_jobs_result(res, is_train=True)\n",
    "\n",
    "# # print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "# df = pd.read_csv(TEST_DATA_FILE, encoding='utf-8')\n",
    "# SPLIT_SIZE = int(ceil(len(df)/TOTAL_SPLITS))\n",
    "# res = Parallel(n_jobs=-1, verbose=11)(delayed(batch_process)(df[i*SPLIT_SIZE:(i+1)*SPLIT_SIZE],is_train=False) for i in range(TOTAL_SPLITS))\n",
    "# test_texts_1, test_texts_2, test_mod1, test_mod2, test_ids = join_jobs_result(res, is_train=False)\n",
    "\n",
    "# print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "# del df\n",
    "# del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## dump modified count feature to file\n",
    "# def write_modified_features(m1, m2, is_train):\n",
    "#     if is_train:\n",
    "#         path = './features_from_model/train/modified_count.csv'\n",
    "#     else:\n",
    "#         path = './features_from_model/test/modified_count.csv'\n",
    "#     df = pd.DataFrame(np.array([m1, m2]).reshape(-1,2), columns=['mod1','mod2'])\n",
    "#     df.to_csv(path)\n",
    "    \n",
    "# # write_modified_features(mod1, mod2, is_train=True)\n",
    "# write_modified_features(test_mod1, test_mod2, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle.dump([texts_1, texts_2, labels],open('./leaks_cache/train_text_processed.pkl','wb'))\n",
    "# pickle.dump([test_texts_1, test_texts_2, test_ids],open('./leaks_cache/test_text_processed.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed sentences into vectors (cached)\n",
    "<font color='red'>**Following codes are commented since their results are cached**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # use NER (name entity recognition)\n",
    "# is_use_NER = True \n",
    "\n",
    "# correct_words = True\n",
    "# use_fasttext = False\n",
    "\n",
    "# if is_use_NER:\n",
    "#     [texts_1, texts_2, labels] = pickle.load(open('./leaks_cache/train_text_spacy_cleaned.pkl','rb'))\n",
    "#     [test_texts_1, test_texts_2, test_ids] = pickle.load(open('./leaks_cache/test_text_spacy_cleaned.pkl','rb'))\n",
    "# else:\n",
    "#     if not correct_words:\n",
    "#         [texts_1, texts_2, labels] = pickle.load(open('./leaks_cache/train_text_without_process.pkl','rb'))\n",
    "#         [test_texts_1, test_texts_2, test_ids] = pickle.load(open('./leaks_cache/test_text_without_process.pkl','rb'))\n",
    "#     else:\n",
    "#         [texts_1, texts_2, labels] = pickle.load(open('./leaks_cache/train_text_processed.pkl','rb'))\n",
    "#         [test_texts_1, test_texts_2, test_ids] = pickle.load(open('./leaks_cache/test_text_processed.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Tokenizing ... ')\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "# sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "# sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "# test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "# test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "# data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# labels = np.array(labels)\n",
    "# print('Shape of data tensor:', data_1.shape)\n",
    "# print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# ## index word vectors\n",
    "# ########################################\n",
    "# print('Indexing word vectors')\n",
    "\n",
    "# embeddings_index = {}\n",
    "# f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "# count = 0\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %d word vectors of glove.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# ## prepare embeddings\n",
    "# ########################################\n",
    "# print('Preparing embedding matrix')\n",
    "\n",
    "# if use_fasttext:\n",
    "#     fasttext_model = pickle.load(open('../Howard/features/fasttext/fasttext_model.pkl','rb'))\n",
    "\n",
    "# nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "# embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     if use_fasttext:\n",
    "#         embedding_vector = fasttext_model.get(word, np.zeros((300,)))\n",
    "#     else:\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('Saving')\n",
    "# if is_use_NER:\n",
    "#     pickle.dump([data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix,nb_words],open('./leaks_cache/cache_NER.pkl','wb'))\n",
    "# else:\n",
    "#     if correct_words:\n",
    "#         if use_fasttext:\n",
    "#             pickle.dump([data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix,nb_words],open('./leaks_cache/cache_fasttext_text_correction.pkl','wb'))\n",
    "#         else:\n",
    "#             pickle.dump([data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix,nb_words],open('./leaks_cache/cache_text_correction.pkl','wb'))\n",
    "\n",
    "#     else:\n",
    "#         if use_fasttext:\n",
    "#             pickle.dump([data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix,nb_words],open('./leaks_cache/cache_fasttext.pkl','wb'))\n",
    "#         else:\n",
    "#             pickle.dump([data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix,nb_words],open('./leaks_cache/cache.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load cached data\n",
    "\n",
    "<font color='red'>**Choose which version of cache to be used**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# '''no correction , use GloVe'''\n",
    "# data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./leaks_cache/cache.pkl','rb'))\n",
    "# model_name = 'glove_without_word_correction'\n",
    "\n",
    "# '''correction , use GloVe'''\n",
    "# data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./leaks_cache/cache_text_correction.pkl','rb'))\n",
    "# model_name = 'glove_with_word_correction'\n",
    "\n",
    "\n",
    "# '''no correction , use fasttext , 0.1602 -> 0.1607'''\n",
    "# data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./leaks_cache/cache_fasttext.pkl','rb'))\n",
    "# model_name = 'fasttext_without_word_correction'\n",
    "\n",
    "# '''correction , use fasttext , 0.1592 -> 0.1614'''\n",
    "# data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./leaks_cache/cache_fasttext_text_correction.pkl','rb'))\n",
    "# model_name = 'fasttext_with_word_correction'\n",
    "\n",
    "'''no correction , use fasttext , 0.1602 -> 0.1607'''\n",
    "data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./leaks_cache/cache_NER.pkl','rb'))\n",
    "model_name = 'use_NER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen or read features\n",
    "\n",
    "Select which leak feature will be chosen to be used in the training phase. These can be treated as a kind of hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original leaks\n",
      "Loading Abhishek_features\n",
      "Loading magic_feature\n",
      "Loading magic_feature_v1\n",
      "Loading Howard_feature\n",
      "Loading HubertLin_features_raw\n",
      "Loading HubertLin_features_simple_tokenizer\n",
      "Loading HubertLin_features_word_corrected\n",
      "Loading pagerank\n",
      "Loading word_match_share\n",
      "Loading fasttext_distance\n",
      "Loading magic_v25_qid\n",
      "Loading k_scrore\n",
      "Loading pos_dist\n",
      "Loading dep_dist\n",
      "Loading lystdo_leaks\n"
     ]
    }
   ],
   "source": [
    "#### Add extra features to leak\n",
    "\n",
    "is_all_features_in_single_dense = False\n",
    "\n",
    "feature_files = [\n",
    "#     'lystdo_correctwords_lstm',\n",
    "#     'lystdo_1234_loss017',\n",
    "    'Abhishek_features',\n",
    "    'magic_feature',\n",
    "    'magic_feature_v1',\n",
    "    'Howard_feature',\n",
    "    'HubertLin_features_raw',\n",
    "    'HubertLin_features_simple_tokenizer',\n",
    "    'HubertLin_features_word_corrected',\n",
    "    'pagerank',\n",
    "    'word_match_share',\n",
    "    'fasttext_distance',\n",
    "#     'modified_count',\n",
    "    'magic_v25_qid',\n",
    "    'k_scrore',\n",
    "    'pos_dist',\n",
    "    'dep_dist',\n",
    "    'lystdo_leaks'\n",
    "]\n",
    "\n",
    "def inf_nan_to_zero(arr):\n",
    "    nan = np.isnan(arr)\n",
    "    inf = np.isinf(arr)\n",
    "    arr[nan] = 0\n",
    "    arr[inf] = 0\n",
    "    return arr\n",
    "\n",
    "def standardize(train,test):\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((train, test)))\n",
    "    train = ss.transform(train)\n",
    "    test = ss.transform(test)\n",
    "    return train, test\n",
    "\n",
    "print('Loading original leaks')\n",
    "leaks,test_leaks = None, None\n",
    "if not is_all_features_in_single_dense:\n",
    "    leaks = []\n",
    "    test_leaks = []\n",
    "\n",
    "for feature_file in feature_files:\n",
    "    \n",
    "    try:\n",
    "        print('Loading '+feature_file)\n",
    "        train_features,test_features = pickle.load(open('./leaks_cache/ss_cache/'+feature_file+'.pkl' , 'rb'))\n",
    "    except:\n",
    "        train_features = pd.read_csv('./features_from_model/train/'+feature_file+'.csv').as_matrix()\n",
    "        test_features = pd.read_csv('./features_from_model/test/'+feature_file+'.csv').as_matrix()\n",
    "        train_features = inf_nan_to_zero(train_features)\n",
    "        test_features = inf_nan_to_zero(test_features)\n",
    "\n",
    "        train_features,test_features = standardize(train_features,test_features)\n",
    "        pickle.dump([train_features,test_features], open('./leaks_cache/ss_cache/'+feature_file+'.pkl' , 'wb'))\n",
    "\n",
    "    if is_all_features_in_single_dense:\n",
    "        if leaks==None:\n",
    "            leaks = train_features\n",
    "            test_leaks = test_features\n",
    "        else:\n",
    "            leaks = np.hstack([leaks,train_features])\n",
    "            test_leaks = np.hstack([test_leaks,test_features])\n",
    "    else:\n",
    "        leaks.append(train_features)\n",
    "        test_leaks.append(test_features)\n",
    "        \n",
    "# need to wrap it up\n",
    "if is_all_features_in_single_dense:\n",
    "    leaks = [leaks]\n",
    "    test_leaks = [test_leaks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Abhishek_features\n",
      "Loading magic_feature\n",
      "Loading magic_feature_v1\n",
      "Loading Howard_feature\n",
      "Loading HubertLin_features_raw\n",
      "Loading HubertLin_features_simple_tokenizer\n",
      "Loading HubertLin_features_word_corrected\n",
      "Loading pagerank\n",
      "Loading word_match_share\n",
      "Loading fasttext_distance\n",
      "Loading magic_v25_qid\n",
      "Loading k_scrore\n",
      "Loading pos_dist\n",
      "Loading dep_dist\n",
      "Loading lystdo_leaks\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check features name \"\"\"\n",
    "feature_names = []\n",
    "for feature_file in feature_files:\n",
    "    print('Loading '+feature_file)\n",
    "    feature_names += list(pd.read_csv('./features_from_model/train/'+feature_file+'.csv').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merged leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# is_all_features_in_single_dense = True\n",
    "\n",
    "# def standardize(train,test):\n",
    "#     ss = StandardScaler()\n",
    "#     ss.fit(np.vstack((train, test)))\n",
    "#     train = ss.transform(train)\n",
    "#     test = ss.transform(test)\n",
    "#     return train, test\n",
    "\n",
    "# leaks  = pd.read_csv('../Howard/features/merged/train_145_handcraft_features.csv').as_matrix()\n",
    "# test_leaks  = pd.read_csv('../Howard/features/merged/test_145_handcraft_features.csv').as_matrix()\n",
    "\n",
    "# # leaks, test_leaks = standardize(leaks, test_leaks)\n",
    "\n",
    "# leaks = [leaks]\n",
    "# test_leaks = [test_leaks]\n",
    "\n",
    "# # pickle.dump([leaks, test_leaks], open('merge_cache.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [leaks, test_leaks] = pickle.load(open('merge_cache.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use previous models' outputs as helper (seems not good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_prev_model_as_helper = False\n",
    "\n",
    "helpers = [\n",
    "    'lystdo_full_features_0.1744_prediction_max',\n",
    "    'lystdo_full_features_0.1744_prediction_mean',\n",
    "    'lystdo_full_features_and_poolings_0.1806_prediction_max',\n",
    "    'lystdo_onlyAB_0.1704_prediction_max',\n",
    "    'lystdo_onlyAB_0.1704_prediction_mean',\n",
    "    'lystdo_original_fullfeature_prev_lstm_0.1739_prediction_max',\n",
    "    'lystdo_original_fullfeature_prev_lstm_0.1739_prediction_mean',\n",
    "    'lystdo_text_corrected_0.1719_prediction_max',\n",
    "    'lystdo_text_corrected_0.1719_prediction_mean',\n",
    "]\n",
    "\n",
    "if use_prev_model_as_helper:\n",
    "    train_tmp = []\n",
    "    test_tmp = []\n",
    "\n",
    "    def read_csv(file, is_train):\n",
    "        if is_train:\n",
    "            name = './model_predictions/train/'+file+'.csv'\n",
    "        else:\n",
    "            name = './model_predictions/test/'+file+'.csv'\n",
    "        df = pd.read_csv(name)\n",
    "        df = pd.DataFrame(df['is_duplicate'].as_matrix(), columns=[file+'_is_duplicate'])\n",
    "        return df\n",
    "\n",
    "    for helper in helpers:\n",
    "        print('Loading :', helper)\n",
    "        train_tmp.append(read_csv(helper,is_train=True))\n",
    "        test_tmp.append(read_csv(helper,is_train=False))\n",
    "\n",
    "    train_helpers = [pd.concat(train_tmp, axis=1).as_matrix()]\n",
    "    test_helpers = [pd.concat(test_tmp, axis=1).as_matrix()]\n",
    "\n",
    "    del train_tmp\n",
    "    del test_tmp\n",
    "else:\n",
    "    train_helpers = []\n",
    "    val_helpers = []\n",
    "    test_helpers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## prepare validation data index\n",
    "########################################\n",
    "\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "# import pickle\n",
    "# idx_val = np.unique(pickle.load(open('./val/val_idxes.pkl','rb'))).astype('int')\n",
    "# idx_train = np.delete(np.arange(len(data_1)), idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "\n",
    "def select_leaks(leaks, idxes, stack):\n",
    "    ret = []\n",
    "    for leak in leaks:\n",
    "        if stack:\n",
    "            ret.append(np.vstack([leak[idxes],leak[idxes]]))\n",
    "        else:\n",
    "            ret.append(leak[idxes])\n",
    "    return ret\n",
    "\n",
    "stack = True\n",
    "\n",
    "if stack:\n",
    "    data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "    data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "    leaks_train = select_leaks(leaks,idx_train,stack)\n",
    "    labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    leaks_val = select_leaks(leaks,idx_val,stack=False)\n",
    "    labels_val = labels[idx_val]\n",
    "    \n",
    "    if use_prev_model_as_helper:\n",
    "        val_helpers = [train_helpers[0][idx_val]]\n",
    "        train_helpers = [np.vstack([train_helpers[0][idx_train],train_helpers[0][idx_train]])]\n",
    "else:\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    leaks_train = select_leaks(leaks,idx_train,stack)\n",
    "    labels_train = labels[idx_train]\n",
    "    \n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    leaks_val = select_leaks(leaks,idx_val,stack)\n",
    "    labels_val = labels[idx_val]\n",
    "    \n",
    "    if use_prev_model_as_helper:\n",
    "        val_helpers = [train_helpers[0][idx_val]]\n",
    "        train_helpers = [train_helpers[0][idx_train]]\n",
    "    \n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_lstm = 256 # np.random.randint(175, 256)\n",
    "num_dense = 128 # np.random.randint(100, 128)\n",
    "rate_drop_lstm = 0.1 # + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.1 # + np.random.rand() * 0.25\n",
    "\n",
    "STAMP = './leaks_cache/lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.activations import relu\n",
    "\n",
    "# act = PReLU\n",
    "act = 'relu'\n",
    "\n",
    "def dropout_dense(layer, d_rate=0.05, dense_size=num_dense//2):\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(d_rate)(layer)\n",
    "    layer = Dense(dense_size, activation=act)(layer)\n",
    "    return layer\n",
    "\n",
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "extra_inputs = []\n",
    "extra_outputs = []\n",
    "\n",
    "features_len = len(leaks)\n",
    "for i in range(features_len):\n",
    "    leaks_input = Input(shape=(leaks[i].shape[1],))\n",
    "    extra_inputs.append(leaks_input)\n",
    "    \n",
    "    leaks_dense = Dropout(0.05)(leaks_input)\n",
    "    leaks_dense = Dense(num_dense//2, activation=act)(leaks_dense)\n",
    "    extra_outputs.append(leaks_dense)\n",
    "    # leaks_dense = BatchNormalization()(leaks_dense)\n",
    "    # leaks_dense = Activation('relu')(leaks_dense)\n",
    "    # leaks_dense = Dropout(0.25)(leaks_dense)\n",
    "    # leaks_dense = Dense(num_dense, activation=act)(leaks_dense)\n",
    "    \n",
    "if features_len>1:\n",
    "    extra_merge = concatenate(extra_outputs)\n",
    "    extra_merge = BatchNormalization()(extra_merge)\n",
    "    extra_merge = Activation('relu')(extra_merge)\n",
    "    extra_merge = Dropout(rate_drop_dense)(extra_merge)\n",
    "    extra_merge = Dense(num_dense*2, activation=act)(extra_merge)\n",
    "\n",
    "    extra_merge = BatchNormalization()(extra_merge)\n",
    "    extra_merge = Activation('relu')(extra_merge)\n",
    "    extra_merge = Dropout(rate_drop_dense)(extra_merge)\n",
    "    extra_merge = Dense(num_dense*2, activation=act)(extra_merge)\n",
    "else:\n",
    "    extra_merge = BatchNormalization()(extra_outputs[0])\n",
    "    extra_merge = Activation('relu')(extra_merge)\n",
    "    extra_merge = Dropout(rate_drop_dense)(extra_merge)\n",
    "    extra_merge = Dense(num_dense*2, activation=act)(extra_merge)\n",
    "\n",
    "    \n",
    "first_dense = Dense(num_dense, activation=act)\n",
    "\n",
    "merged1 = concatenate([x1, y1, extra_merge])\n",
    "merged1 = BatchNormalization()(merged1)\n",
    "merged1 = Activation('relu')(merged1)\n",
    "merged1 = Dropout(rate_drop_dense)(merged1)\n",
    "merged1 = first_dense(merged1)\n",
    "\n",
    "merged2 = concatenate([y1, x1, extra_merge])\n",
    "merged2 = BatchNormalization()(merged2)\n",
    "merged2 = Activation('relu')(merged2)\n",
    "merged2 = Dropout(rate_drop_dense)(merged2)\n",
    "merged2 = first_dense(merged2)\n",
    "\n",
    "merged = concatenate([merged1, merged2])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Activation('relu')(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Activation('relu')(merged)\n",
    "merged = Dropout(0.05)(merged)\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "\n",
    "# merged = Dense(16)(merged)\n",
    "\n",
    "# merged = concatenate([x1, y1, extra_merge])\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "# merged = Dense(num_dense, activation=act)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "if use_prev_model_as_helper:\n",
    "    helper_input = Input(shape=(train_helpers[0].shape[1],))\n",
    "    merged = concatenate([merged, helper_input])\n",
    "    merged = Dropout(0.05)(merged)\n",
    "    merged = Dense(num_dense, activation=act)(merged)\n",
    "    merged = Dropout(0.05)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_256_128_0.10_0.10\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "\n",
    "if use_prev_model_as_helper:\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, helper_input]+extra_inputs, \\\n",
    "            outputs=preds)\n",
    "else:\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input]+extra_inputs, \\\n",
    "            outputs=preds)\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer=Nadam(lr=0.002),\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "bst_model_path = STAMP + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Learn with fitting all data at once, requires large memory\n",
    "'''\n",
    "\n",
    "# early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "# model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# hist = model.fit([data_1_train, data_2_train]+train_helpers+leaks_train, labels_train, \\\n",
    "#         validation_data=([data_1_val, data_2_val]+val_helpers+leaks_val, labels_val, weight_val), \\\n",
    "#         epochs=200, batch_size=1024, shuffle=True, \\\n",
    "#         class_weight=class_weight, callbacks=[early_stopping, model_checkpoint], verbose=2)\n",
    "# model.load_weights(bst_model_path)\n",
    "# bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9s - loss: 0.2408 - acc: 0.8210 - val_loss: 0.2663 - val_acc: 0.8604\n",
      "Epoch 2/1000\n",
      "2s - loss: 0.2025 - acc: 0.8497 - val_loss: 0.2299 - val_acc: 0.8652\n",
      "Epoch 3/1000\n",
      "2s - loss: 0.1960 - acc: 0.8562 - val_loss: 0.2044 - val_acc: 0.8491\n",
      "Epoch 4/1000\n",
      "2s - loss: 0.1966 - acc: 0.8533 - val_loss: 0.1951 - val_acc: 0.8492\n",
      "Epoch 5/1000\n",
      "2s - loss: 0.1906 - acc: 0.8610 - val_loss: 0.1855 - val_acc: 0.8587\n",
      "Epoch 6/1000\n",
      "2s - loss: 0.1903 - acc: 0.8587 - val_loss: 0.1877 - val_acc: 0.8706\n",
      "Epoch 7/1000\n",
      "2s - loss: 0.1867 - acc: 0.8622 - val_loss: 0.1845 - val_acc: 0.8535\n",
      "Epoch 8/1000\n",
      "2s - loss: 0.1868 - acc: 0.8636 - val_loss: 0.1858 - val_acc: 0.8732\n",
      "Epoch 9/1000\n",
      "2s - loss: 0.1883 - acc: 0.8616 - val_loss: 0.1850 - val_acc: 0.8616\n",
      "Epoch 10/1000\n",
      "2s - loss: 0.1874 - acc: 0.8623 - val_loss: 0.1856 - val_acc: 0.8512\n",
      "Epoch 11/1000\n",
      "2s - loss: 0.1850 - acc: 0.8619 - val_loss: 0.1886 - val_acc: 0.8423\n",
      "Epoch 12/1000\n",
      "2s - loss: 0.1841 - acc: 0.8655 - val_loss: 0.1787 - val_acc: 0.8695\n",
      "Epoch 13/1000\n",
      "2s - loss: 0.1837 - acc: 0.8645 - val_loss: 0.1810 - val_acc: 0.8765\n",
      "Epoch 14/1000\n",
      "2s - loss: 0.1819 - acc: 0.8673 - val_loss: 0.1798 - val_acc: 0.8672\n",
      "Epoch 15/1000\n",
      "2s - loss: 0.1810 - acc: 0.8660 - val_loss: 0.1828 - val_acc: 0.8601\n",
      "Epoch 16/1000\n",
      "2s - loss: 0.1803 - acc: 0.8668 - val_loss: 0.1796 - val_acc: 0.8619\n",
      "Epoch 17/1000\n",
      "2s - loss: 0.1809 - acc: 0.8668 - val_loss: 0.1789 - val_acc: 0.8711\n",
      "Epoch 18/1000\n",
      "2s - loss: 0.1803 - acc: 0.8686 - val_loss: 0.1800 - val_acc: 0.8602\n",
      "Epoch 19/1000\n",
      "2s - loss: 0.1794 - acc: 0.8678 - val_loss: 0.1773 - val_acc: 0.8700\n",
      "Epoch 20/1000\n",
      "2s - loss: 0.1808 - acc: 0.8661 - val_loss: 0.1790 - val_acc: 0.8716\n",
      "Epoch 21/1000\n",
      "2s - loss: 0.1793 - acc: 0.8662 - val_loss: 0.1771 - val_acc: 0.8658\n",
      "Epoch 22/1000\n",
      "2s - loss: 0.1803 - acc: 0.8641 - val_loss: 0.1805 - val_acc: 0.8544\n",
      "Epoch 23/1000\n",
      "2s - loss: 0.1795 - acc: 0.8664 - val_loss: 0.1827 - val_acc: 0.8542\n",
      "Epoch 24/1000\n",
      "2s - loss: 0.1785 - acc: 0.8684 - val_loss: 0.1776 - val_acc: 0.8642\n",
      "Epoch 25/1000\n",
      "2s - loss: 0.1780 - acc: 0.8683 - val_loss: 0.1794 - val_acc: 0.8728\n",
      "Epoch 26/1000\n",
      "2s - loss: 0.1801 - acc: 0.8668 - val_loss: 0.1777 - val_acc: 0.8607\n",
      "Epoch 27/1000\n",
      "2s - loss: 0.1774 - acc: 0.8674 - val_loss: 0.1769 - val_acc: 0.8634\n",
      "Epoch 28/1000\n",
      "2s - loss: 0.1783 - acc: 0.8651 - val_loss: 0.1790 - val_acc: 0.8614\n",
      "Epoch 29/1000\n",
      "2s - loss: 0.1769 - acc: 0.8700 - val_loss: 0.1842 - val_acc: 0.8495\n",
      "Epoch 30/1000\n",
      "2s - loss: 0.1779 - acc: 0.8682 - val_loss: 0.1798 - val_acc: 0.8559\n",
      "Epoch 31/1000\n",
      "2s - loss: 0.1761 - acc: 0.8704 - val_loss: 0.1773 - val_acc: 0.8635\n",
      "Epoch 32/1000\n",
      "2s - loss: 0.1752 - acc: 0.8706 - val_loss: 0.1751 - val_acc: 0.8725\n",
      "Epoch 33/1000\n",
      "2s - loss: 0.1751 - acc: 0.8691 - val_loss: 0.1773 - val_acc: 0.8647\n",
      "Epoch 34/1000\n",
      "2s - loss: 0.1736 - acc: 0.8718 - val_loss: 0.1806 - val_acc: 0.8609\n",
      "Epoch 35/1000\n",
      "2s - loss: 0.1758 - acc: 0.8704 - val_loss: 0.1783 - val_acc: 0.8608\n",
      "Epoch 36/1000\n",
      "2s - loss: 0.1760 - acc: 0.8705 - val_loss: 0.1796 - val_acc: 0.8557\n",
      "Epoch 37/1000\n",
      "2s - loss: 0.1740 - acc: 0.8708 - val_loss: 0.1759 - val_acc: 0.8639\n",
      "Epoch 38/1000\n",
      "2s - loss: 0.1751 - acc: 0.8706 - val_loss: 0.1773 - val_acc: 0.8649\n",
      "Epoch 39/1000\n",
      "2s - loss: 0.1750 - acc: 0.8702 - val_loss: 0.1744 - val_acc: 0.8679\n",
      "Epoch 40/1000\n",
      "2s - loss: 0.1737 - acc: 0.8712 - val_loss: 0.1765 - val_acc: 0.8755\n",
      "Epoch 41/1000\n",
      "2s - loss: 0.1739 - acc: 0.8716 - val_loss: 0.1784 - val_acc: 0.8581\n",
      "Epoch 42/1000\n",
      "2s - loss: 0.1742 - acc: 0.8716 - val_loss: 0.1754 - val_acc: 0.8716\n",
      "Epoch 43/1000\n",
      "2s - loss: 0.1758 - acc: 0.8708 - val_loss: 0.1771 - val_acc: 0.8644\n",
      "Epoch 44/1000\n",
      "2s - loss: 0.1744 - acc: 0.8710 - val_loss: 0.1764 - val_acc: 0.8778\n",
      "Epoch 45/1000\n",
      "2s - loss: 0.1753 - acc: 0.8694 - val_loss: 0.1741 - val_acc: 0.8685\n",
      "Epoch 46/1000\n",
      "2s - loss: 0.1729 - acc: 0.8710 - val_loss: 0.1759 - val_acc: 0.8682\n",
      "Epoch 47/1000\n",
      "2s - loss: 0.1717 - acc: 0.8726 - val_loss: 0.1770 - val_acc: 0.8687\n",
      "Epoch 48/1000\n",
      "2s - loss: 0.1717 - acc: 0.8727 - val_loss: 0.1749 - val_acc: 0.8651\n",
      "Epoch 49/1000\n",
      "2s - loss: 0.1716 - acc: 0.8740 - val_loss: 0.1748 - val_acc: 0.8701\n",
      "Epoch 50/1000\n",
      "2s - loss: 0.1711 - acc: 0.8760 - val_loss: 0.1731 - val_acc: 0.8717\n",
      "Epoch 51/1000\n",
      "2s - loss: 0.1704 - acc: 0.8753 - val_loss: 0.1768 - val_acc: 0.8634\n",
      "Epoch 52/1000\n",
      "2s - loss: 0.1693 - acc: 0.8743 - val_loss: 0.1744 - val_acc: 0.8724\n",
      "Epoch 53/1000\n",
      "2s - loss: 0.1712 - acc: 0.8752 - val_loss: 0.1803 - val_acc: 0.8544\n",
      "Epoch 54/1000\n",
      "2s - loss: 0.1709 - acc: 0.8733 - val_loss: 0.1746 - val_acc: 0.8691\n",
      "Epoch 55/1000\n",
      "2s - loss: 0.1711 - acc: 0.8734 - val_loss: 0.1745 - val_acc: 0.8656\n",
      "Epoch 56/1000\n",
      "2s - loss: 0.1709 - acc: 0.8735 - val_loss: 0.1758 - val_acc: 0.8603\n",
      "Epoch 57/1000\n",
      "2s - loss: 0.1690 - acc: 0.8737 - val_loss: 0.1751 - val_acc: 0.8719\n",
      "Epoch 58/1000\n",
      "2s - loss: 0.1709 - acc: 0.8727 - val_loss: 0.1747 - val_acc: 0.8629\n",
      "Epoch 59/1000\n",
      "2s - loss: 0.1682 - acc: 0.8754 - val_loss: 0.1760 - val_acc: 0.8616\n",
      "Epoch 60/1000\n",
      "2s - loss: 0.1682 - acc: 0.8761 - val_loss: 0.1760 - val_acc: 0.8763\n",
      "Epoch 61/1000\n",
      "2s - loss: 0.1693 - acc: 0.8738 - val_loss: 0.1743 - val_acc: 0.8662\n",
      "Epoch 00060: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "def batch_generator(q1, q2, leaks, labels):\n",
    "    \n",
    "    def select_leaks(leaks, idxes):\n",
    "        ret = []\n",
    "        for leak in leaks:\n",
    "            ret.append(leak[idxes])\n",
    "        return ret\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        idxes = np.array([np.random.randint(0,len(q1)) for i in range(batch_size)])\n",
    "        extra_features = select_leaks(leaks, idxes)\n",
    "        if use_prev_model_as_helper:\n",
    "            helper = [train_helpers[0][idxes]]\n",
    "        else:\n",
    "            helper = []\n",
    "        yield [q1[idxes],q2[idxes]]+helper+extra_features, labels[idxes]\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1),\n",
    "    ModelCheckpoint(bst_model_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min', period=1, save_weights_only=True)\n",
    "]\n",
    "    \n",
    "try:\n",
    "        \n",
    "    hist = model.fit_generator(batch_generator(data_1_train, data_2_train, leaks_train, labels_train),\n",
    "                            steps_per_epoch=128,\n",
    "                            class_weight=class_weight,\n",
    "                            epochs=1000,\n",
    "                            validation_data=([data_1_val, data_2_val]+val_helpers+leaks_val, labels_val, weight_val), \n",
    "                            verbose=2,\n",
    "                            callbacks=callbacks)\n",
    "\n",
    "        \n",
    "    model.load_weights(bst_model_path)\n",
    "    bst_val_score = min(hist.history['val_loss'])\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('\\nEarly stopped by user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_name = 'lystdo_Fasttext_AllFeatures_WordCorrection_Loss' + '_%.4f'%(bst_val_score)\n",
    "# model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17339470358864317"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "SET = 'VAL'\n",
    "\n",
    "if SET=='VAL':\n",
    "    pred = model.predict([data_1_val, data_2_val]+leaks_val)\n",
    "#     weights = weight_val\n",
    "    gts = labels_val\n",
    "    weights = [class_weight[label] for label in gts]\n",
    "elif SET=='TRAIN':\n",
    "    sample_size = 30000\n",
    "    rnd_idxes = np.random.permutation(sample_size)\n",
    "    pred = model.predict([data_1_train[rnd_idxes], data_2_train[rnd_idxes]]+ leaks_train[rnd_idxes])\n",
    "    gts = labels_train[rnd_idxes]\n",
    "    weights = [class_weight[label] for label in gts]\n",
    "\n",
    "pred[pred==0] = 0.000001\n",
    "pred[pred==1] = 0.999999\n",
    "log_loss(gts,pred.flatten(), sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 0.173394703589\n",
      "loss= 0.173394703589\n",
      "loss= 0.173394703589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def abs_dist(a):\n",
    "    return abs(a-0.5)\n",
    "\n",
    "pred_a = model.predict([data_1_val, data_2_val]+leaks_val)\n",
    "pred_b = model.predict([data_2_val, data_1_val]+leaks_val)\n",
    "preds = [\n",
    "    np.array([pred_a[i] if abs_dist(pred_a[i])>abs_dist(pred_b[i]) else pred_b[i] for i in range(pred_a.shape[0])]),\n",
    "    (pred_a + pred_b)/2,\n",
    "    pred_a,\n",
    "]\n",
    "\n",
    "gts = labels_val\n",
    "weights = [class_weight[label] for label in gts]\n",
    "\n",
    "for pred in preds:\n",
    "\n",
    "    pred[pred==0] = 0.000001\n",
    "    pred[pred==1] = 0.999999\n",
    "    loss = log_loss(gts,pred.flatten(), sample_weight=weights)\n",
    "    \n",
    "    print('loss=',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prediction CSV for testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making the submission before fine-tuning\n",
      "Predicting first part\n",
      "Predicting second part\n",
      "Predicting first part\n",
      "Predicting second part\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "def do_pred(SET, mode):\n",
    "    if SET=='train':\n",
    "        q1 = data_1\n",
    "        q2 = data_2\n",
    "        extra = leaks\n",
    "    else:\n",
    "        q1 = test_data_1\n",
    "        q2 = test_data_2\n",
    "        extra = test_leaks\n",
    "\n",
    "    if mode=='mean':\n",
    "        print('Predicting first part')\n",
    "        preds = model.predict([q1, q2]+extra, batch_size=1024, verbose=0)\n",
    "        print('Predicting second part')\n",
    "        preds += model.predict([q2, q1]+extra, batch_size=1024, verbose=0)\n",
    "        preds /= 2\n",
    "    else:\n",
    "        def abs_dist(a):\n",
    "            return abs(a-0.5)\n",
    "\n",
    "        print('Predicting first part')\n",
    "        pred_a = model.predict([q1, q2]+extra, batch_size=1024, verbose=0)\n",
    "        print('Predicting second part')\n",
    "        pred_b = model.predict([q2, q1]+extra, batch_size=1024, verbose=0)\n",
    "        preds = np.array([pred_a[i] if abs_dist(pred_a[i])>abs_dist(pred_b[i]) else pred_b[i] for i in range(pred_a.shape[0])])\n",
    "\n",
    "    if SET=='train':\n",
    "        submission = pd.DataFrame({'is_duplicate':preds.ravel()})\n",
    "        submission.to_csv('./model_predictions/train/' + model_name + '_%.4f'%(bst_val_score) + '_prediction_' + mode + '.csv', index=False)\n",
    "    else:\n",
    "        submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "        submission.to_csv('./model_predictions/test/' + model_name + '_%.4f'%(bst_val_score) + '_prediction_' + mode + '.csv', index=False)\n",
    "        \n",
    "do_pred(SET='train', mode='mean')\n",
    "do_pred(SET='test', mode='mean')\n",
    "\n",
    "# do_pred(SET='train', mode='max')\n",
    "# do_pred(SET='test', mode='max')\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Get the internal output of model, which can be used as extracted features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f3947b99828>,\n",
       " <keras.engine.topology.InputLayer at 0x7f3947776e80>,\n",
       " <keras.engine.topology.InputLayer at 0x7f394764b550>,\n",
       " <keras.engine.topology.InputLayer at 0x7f394758e860>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3947b999e8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3947739e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3947677ef0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39475bc860>,\n",
       " <keras.layers.core.Activation at 0x7f39477f2da0>,\n",
       " <keras.layers.core.Activation at 0x7f39476cf358>,\n",
       " <keras.layers.core.Activation at 0x7f394760e198>,\n",
       " <keras.layers.core.Activation at 0x7f3947550048>,\n",
       " <keras.layers.core.Dropout at 0x7f39477f2d30>,\n",
       " <keras.layers.core.Dropout at 0x7f39476cf320>,\n",
       " <keras.layers.core.Dropout at 0x7f394760e630>,\n",
       " <keras.layers.core.Dropout at 0x7f3947550940>,\n",
       " <keras.layers.core.Dense at 0x7f39477f2f60>,\n",
       " <keras.layers.core.Dense at 0x7f39476cf438>,\n",
       " <keras.layers.core.Dense at 0x7f394760e748>,\n",
       " <keras.layers.core.Dense at 0x7f3947550a58>,\n",
       " <keras.layers.merge.Concatenate at 0x7f3947eb7c88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39474d2b70>,\n",
       " <keras.engine.topology.InputLayer at 0x7f3947eb7940>,\n",
       " <keras.engine.topology.InputLayer at 0x7f3947eb7ba8>,\n",
       " <keras.layers.core.Activation at 0x7f3947eb7cc0>,\n",
       " <keras.layers.embeddings.Embedding at 0x7f3947eb7898>,\n",
       " <keras.layers.core.Dropout at 0x7f3947493cf8>,\n",
       " <keras.layers.recurrent.LSTM at 0x7f3947eb7860>,\n",
       " <keras.layers.core.Dense at 0x7f3947493588>,\n",
       " <keras.layers.merge.Concatenate at 0x7f3947493c88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3947414fd0>,\n",
       " <keras.layers.core.Dropout at 0x7f3947429e48>,\n",
       " <keras.layers.core.Dense at 0x7f39473d5470>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39473fd128>,\n",
       " <keras.layers.core.Dropout at 0x7f39473419b0>,\n",
       " <keras.layers.core.Dense at 0x7f3947317eb8>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "layer_id = -9\n",
    "# model_segment = K.function(model.input+[K.learning_phase()], [model.layers[layer_id].output])\n",
    "model_segment = K.function(model.input+[K.learning_phase()], [model.layers[layer_id].get_output_at(-1)])\n",
    "\n",
    "def get_layer_output(inputs):\n",
    "    return model_segment(inputs+[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'lstm_3_1/TensorArrayReadV3:0' shape=(?, 199) dtype=float32>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_segment.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_count = num_lstm\n",
    "feature_names = [model_name+'_lstm_' + str(i) for i in range(feature_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "99328\n",
      "198656\n",
      "297984\n",
      "397312\n",
      "496640\n",
      "595968\n",
      "695296\n",
      "794624\n",
      "893952\n",
      "993280\n",
      "1092608\n",
      "1191936\n",
      "1291264\n",
      "1390592\n",
      "1489920\n",
      "1589248\n",
      "1688576\n",
      "1787904\n",
      "1887232\n",
      "1986560\n",
      "2085888\n",
      "2185216\n",
      "2284544\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import csv\n",
    "\n",
    "SET = 'TRAIN'\n",
    "# SET = 'TEST'\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "f_name = './leaks_cache/'+model_name + '_%.4f'%(bst_val_score) + '_feature.csv'\n",
    "\n",
    "if SET == 'TRAIN':\n",
    "    q1 = data_1\n",
    "    q2 = data_2\n",
    "    other = leaks\n",
    "else:\n",
    "    q1 = test_data_1\n",
    "    q2 = test_data_2\n",
    "    other = test_leaks\n",
    "\n",
    "    \n",
    "with open(f_name,'w') as f:\n",
    "\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(feature_names)\n",
    "\n",
    "    batch_num = int(ceil(q1.shape[0] / batch_size))\n",
    "    \n",
    "    for b_id in range(batch_num):\n",
    "        low = b_id*batch_size\n",
    "        high = (b_id+1)*batch_size\n",
    "\n",
    "        extra_inputs = [i[low:high] for i in other]\n",
    "        extracted_features = get_layer_output([q1[low:high],q2[low:high]]+extra_inputs)\n",
    "\n",
    "        for feature in extracted_features:\n",
    "            writer.writerow(feature)\n",
    "\n",
    "        if b_id % (100000//batch_size) ==0:\n",
    "            print( b_id*batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
