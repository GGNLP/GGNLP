{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Some ideas\n",
    "\n",
    "1. Since we already have data that labelled as same question paris. Is it possible for us to train another model which can sequetially generate a same question string depends on the input? \n",
    "2. curriculum learning\n",
    "3. Ensombling:\n",
    "\n",
    "        a. Naive RNN\n",
    "        b. xgboost\n",
    "        c. randomforest\n",
    "        \n",
    "4. Feature engineering\n",
    "\n",
    "        a. question length (May not be very powerful)\n",
    "        b. similarity metric score (Directly count it and feed it as training feature. For exampole: cosine similarity LOL)\n",
    "        c. I don't think TF-IDF would work, but still worth trying if we have time.\n",
    "        \n",
    "5. Create tags for each question by using the sentense paired with it. \n",
    "\n",
    "        For example:\n",
    "        Q1: I am a good man.    <->    Q2: A good man is me.\n",
    "        \n",
    "        Q1 is tagged with [A, good, man, is, me]\n",
    "        Q2 is tagged with [I, am, a, good, man]\n",
    "        \n",
    "        Then use these data to train a model predicting what tags would appear for each question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "\n",
    "duplicated_qid_pair = pickle.load(open('../dataset/processed/positive_question_id_pairs.pkl', 'rb'))\n",
    "qid_question_dict = pickle.load(open('../dataset/processed/qid_question_wrods_list_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question(question, enc_map):\n",
    "    \n",
    "    if type(question)!=str or question==\"\":\n",
    "        return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "    \n",
    "    def map_wrod(word):\n",
    "        return enc_map[word] if word in enc_map else enc_map['<RARE>']\n",
    "         \n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "    separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>]|$)'\n",
    "    single_word = '[\\S]+' # non-empty is enough here\n",
    "    \n",
    "    words_list = re.findall(single_word+separator, question)\n",
    "    \n",
    "    return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip training questions\n",
    "\n",
    "Normalize the length of all question to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in qid_question_dict:\n",
    "    q_len = len(qid_question_dict[key])\n",
    "    if q_len>=30:\n",
    "        qid_question_dict[key] = qid_question_dict[key][:30]\n",
    "    else:\n",
    "        qid_question_dict[key] = qid_question_dict[key] + ['<ED>'] * (30-q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(qid_question_dict, open('../dataset/processed/qid_question_wrods_list_dict_clipped.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and encode testing data (this is very slow)\n",
    "\n",
    "I still parses question length feature, although I haven't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def change_to_list_and_clip(data, clip_st_ed=False):\n",
    "    \n",
    "    global counter\n",
    "    counter+=1\n",
    "    if counter%10000==0:\n",
    "        print('Changed',counter,'data')\n",
    "    \n",
    "    clip_len = 30\n",
    "    data['q1_len'] = len(data['question1'])\n",
    "    data['q2_len'] = len(data['question2'])\n",
    "\n",
    "    if clip_st_ed:\n",
    "        data['question1'] = data['question1'][1:-1]\n",
    "        data['question2'] = data['question2'][1:-1]\n",
    "\n",
    "    if len(data['question1'])<clip_len:\n",
    "        data['question1'] = data['question1'] + [enc_map['<ED>']] * (30-data['q1_len'])\n",
    "    else:\n",
    "        data['question1'] = data['question1'][:30]\n",
    "        \n",
    "    if len(data['question2'])<clip_len:\n",
    "        data['question2'] = data['question2'] + [enc_map['<ED>']] * (30-data['q2_len'])\n",
    "    else:\n",
    "        data['question2'] = data['question2'][:30]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "df_test = pickle.load(open('../dataset/processed/processed_testing_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed 10000 data\n",
      "Changed 20000 data\n",
      "Changed 30000 data\n",
      "Changed 40000 data\n",
      "Changed 50000 data\n",
      "Changed 60000 data\n",
      "Changed 70000 data\n",
      "Changed 80000 data\n",
      "Changed 90000 data\n",
      "Changed 100000 data\n",
      "Changed 110000 data\n",
      "Changed 120000 data\n",
      "Changed 130000 data\n",
      "Changed 140000 data\n",
      "Changed 150000 data\n",
      "Changed 160000 data\n",
      "Changed 170000 data\n",
      "Changed 180000 data\n",
      "Changed 190000 data\n",
      "Changed 200000 data\n",
      "Changed 210000 data\n",
      "Changed 220000 data\n",
      "Changed 230000 data\n",
      "Changed 240000 data\n",
      "Changed 250000 data\n",
      "Changed 260000 data\n",
      "Changed 270000 data\n",
      "Changed 280000 data\n",
      "Changed 290000 data\n",
      "Changed 300000 data\n",
      "Changed 310000 data\n",
      "Changed 320000 data\n",
      "Changed 330000 data\n",
      "Changed 340000 data\n",
      "Changed 350000 data\n",
      "Changed 360000 data\n",
      "Changed 370000 data\n",
      "Changed 380000 data\n",
      "Changed 390000 data\n",
      "Changed 400000 data\n",
      "Changed 410000 data\n",
      "Changed 420000 data\n",
      "Changed 430000 data\n",
      "Changed 440000 data\n",
      "Changed 450000 data\n",
      "Changed 460000 data\n",
      "Changed 470000 data\n",
      "Changed 480000 data\n",
      "Changed 490000 data\n",
      "Changed 500000 data\n",
      "Changed 510000 data\n",
      "Changed 520000 data\n",
      "Changed 530000 data\n",
      "Changed 540000 data\n",
      "Changed 550000 data\n",
      "Changed 560000 data\n",
      "Changed 570000 data\n",
      "Changed 580000 data\n",
      "Changed 590000 data\n",
      "Changed 600000 data\n",
      "Changed 610000 data\n",
      "Changed 620000 data\n",
      "Changed 630000 data\n",
      "Changed 640000 data\n",
      "Changed 650000 data\n",
      "Changed 660000 data\n",
      "Changed 670000 data\n",
      "Changed 680000 data\n",
      "Changed 690000 data\n",
      "Changed 700000 data\n",
      "Changed 710000 data\n",
      "Changed 720000 data\n",
      "Changed 730000 data\n",
      "Changed 740000 data\n",
      "Changed 750000 data\n",
      "Changed 760000 data\n",
      "Changed 770000 data\n",
      "Changed 780000 data\n",
      "Changed 790000 data\n",
      "Changed 800000 data\n",
      "Changed 810000 data\n",
      "Changed 820000 data\n",
      "Changed 830000 data\n",
      "Changed 840000 data\n",
      "Changed 850000 data\n",
      "Changed 860000 data\n",
      "Changed 870000 data\n",
      "Changed 880000 data\n",
      "Changed 890000 data\n",
      "Changed 900000 data\n",
      "Changed 910000 data\n",
      "Changed 920000 data\n",
      "Changed 930000 data\n",
      "Changed 940000 data\n",
      "Changed 950000 data\n",
      "Changed 960000 data\n",
      "Changed 970000 data\n",
      "Changed 980000 data\n",
      "Changed 990000 data\n",
      "Changed 1000000 data\n",
      "Changed 1010000 data\n",
      "Changed 1020000 data\n",
      "Changed 1030000 data\n",
      "Changed 1040000 data\n",
      "Changed 1050000 data\n",
      "Changed 1060000 data\n",
      "Changed 1070000 data\n",
      "Changed 1080000 data\n",
      "Changed 1090000 data\n",
      "Changed 1100000 data\n",
      "Changed 1110000 data\n",
      "Changed 1120000 data\n",
      "Changed 1130000 data\n",
      "Changed 1140000 data\n",
      "Changed 1150000 data\n",
      "Changed 1160000 data\n",
      "Changed 1170000 data\n",
      "Changed 1180000 data\n",
      "Changed 1190000 data\n",
      "Changed 1200000 data\n",
      "Changed 1210000 data\n",
      "Changed 1220000 data\n",
      "Changed 1230000 data\n",
      "Changed 1240000 data\n",
      "Changed 1250000 data\n",
      "Changed 1260000 data\n",
      "Changed 1270000 data\n",
      "Changed 1280000 data\n",
      "Changed 1290000 data\n",
      "Changed 1300000 data\n",
      "Changed 1310000 data\n",
      "Changed 1320000 data\n",
      "Changed 1330000 data\n",
      "Changed 1340000 data\n",
      "Changed 1350000 data\n",
      "Changed 1360000 data\n",
      "Changed 1370000 data\n",
      "Changed 1380000 data\n",
      "Changed 1390000 data\n",
      "Changed 1400000 data\n",
      "Changed 1410000 data\n",
      "Changed 1420000 data\n",
      "Changed 1430000 data\n",
      "Changed 1440000 data\n",
      "Changed 1450000 data\n",
      "Changed 1460000 data\n",
      "Changed 1470000 data\n",
      "Changed 1480000 data\n",
      "Changed 1490000 data\n",
      "Changed 1500000 data\n",
      "Changed 1510000 data\n",
      "Changed 1520000 data\n",
      "Changed 1530000 data\n",
      "Changed 1540000 data\n",
      "Changed 1550000 data\n",
      "Changed 1560000 data\n",
      "Changed 1570000 data\n",
      "Changed 1580000 data\n",
      "Changed 1590000 data\n",
      "Changed 1600000 data\n",
      "Changed 1610000 data\n",
      "Changed 1620000 data\n",
      "Changed 1630000 data\n",
      "Changed 1640000 data\n",
      "Changed 1650000 data\n",
      "Changed 1660000 data\n",
      "Changed 1670000 data\n",
      "Changed 1680000 data\n",
      "Changed 1690000 data\n",
      "Changed 1700000 data\n",
      "Changed 1710000 data\n",
      "Changed 1720000 data\n",
      "Changed 1730000 data\n",
      "Changed 1740000 data\n",
      "Changed 1750000 data\n",
      "Changed 1760000 data\n",
      "Changed 1770000 data\n",
      "Changed 1780000 data\n",
      "Changed 1790000 data\n",
      "Changed 1800000 data\n",
      "Changed 1810000 data\n",
      "Changed 1820000 data\n",
      "Changed 1830000 data\n",
      "Changed 1840000 data\n",
      "Changed 1850000 data\n",
      "Changed 1860000 data\n",
      "Changed 1870000 data\n",
      "Changed 1880000 data\n",
      "Changed 1890000 data\n",
      "Changed 1900000 data\n",
      "Changed 1910000 data\n",
      "Changed 1920000 data\n",
      "Changed 1930000 data\n",
      "Changed 1940000 data\n",
      "Changed 1950000 data\n",
      "Changed 1960000 data\n",
      "Changed 1970000 data\n",
      "Changed 1980000 data\n",
      "Changed 1990000 data\n",
      "Changed 2000000 data\n",
      "Changed 2010000 data\n",
      "Changed 2020000 data\n",
      "Changed 2030000 data\n",
      "Changed 2040000 data\n",
      "Changed 2050000 data\n",
      "Changed 2060000 data\n",
      "Changed 2070000 data\n",
      "Changed 2080000 data\n",
      "Changed 2090000 data\n",
      "Changed 2100000 data\n",
      "Changed 2110000 data\n",
      "Changed 2120000 data\n",
      "Changed 2130000 data\n",
      "Changed 2140000 data\n",
      "Changed 2150000 data\n",
      "Changed 2160000 data\n",
      "Changed 2170000 data\n",
      "Changed 2180000 data\n",
      "Changed 2190000 data\n",
      "Changed 2200000 data\n",
      "Changed 2210000 data\n",
      "Changed 2220000 data\n",
      "Changed 2230000 data\n",
      "Changed 2240000 data\n",
      "Changed 2250000 data\n",
      "Changed 2260000 data\n",
      "Changed 2270000 data\n",
      "Changed 2280000 data\n",
      "Changed 2290000 data\n",
      "Changed 2300000 data\n",
      "Changed 2310000 data\n",
      "Changed 2320000 data\n",
      "Changed 2330000 data\n",
      "Changed 2340000 data\n",
      "Wall time: 1h 11min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "counter = 0\n",
    "df_transformed = df_test.apply(change_to_list_and_clip, axis=1)\n",
    "pickle.dump(df_transformed, open('../dataset/processed/df_test_hubertLin_version.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "embedding_matrix = pickle.load(open('../dataset/processed/embedding_matrix.pkl','rb'))\n",
    "\n",
    "positive_question_pairs = pickle.load(open('../dataset/processed/positive_question_id_pairs.pkl', 'rb'))\n",
    "negative_question_pairs = pickle.load(open('../dataset/processed/negative_question_id_pairs.pkl', 'rb'))\n",
    "\n",
    "qid_question_dict = pickle.load(open('../dataset/processed/qid_question_wrods_list_dict_clipped.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to None if you don't want to do N_gram parsing/training (but you still need to modify the model's structure by yourself)\n",
    "N_gram = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on all data directly. Overcome the imbalance problem by directly assign weights for each class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pos_label_ratio = 0.1746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove validation data from training data\n",
    "def holdout(qid_pair_list, ratio=0.025):\n",
    "    \n",
    "    def gen_suffle_idx(arr):\n",
    "        random.shuffle(arr)\n",
    "        return arr\n",
    "    \n",
    "    holdout_size = int(len(qid_pair_list)*ratio)\n",
    "    idxes = gen_suffle_idx(np.arange(len(qid_pair_list)))\n",
    "    holdout_idxes = idxes[:holdout_size]\n",
    "    train_idxes = idxes[holdout_size:]\n",
    "    \n",
    "    train = qid_pair_list[train_idxes]\n",
    "    val   = qid_pair_list[holdout_idxes]\n",
    "    \n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The validation data is heldout in the data augmenting phase, no need to holdout here\n",
    "\n",
    "# train_pairs, val_pairs = holdout(train_pairs, ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ex: [a,b,c] with N=3 =>  [ED,ED,A], [ED,A,B], [A,B,C], [B,C,ED], [C,ED,ED], where A,B,C,ED are embedded vectors\n",
    "def N_gram_embedding(sample_lists, N):\n",
    "    \n",
    "    sample_size, question_len = sample_lists.shape\n",
    "    sub_seq_len = question_len+N-1\n",
    "    res = np.zeros((sample_size, sub_seq_len, N, 300))\n",
    "\n",
    "    for s in range(sample_size):\n",
    "\n",
    "        # insert ED\n",
    "        res[s,0,0], res[s,0,1], res[s,1,0], res[s,-1,-1], res[s,-1,-2], res[s,-2,-1] = [embedding_matrix[enc_map['<ED>']]]*6\n",
    "\n",
    "        for i,val in enumerate(sample_lists[s]):\n",
    "            for n in range(N):\n",
    "                if i+n<sub_seq_len and N-n-1>-1:\n",
    "                    res[s,i+n,N-n-1] = embedding_matrix[val]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_validation_samples(df, enc_map, N_gram=None):\n",
    "    \n",
    "    def parse_word_list(question):\n",
    "        \n",
    "        if type(question)!=str:\n",
    "            return []\n",
    "\n",
    "        # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "        separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "        single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "        return re.findall(single_word+separator, question)\n",
    "    \n",
    "    # encode single question pair\n",
    "    def enc_question_pair(pair):\n",
    "\n",
    "        # This array is a local cache that identify those non-encoded rare words. \n",
    "        # We'll assign identical index (in this question pair) for each of rare words\n",
    "        rare_words = []\n",
    "\n",
    "        def enc_question(question):\n",
    "\n",
    "            def map_word(word):\n",
    "                if word in enc_map:\n",
    "                    return enc_map[word]\n",
    "                elif word in rare_words:\n",
    "                    idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                    return enc_map['<RARE' + str(idx) + '>']\n",
    "                else:\n",
    "                    rare_words.append(word)\n",
    "                    return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "            word_list = parse_word_list(question)\n",
    "            word_list = [map_word(word) for word in word_list]\n",
    "            \n",
    "            # clip the question length to 30\n",
    "            q_len = len(word_list)\n",
    "            if q_len>=30:\n",
    "                word_list = word_list[:30]\n",
    "            else:\n",
    "                word_list = word_list + [enc_map['<ED>']] * (30-q_len)\n",
    "            \n",
    "            return np.array(word_list)\n",
    "        \n",
    "        return [enc_question(pair[0]) , enc_question(pair[1])]\n",
    "    \n",
    "    X = np.array([enc_question_pair([series['question1'],series['question2']]) for i,series in df.iterrows()])\n",
    "    y = np.array([series['is_duplicate'] for i,series in df.iterrows()])\n",
    "    \n",
    "    if N_gram==None:\n",
    "        return [embedding_matrix[X[:,0]], embedding_matrix[X[:,1]]], y\n",
    "    else:\n",
    "        return [N_gram_embedding(X[:,0],N_gram),N_gram_embedding(X[:,1],N_gram)], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_val = pickle.load(open('../dataset/processed/validation_df.pkl','rb'))\n",
    "val_data = process_validation_samples(df_val, enc_map, N_gram=N_gram)\n",
    "del df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate validation data's class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the class weights to simulate the class disribution of testing samples\n",
    "#\n",
    "# These numbers and formula is got from here: \n",
    "#     https://www.kaggle.com/lystdo/quora-question-pairs/lstm-with-word2vec-embeddings#175198\n",
    "\n",
    "test_set_pos_label_ratio = 0.1746\n",
    "\n",
    "X_val = val_data[0]\n",
    "y_val = val_data[1]\n",
    "validation_pos_ratio = sum(y_val)/len(y_val)\n",
    "\n",
    "val_weights = {\n",
    "    0: (1-test_set_pos_label_ratio) / (1-validation_pos_ratio),\n",
    "    1: test_set_pos_label_ratio/validation_pos_ratio\n",
    "}\n",
    "\n",
    "validation_weights = np.repeat(val_weights[1], len(y_val))\n",
    "validation_weights[y_val==0] = val_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the list of words to list of embedded arrays\n",
    "def enc_question_pairs(qid_pairs, qid_question_dict, enc_map, swap=False):\n",
    "\n",
    "    # encode single question pair\n",
    "    def enc_question_pair(pair):\n",
    "\n",
    "        # This array is a local cache that identify those non-encoded rare words. \n",
    "        # We'll assign identical index (in this question pair) for each of rare words\n",
    "        rare_words = []\n",
    "\n",
    "        def enc_question(qid):\n",
    "\n",
    "            def map_word(word):\n",
    "                if word in enc_map:\n",
    "                    return enc_map[word]\n",
    "                elif word in rare_words:\n",
    "                    idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                    return enc_map['<RARE' + str(idx) + '>']\n",
    "                else:\n",
    "                    rare_words.append(word)\n",
    "                    return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "            question = qid_question_dict[qid]\n",
    "            return [map_word(word) for word in question]\n",
    "        \n",
    "        if swap:\n",
    "            return [enc_question(pair[1]) , enc_question(pair[0])]\n",
    "        else:\n",
    "            return [enc_question(pair[0]) , enc_question(pair[1])]\n",
    "    \n",
    "\n",
    "    return np.array([enc_question_pair(pair) for pair in qid_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate training/validation data\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "1. Random assign a vector for <RARE> tags, this vector values only used in this batch\n",
    "2. Generate positive samples \n",
    "        if same_question_ratio is set:\n",
    "            Random assign partial positive samples as same question pairs (i.e. [qid_A, qid_A] )\n",
    "        the remaining ratio of samples would be picked from pos_pairs\n",
    "3. Generate negative samples \n",
    "        if random_negative_samples_ratio is set:\n",
    "            Random assigning qid pairs as negative sample. (The number of negative samples is decided by \"pos_ratio\" parameter)\n",
    "        the remaining ratio of samples would be picked from neg_pairs\n",
    "3. Encode all question pairs.\n",
    "        The same RARE word in a single question pair is gauranteed to be mapped to same vector.   \n",
    "4. Shuffle the order of all question pairs.\n",
    "5. Embed the question pairs.\n",
    "'''\n",
    "def gen_batch_data(pos_pairs, neg_pairs, batch_size, pos_ratio, swap=False, N_gram=None):\n",
    "    \n",
    "    global embedding_matrix\n",
    "    \n",
    "    all_id_list = list(qid_question_dict.keys())\n",
    "    \n",
    "    def gen_rnd_idx(list_data):\n",
    "        return random.randint(0,len(list_data)-1)\n",
    "    \n",
    "    def random_pick_from(list_data):\n",
    "        return list_data[gen_rnd_idx(list_data)]\n",
    "    \n",
    "    def random_assign_rare_vectors():\n",
    "        rare_idxes = [enc_map['<RARE'+str(i)+'>'] for i in range(30)]\n",
    "        for i in rare_idxes:\n",
    "            embedding_matrix[i] = np.random.rand(300)\n",
    "            \n",
    "    def gen_shuffle_idxes(num):\n",
    "        a = np.arange(num)\n",
    "        random.shuffle(a)\n",
    "        return a\n",
    "    \n",
    "    def gen_positive_qid_pairs(pos_pairs, N):\n",
    "        \n",
    "        N_pos_pairs = [pos_pairs[random.randint(0,pos_pairs.shape[0]-1)] for i in range(N)]\n",
    "        \n",
    "        if same_question_ratio!=0:\n",
    "            same_question_count = round(same_question_ratio*N)\n",
    "            replaced_by_same_question_idxes = [gen_rnd_idx(N_pos_pairs) for i in range(same_question_count)]\n",
    "            for i in replaced_by_same_question_idxes:\n",
    "                rnd_qid = random_pick_from(all_id_list)\n",
    "                N_pos_pairs[i] = [rnd_qid,rnd_qid]\n",
    "        \n",
    "        return N_pos_pairs\n",
    "        \n",
    "    def gen_negative_qid_pairs(neg_pairs, N):\n",
    "        \n",
    "        '''\n",
    "        TODO:\n",
    "            Maybe we still need to check if generated question pair belongs to duplicated question samples.\n",
    "            Although I think this is not very necessay, but we still can give it a try if we have time.\n",
    "        '''\n",
    "        \n",
    "        N_neg_pairs = [random_pick_from(neg_pairs) for i in range(N)]\n",
    "        \n",
    "        if random_negative_samples_ratio!=0:\n",
    "            random_sample_count = round(random_negative_samples_ratio*N)\n",
    "            replaced_by_random_idxes = [gen_rnd_idx(N_neg_pairs) for i in range(random_sample_count)]\n",
    "            for i in replaced_by_random_idxes:\n",
    "                N_neg_pairs[i] = [random_pick_from(all_id_list),random_pick_from(all_id_list)]\n",
    "        \n",
    "        return N_neg_pairs\n",
    "    \n",
    "    # gaurantee <RARE> would be different in each batch\n",
    "    random_assign_rare_vectors()\n",
    "    \n",
    "    # generate negative samples to match the given pos:neg ratio\n",
    "    \n",
    "    pos_count = int(batch_size*training_pos_ratio)\n",
    "    neg_count = int( ((1-pos_ratio)/pos_ratio) * pos_count )\n",
    "    \n",
    "    pos_pairs = gen_positive_qid_pairs(pos_pairs, pos_count)\n",
    "    neg_pairs = gen_negative_qid_pairs(neg_pairs, neg_count)\n",
    "    \n",
    "    pos = enc_question_pairs(pos_pairs, qid_question_dict, enc_map, swap=swap)\n",
    "    neg = enc_question_pairs(neg_pairs, qid_question_dict, enc_map, swap=swap)\n",
    "    X = np.vstack([pos,neg])\n",
    "    \n",
    "    # create y\n",
    "    \n",
    "    y = np.zeros(X.shape[0])\n",
    "    y[:pos.shape[0]] = 1\n",
    "    \n",
    "    # shuffle\n",
    "    \n",
    "    shuffle_idxes = gen_shuffle_idxes(y.shape[0])\n",
    "    X = X[shuffle_idxes]\n",
    "    y = y[shuffle_idxes]\n",
    "    \n",
    "    # embed X\n",
    "    if N_gram==None:\n",
    "        embed_q1 = embedding_matrix[X[:,0,:]]\n",
    "        embed_q2 = embedding_matrix[X[:,1,:]]\n",
    "    else:\n",
    "        embed_q1 = N_gram_embedding(X[:,0,:],N_gram)\n",
    "        embed_q2 = N_gram_embedding(X[:,1,:],N_gram)\n",
    "        \n",
    "    weights = np.zeros(y.shape[0])\n",
    "    weights[y==0] = training_weights[0]\n",
    "    weights[y==1] = training_weights[1]\n",
    "    \n",
    "    return [embed_q1,embed_q2], y, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the class weights to simulate the class disribution of testing samples\n",
    "#\n",
    "# These numbers and formula is got from here: \n",
    "#     https://www.kaggle.com/lystdo/quora-question-pairs/lstm-with-word2vec-embeddings#175198\n",
    "\n",
    "test_set_pos_label_ratio = 0.1746\n",
    "training_pos_ratio = 0.5\n",
    "\n",
    "same_question_ratio = 0.1 # a training hyper paramter to add testcases like [A,A] \n",
    "random_negative_samples_ratio = 0.01\n",
    "\n",
    "training_weights = {\n",
    "    0: (1-test_set_pos_label_ratio) / (1-training_pos_ratio),\n",
    "    1: test_set_pos_label_ratio/training_pos_ratio\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def batch_generator(pos_pairs, neg_pairs, switch_position=False):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if switch_position and random.randint(0,1)==0:\n",
    "            switch_position = True\n",
    "        \n",
    "        yield gen_batch_data(pos_pairs, neg_pairs, batch_size, training_pos_ratio, swap=switch_position, N_gram=N_gram)\n",
    "        \n",
    "\n",
    "# callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n",
    "# ]\n",
    "    \n",
    "# try:\n",
    "    \n",
    "#     model.fit_generator(batch_generator(positive_question_pairs, negative_question_pairs, switch_position=True),\n",
    "#                         steps_per_epoch=50,\n",
    "#                         class_weight=weights,\n",
    "#                         epochs=1000,\n",
    "#                         validation_data=(X_val,y_val,validation_weights), \n",
    "#                         callbacks=callbacks)\n",
    "    \n",
    "# except KeyboardInterrupt:\n",
    "#     print('\\nEarly stopped by user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'final_sigmoid:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import GRUCell, DropoutWrapper, MultiRNNCell\n",
    "\n",
    "# clear all preset variables and scopes\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# hyper-parameters that should be passed as function argument\n",
    "\n",
    "vocab_size = len(enc_map)\n",
    "vocab_dim = 300 # embedded by GloVe with dimension=300\n",
    "text_length = 30 # how many words are we going to feed in one time\n",
    "time_stamps = text_length + N_gram - 1\n",
    "\n",
    "# hyper-params\n",
    "\n",
    "RNN_cell = GRUCell # or, LSTMCell\n",
    "\n",
    "meaning_feature_size = 128\n",
    "fc1_size = 32\n",
    "\n",
    "semantic_compressor_num_layers = 1\n",
    "semantic_compressor_dropout_rate = 0.05\n",
    "\n",
    "meaning_analyzer_num_layers = 1\n",
    "meaning_analyzer_dropout_rate = 0.05\n",
    "\n",
    "fc1_dropout_rate = 0.05\n",
    "fc2_dropout_rate = 0.05\n",
    "\n",
    "noise_std = 0.001\n",
    "\n",
    "# start build model\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "q1 = tf.placeholder(tf.float32, shape=[None, time_stamps, N_gram, vocab_dim], name='q1')\n",
    "q2 = tf.placeholder(tf.float32, shape=[None, time_stamps, N_gram, vocab_dim], name='q2')\n",
    "ground_truth_labels = tf.placeholder(tf.float32, shape=[None, 1], name='GT')\n",
    "class_weights = tf.placeholder(tf.float32, shape=[None, 1], name='sample_weights')\n",
    "\n",
    "# add noise\n",
    "\n",
    "noise = tf.random_normal(shape=tf.shape(q1), mean=0.0, stddev=noise_std) \n",
    "noised1 = noise + q1\n",
    "noised2 = noise + q2\n",
    "\n",
    "reshaped1 = tf.reshape(noised1, shape=[-1, time_stamps, N_gram*vocab_dim], name='reshape_input_1')\n",
    "reshaped2 = tf.reshape(noised2, shape=[-1, time_stamps, N_gram*vocab_dim], name='reshape_input_2')\n",
    "\n",
    "# compress the N-gram vector back to 1-word vector (i.e. 300 dimensions)\n",
    "\n",
    "def semantic_compressor(inputs):\n",
    "    with tf.variable_scope(\"semantic_compressor\"):\n",
    "        dropout_rate = tf.Variable(semantic_compressor_dropout_rate, name='semantic_rnn_dropout')\n",
    "        semantic_compressor_cell = RNN_cell(vocab_dim)\n",
    "        semantic_compressor_cell = DropoutWrapper(semantic_compressor_cell, output_keep_prob=dropout_rate)\n",
    "        semantic_compressor_cells = MultiRNNCell([semantic_compressor_cell] * semantic_compressor_num_layers)\n",
    "        output, state = tf.nn.dynamic_rnn(semantic_compressor_cells, inputs, dtype=tf.float32)\n",
    "#         tf.summary.histogram('semantic_compressor_cells', semantic_compressor_cells) # TODO: find a way to visualize RNN weights\n",
    "        tf.summary.histogram('semantic_compressor_output', output)\n",
    "        return output\n",
    "\n",
    "with tf.variable_scope(\"compress_inputs\") as scope:\n",
    "    compressed_input1 = semantic_compressor(reshaped1)\n",
    "    scope.reuse_variables()\n",
    "    compressed_input2 = semantic_compressor(reshaped2)\n",
    "    \n",
    "    tf.summary.histogram('compressed_input1', compressed_input1)\n",
    "    tf.summary.histogram('compressed_input2', compressed_input2)\n",
    "\n",
    "# concat the two questions and use RNN extract its meaning features\n",
    "\n",
    "with tf.variable_scope(\"analyze_meaning_features\"):\n",
    "    concat_inputs = tf.concat([compressed_input1, compressed_input2], axis=2, name='concat_compressed_inputs')\n",
    "\n",
    "    with tf.variable_scope(\"meaning_analyzer\"):\n",
    "        meaning_analyzer_cell = RNN_cell(meaning_feature_size)\n",
    "        meaning_analyzer_cell = DropoutWrapper(meaning_analyzer_cell, output_keep_prob=meaning_analyzer_dropout_rate)\n",
    "        meaning_analyzer_cells = MultiRNNCell([meaning_analyzer_cell] * meaning_analyzer_num_layers)\n",
    "        compressed_meaning, state = tf.nn.dynamic_rnn(meaning_analyzer_cells, concat_inputs, dtype=tf.float32)\n",
    "        \n",
    "    # we don't care about the sequences distributed over time returned by RNN, just want the last time stamp\n",
    "\n",
    "    def get_last_time_stamp(inputs):\n",
    "        inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "        last = tf.gather(inputs, int(inputs.get_shape()[0]) - 1)\n",
    "        return last\n",
    "        \n",
    "    compressed_meaning = get_last_time_stamp(compressed_meaning)\n",
    "    \n",
    "#     tf.summary.histogram('meaning_analyzer_cells', meaning_analyzer_cells) # TODO: find a way to visualize RNN weights\n",
    "    tf.summary.histogram('compressed_meaning', compressed_meaning)\n",
    "\n",
    "# fc1\n",
    "with tf.variable_scope(\"fc1\"):\n",
    "\n",
    "    fc_serial = 1\n",
    "    prev_tensor = compressed_meaning\n",
    "\n",
    "    fc1 = tf.contrib.layers.batch_norm(prev_tensor)\n",
    "#     fc1 = tf.nn.relu(fc1, name='pre_act_fc'+str(fc_serial))\n",
    "    fc1 = tf.layers.dropout(fc1, fc1_dropout_rate)\n",
    "    fc1 = tf.contrib.layers.fully_connected(fc1, fc1_size)\n",
    "    \n",
    "    tf.summary.histogram('fc1_result', fc1)\n",
    "\n",
    "# fc2\n",
    "with tf.variable_scope(\"fc2\"):\n",
    "\n",
    "    fc_serial = 2\n",
    "    prev_tensor = fc1\n",
    "\n",
    "    fc2 = tf.contrib.layers.batch_norm(prev_tensor)\n",
    "#     fc2 = tf.nn.relu(fc2, name='pre_act_fc'+str(fc_serial))\n",
    "    fc2 = tf.layers.dropout(fc2, fc2_dropout_rate)\n",
    "    fc2 = tf.contrib.layers.fully_connected(fc2, 1)\n",
    "\n",
    "out = tf.sigmoid(fc2, name='sigmoid')\n",
    "tf.summary.histogram('final_sigmoid', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'log_loss_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean( #reduce_logsumexp\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=out, labels=ground_truth_labels, name='sigmoid_cross_entropy'))\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('log_loss'):\n",
    "    log_loss = tf.reduce_mean(tf.losses.log_loss(ground_truth_labels, out, weights=class_weights))\n",
    "tf.summary.scalar('log_loss', log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_summary = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter('./tmp/test/0')\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at  0 :  0.93714\n",
      "Training loss at  30 :  0.867157\n",
      "Training loss at  60 :  0.742115\n",
      "Training loss at  90 :  0.718826\n",
      "Training loss at  120 :  0.711182\n",
      "Training loss at  150 :  0.698874\n",
      "Training loss at  180 :  0.697731\n",
      "Training loss at  210 :  0.697504\n",
      "Training loss at  240 :  0.69692\n",
      "Training loss at  270 :  0.695463\n",
      "Training loss at  300 :  0.693158\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-0cc21f5dca26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mclass_weights\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     }\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "steps_pre_epoch = 1\n",
    "\n",
    "training_generator = batch_generator(positive_question_pairs, negative_question_pairs, switch_position=True)\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # training\n",
    "\n",
    "    batch = next(training_generator)\n",
    "    feed_dict={\n",
    "        q1: batch[0][0], \n",
    "        q2: batch[0][1], \n",
    "        ground_truth_labels: batch[1].reshape(-1,1), \n",
    "        class_weights: batch[2].reshape(-1,1)\n",
    "    }\n",
    "    _, loss_val = sess.run([train_step, log_loss], feed_dict=feed_dict)\n",
    "\n",
    "    if i%30==0:\n",
    "        print('Training loss at ', i, ': ', loss_val)\n",
    "    \n",
    "    # write to tensor board\n",
    "    \n",
    "    if i%5==0 and i!=0:\n",
    "        summary = sess.run(merge_summary, feed_dict=feed_dict)\n",
    "        writer.add_summary(summary,i)\n",
    "    \n",
    "    if i%30==0 and i!=0:\n",
    "        pass\n",
    "#         # validation \n",
    "#         val_dict={\n",
    "#             q1: X_val[0][100],\n",
    "#             q2: X_val[1][100], \n",
    "#             ground_truth_labels: y_val.reshape(-1,1)[100], \n",
    "#             class_weights: validation_weights.reshape(-1,1)[100]\n",
    "#         }\n",
    "#         val_log_loss = sess.run([log_loss], feed_dict=val_dict)\n",
    "\n",
    "\n",
    "#         print('validation log_loss=', val_log_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the ROC_AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "pred = model.predict(X_val)\n",
    "\n",
    "# pred_bin = np.zeros(len(pred))\n",
    "# pred_bin[(pred>0.5).reshape(len(pred))] = 0.999999\n",
    "# pred_bin[(pred<=0.5).reshape(len(pred))] = 0.000001\n",
    "\n",
    "pred[pred>0.999] = 0.999999\n",
    "pred[pred<=0.001] = 0.000001\n",
    "\n",
    "print('log_loss=',log_loss(y_val, pred, sample_weight=validation_weights))\n",
    "\n",
    "print('roc_auc=',roc_auc_score(y_val, pred, sample_weight=validation_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_name = 'HubertLin_naive_LSTM_dynamic_training_tri_earlystop'\n",
    "# model.save('../model/'+model_name+'.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customized testcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('../model/HubertLin_naive_LSTM_alot_dropouts.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question_pair(question_pair_list, enc_map, capsulate_st_ed=False):\n",
    "\n",
    "    def enc_question(question):\n",
    "\n",
    "        if type(question)!=str or question==\"\":\n",
    "            if capsulate_st_ed:\n",
    "                return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def map_wrod(word):\n",
    "            if word in enc_map:\n",
    "                return enc_map[word]\n",
    "            elif word in rare_words:\n",
    "                idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                return enc_map['<RARE' + str(idx) + '>']\n",
    "            else:\n",
    "                rare_words.append(word)\n",
    "                return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "        # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "        separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "        single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "        words_list = re.findall(single_word+separator, question)\n",
    "\n",
    "        if capsulate_st_ed:\n",
    "            return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "        else:\n",
    "            return [map_wrod(word) for word in words_list]\n",
    "        \n",
    "    # This array is a local cache that identify those non-encoded rare words. \n",
    "    # We'll assign identical index (in this question pair) for each of rare words\n",
    "    rare_words = []\n",
    "    \n",
    "    return enc_question(question_pair_list[0]) , enc_question(question_pair_list[1])\n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]\n",
    "\n",
    "def clip_length(data, clip_st_ed=False):\n",
    "    \n",
    "    clip_len = 30\n",
    "    \n",
    "    if clip_st_ed:\n",
    "        data[0] = eval(data[0])[1:-1]\n",
    "        data[1] = eval(data[1])[1:-1]\n",
    "    \n",
    "    if len(data[0])<clip_len:\n",
    "        data[0] = data[0] + [enc_map['<ED>']] * (30-len(data[0]))\n",
    "    else:\n",
    "        data[0] = data[0][:30]\n",
    "        \n",
    "    if len(data[1])<clip_len:\n",
    "        data[1] = data[1] + [enc_map['<ED>']] * (30-len(data[1]))\n",
    "    else:\n",
    "        data[1] = data[1][:30]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a good man <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "A good man is me <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.20090792]]\n",
      "\n",
      "The <RARE0> is totally fucking awful <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "The <RARE0> is the best thing I have ever seen <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.2374358]]\n",
      "\n",
      "The <RARE0> is the best thing I have ever seen <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "The <RARE0> is totally fucking awful <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.22899517]]\n",
      "\n",
      "How <RARE0> can be used in cooking <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Can we find <RARE1> on the moon <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.26953128]]\n",
      "\n",
      "A bird is flying in the sky <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "My bird is flying over the ocean <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.27302998]]\n",
      "\n",
      "What is your favorite food <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Which is your favorite one <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.27969882]]\n",
      "\n",
      "<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "<RARE6> <RARE7> <RARE8> <RARE9> <RARE10> <RARE11> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.0737754]]\n",
      "\n",
      "<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.07156763]]\n",
      "\n",
      "what a <RARE0> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "what a <RARE0> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.16010971]]\n",
      "\n",
      "what a <RARE0> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "what a <RARE1> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.18313986]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "\n",
    "testcases = [\n",
    "    {\n",
    "        'q1': 'I am a good man',\n",
    "        'q2': 'A good man is me'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'The <RARE0> is totally fucking awful',\n",
    "        'q2': 'The <RARE0> is the best thing I have ever seen'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'The <RARE0> is the best thing I have ever seen',\n",
    "        'q2': 'The <RARE0> is totally fucking awful',\n",
    "    },\n",
    "    {\n",
    "        'q1': 'How <RARE0> can be used in cooking',\n",
    "        'q2': 'Can we find <RARE1> on the moon'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'A bird is flying in the sky',\n",
    "        'q2': 'My bird is flying over the ocean'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'What is your favorite food',\n",
    "        'q2': 'Which is your favorite one'\n",
    "    },\n",
    "    {\n",
    "        'q1': '<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> ',\n",
    "        'q2': '<RARE6> <RARE7> <RARE8> <RARE9> <RARE10> <RARE11> ',\n",
    "    },\n",
    "    {\n",
    "        'q1': '<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> ',\n",
    "        'q2': '<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> ',\n",
    "    },\n",
    "    {\n",
    "        'q1': ' what a <RARE0> program I have never seen such beatiful code ever',\n",
    "        'q2': ' what a <RARE0> program I have never seen such beatiful code ever',\n",
    "    },\n",
    "    {\n",
    "        'q1': ' what a <RARE0> program I have never seen such beatiful code ever',\n",
    "        'q2': ' what a <RARE1> program I have never seen such beatiful code ever',\n",
    "    },\n",
    "]\n",
    "\n",
    "for testcase in testcases:\n",
    "    \n",
    "    enc1, enc2 = enc_question_pair([testcase['q1'],testcase['q2']], enc_map)\n",
    "    enc1, enc2 = clip_length([enc1,enc2])\n",
    "    print(' '.join(dec_question(enc1,dec_map)))\n",
    "    print(' '.join(dec_question(enc2,dec_map)))\n",
    "\n",
    "    if N_gram==None:\n",
    "        emb1 = embedding_matrix[np.array([enc1])]\n",
    "        emb2 = embedding_matrix[np.array([enc2])]\n",
    "    else:\n",
    "        emb1 = N_gram_embedding(np.array([enc1]), N_gram)\n",
    "        emb2 = N_gram_embedding(np.array([enc2]), N_gram)\n",
    "\n",
    "    pred = model.predict([emb1,emb2])\n",
    "    print('Predicting is same question proba =', pred)\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to predict some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pickle.load(open('../dataset/processed/df_test_hubertLin_version.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'customer', 'lifetime', '<RARE0>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "['What', 'is', 'customer', 'lifetime', 'value', 'in', '<RARE1>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "\n",
      "Predicting is same question proba = [[ 0.60896122]]\n"
     ]
    }
   ],
   "source": [
    "a = df_test.ix[186461]\n",
    "q1 = dec_question(a['question1'], dec_map)\n",
    "q2 = dec_question(a['question2'], dec_map)\n",
    "print(q1)\n",
    "print(q2)\n",
    "\n",
    "emb1 = embedding_matrix[np.array([a['question1']])]\n",
    "emb2 = embedding_matrix[np.array([a['question2']])]\n",
    "pred = model.predict([emb1, emb2])\n",
    "print('\\nPredicting is same question proba =', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "try:\n",
    "    model==None\n",
    "except:\n",
    "    model = load_model('../model/'+model_name+'.model')\n",
    "\n",
    "df_test = pickle.load(open('../dataset/processed/df_test_hubertLin_version.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 2345796\n",
      "100000 / 2345796\n",
      "200000 / 2345796\n",
      "300000 / 2345796\n",
      "400000 / 2345796\n",
      "500000 / 2345796\n",
      "600000 / 2345796\n",
      "700000 / 2345796\n",
      "800000 / 2345796\n",
      "900000 / 2345796\n",
      "1000000 / 2345796\n",
      "1100000 / 2345796\n",
      "1200000 / 2345796\n",
      "1300000 / 2345796\n",
      "1400000 / 2345796\n",
      "1500000 / 2345796\n",
      "1600000 / 2345796\n",
      "1700000 / 2345796\n",
      "1800000 / 2345796\n",
      "1900000 / 2345796\n",
      "2000000 / 2345796\n",
      "2100000 / 2345796\n",
      "2200000 / 2345796\n",
      "2300000 / 2345796\n",
      "Wall time: 48min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "partition_size = 10000\n",
    "\n",
    "def predict(i,q):\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        print(i*partition_size, '/', len(df_test))\n",
    "    \n",
    "    x1 = np.array(list(q['question1']))\n",
    "    x2 = np.array(list(q['question2']))\n",
    "    \n",
    "    x1 = embedding_matrix[x1]\n",
    "    x2 = embedding_matrix[x2]\n",
    "    \n",
    "    return model.predict([x1,x2])\n",
    "\n",
    "partition_len = len(df_test)//partition_size +1\n",
    "result = [predict(i,df_test.iloc[i*partition_size:(i+1)*partition_size]) for i in range(partition_len)]\n",
    "con = np.concatenate(result)\n",
    "df_result = pd.DataFrame({'test_id':np.arange(len(con)),'is_duplicate':con.reshape(len(con))}, columns=['test_id','is_duplicate'])\n",
    "df_result.to_csv('../result/prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction success\n"
     ]
    }
   ],
   "source": [
    "if len(df_result)!=2345796:\n",
    "    print('Your result prediction count is not fit to the testing data length 2345796 , yours:', len(df_result))\n",
    "else:\n",
    "    print('Prediction success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can a vacuum cleaner concentrate suck your eye out if it is pressed against your <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Could a vacuum cleaner suck get your eye out if directly pressed on the <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.813005\n",
      "\n",
      "Is web development just building <RARE0> best you get a web developer job if you know how to make a <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Is web development just building <RARE0> Can you get a backend web developer job if you know how year make a <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.966372\n",
      "\n",
      "How do I overcome my shyness with <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "How do you overcome being <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.865856\n",
      "\n",
      "If I like a comment to a computer post by a friend on <RARE0> who will be able to see this <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "I want the public to see my posts on Facebook but only my friends to comment and <RARE2> How can I do <RARE3> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.884993\n",
      "\n",
      "What die good gifts for a foreign visitor to bring when <RARE0> invited to <RARE1> home in Australia for the first <RARE2> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "What are good gifts for a foreign visitor to bring when <RARE0> invited to <RARE1> home in Malta for the yes <RARE2> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.881261\n",
      "\n",
      "How can I speak English fluently and <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "How can I speak english naturally <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.832783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "over = df_test[df_result['is_duplicate']>0.8]\n",
    "\n",
    "c = 0\n",
    "for i,s in over.iterrows():\n",
    "    print(' '.join(dec_question(s['question1'],dec_map)))\n",
    "    print(' '.join(dec_question(s['question2'],dec_map)))\n",
    "    print(df_result.ix[i]['is_duplicate'])\n",
    "    print('')\n",
    "    c+=1\n",
    "    if c>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
