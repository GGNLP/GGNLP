{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new approach for blending\n",
    "\n",
    "###### Why not simply stack the models?\n",
    "\n",
    "We made a huge mistake in stage 1 model training phase: all the model shares exactly same training set, which leads to extreme over-fitting problem when we stacks the models and getting poor generalizabilty. \n",
    "\n",
    "###### What does this algorithm do?\n",
    "\n",
    "Very simple, we split the original validation set (240000) into two parts: new training set (168000) and new validation set (72000). Use the new training set to do stacking and new validation set for validation. Which has following pros and cons:\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1. Out-of-bound training data:\n",
    "        Our model overall takes more training data then original. \n",
    "2. Better generalizability:\n",
    "        And the out-of-bound training data makes the stage 1 models not overfits that much, which finally gives us a very well result.\n",
    "3. Can use strong classifiers for stage two training:\n",
    "        We uses Xgboost as our stage 2 model, and yeilds an extra 1.3% boost on private log-loss score. (0.15380 -> 0.15172)\n",
    "        \n",
    "**Cons:**\n",
    "\n",
    "1. Risky:\n",
    "        The new training data for stage two is relatively small. Since we luckily splited a very large validation set at first, so the blending works very well. But one should consider this approach is very risky in generalizability.\n",
    "\n",
    "----\n",
    "\n",
    "## 新的 blending 作法\n",
    "\n",
    "###### 為什麼我們不直接做 stacking ?\n",
    "\n",
    "我們在 stage 1 model 的 training 上犯了很嚴重的錯誤：「我們所有的 model 都使用了同樣的 training set ，這使得 stack 起來時的 model 在 training set 上非常嚴重的 overfit ，因此也失去了 generalizability 。」\n",
    "\n",
    "###### 這個算法做了什麼？\n",
    "\n",
    "非常簡單：「我們將原本的 validation set (240000) 再次切分成新的 training data (168000) 與新的 validation data (72000) ，使用新的 training data 訓練 stage 2 的 model ，並使用新的 validation data 做 validation 」，這樣的做法有以下的優勢與缺陷：\n",
    "\n",
    "**優勢**\n",
    "\n",
    "1. Out-of-bound training data:\n",
    "        總體而言， model 使用到了比原先預計更多的 training data。\n",
    "2. 更好的 generalizability:\n",
    "        因為使用了 Out-of-bound training data ，這使得 stage 1 的 training data 相對 overfit 得較不嚴重。\n",
    "3. stage 2 model 可以使用較強的分類器：\n",
    "        我們使用了 Xgboost 作為 stage 2 的 model ，並最終得到了 1.3% 的 private log-loss 優化 (0.15380 -> 0.15172) 。\n",
    "\n",
    "**缺陷**\n",
    "\n",
    "1. 風險：\n",
    "        這個做法的風險非常高，主要是因為我們一開始所切的 validation set 相對很大才能成功，若是一開始的 validation set 非常小，這樣的作法並不能保障 generalizability ，也就是可能會在新的 training set 上面嚴重 overfit。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_idxes = pickle.load(open('./lystdo_kernel/validation_index.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../dataset/quora-question-pairs/train.csv')['is_duplicate']\n",
    "# df_test = pd.read_csv('../dataset/quora-question-pairs/test.csv')\n",
    "\n",
    "df_val = df_train.ix[val_idxes]\n",
    "df_train = df_train.drop(val_idxes)\n",
    "\n",
    "y_train = df_train.as_matrix()\n",
    "y_val = df_val.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blending_train_ratio = 0.7\n",
    "\n",
    "np.random.seed(9487)\n",
    "\n",
    "perm = np.random.permutation(len(val_idxes))\n",
    "blending_train_size = int(len(val_idxes)*blending_train_ratio)\n",
    "\n",
    "blending_train_idx = val_idxes[perm[:blending_train_size]]\n",
    "blending_val_idx = val_idxes[perm[blending_train_size:]]\n",
    "\n",
    "y_blending_train = df_val.ix[blending_train_idx].as_matrix()\n",
    "y_blending_val = df_val.ix[blending_val_idx].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the class-weight will apply to the training model\n",
    "def get_pos_neg_weights(data, as_array=False):\n",
    "    test_set_pos_label_ratio = 0.1746\n",
    "    m_pos_ratio = sum(data)/len(data)\n",
    "    weight = {\n",
    "        0: (1-test_set_pos_label_ratio) / (1-m_pos_ratio),\n",
    "        1: test_set_pos_label_ratio/m_pos_ratio\n",
    "    }\n",
    "    \n",
    "    if not as_array:\n",
    "        return weight\n",
    "    else:\n",
    "        return np.array([weight[v] for v in data])\n",
    "\n",
    "\n",
    "val_weight = get_pos_neg_weights(y_val, as_array=True)\n",
    "train_weight = get_pos_neg_weights(y_train, as_array=True)\n",
    "\n",
    "blending_val_weight = get_pos_neg_weights(y_blending_val, as_array=True)\n",
    "blending_train_weight = get_pos_neg_weights(y_blending_train, as_array=True)\n",
    "\n",
    "y_all_train = pd.read_csv('../dataset/quora-question-pairs/train.csv')['is_duplicate'].as_matrix()\n",
    "all_train_weight = get_pos_neg_weights(y_all_train, as_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Cache\n",
    "\n",
    "Since calculating data is time consuming, so we did some caching for these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''no correction , use GloVe'''\n",
    "# data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./cache.pkl','rb'))\n",
    "# model_name = 'glove_without_word_correction'\n",
    "\n",
    "# '''correction , use GloVe'''\n",
    "# data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./cache_text_correction.pkl','rb'))\n",
    "# model_name = 'glove_with_word_correction'\n",
    "\n",
    "\n",
    "'''no correction , use fasttext , 0.1592'''\n",
    "data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./cache_fasttext.pkl','rb'))\n",
    "model_name = 'fasttext_without_word_correction'\n",
    "\n",
    "# '''correction , use fasttext , 0.1602'''\n",
    "# data_1,data_2,labels,test_data_1,test_data_2,test_ids,embedding_matrix, nb_words = pickle.load(open('./cache_fasttext_text_correction.pkl','rb'))\n",
    "# model_name = 'fasttext_with_word_correction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def embed_datas(datas):\n",
    "#     print('need test')\n",
    "#     res = []\n",
    "#     for i,data in enumerate(datas):\n",
    "#         res.append([embedding_matrix[v] for v in data])\n",
    "#     return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all features\n",
    "\n",
    "Select which stage one model will be chosen. These can be considered as a kind of hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Add extra features to leak\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "is_all_features_in_single_dense = False\n",
    "\n",
    "feature_files = [\n",
    "#     'lystdo_correctwords_lstm',\n",
    "#     'lystdo_1234_loss017',\n",
    "    'Abhishek_features',\n",
    "    'magic_feature',\n",
    "    'magic_feature_v1',\n",
    "    'Howard_feature',\n",
    "    'HubertLin_features_raw',\n",
    "    'HubertLin_features_simple_tokenizer',\n",
    "    'HubertLin_features_word_corrected',\n",
    "    'pagerank',\n",
    "    'word_match_share',\n",
    "    'fasttext_distance',\n",
    "#     'modified_count',\n",
    "    'magic_v25_qid',\n",
    "    'k_scrore',\n",
    "    'pos_dist',\n",
    "    'dep_dist',\n",
    "]\n",
    "\n",
    "def inf_nan_to_zero(arr):\n",
    "    nan = np.isnan(arr)\n",
    "    inf = np.isinf(arr)\n",
    "    arr[nan] = 0\n",
    "    arr[inf] = 0\n",
    "    return arr\n",
    "\n",
    "def standardize(train,test):\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((train, test)))\n",
    "    train = ss.transform(train)\n",
    "    test = ss.transform(test)\n",
    "    return train, test\n",
    "\n",
    "print('Loading original leaks')\n",
    "leaks,test_leaks = pickle.load(open('./lystdo_kernel/leaks.pkl','rb'))\n",
    "leaks,test_leaks = standardize(leaks,test_leaks)\n",
    "if not is_all_features_in_single_dense:\n",
    "    leaks = [leaks]\n",
    "    test_leaks = [test_leaks]\n",
    "\n",
    "for feature_file in feature_files:\n",
    "    \n",
    "    try:\n",
    "        print('Loading '+feature_file)\n",
    "        train_features,test_features = pickle.load(open('./lystdo_kernel/ss_cache/'+feature_file+'.pkl' , 'rb'))\n",
    "    except:\n",
    "        train_features = pd.read_csv('./features_from_model/train/'+feature_file+'.csv').as_matrix()\n",
    "        test_features = pd.read_csv('./features_from_model/test/'+feature_file+'.csv').as_matrix()\n",
    "        train_features = inf_nan_to_zero(train_features)\n",
    "        test_features = inf_nan_to_zero(test_features)\n",
    "\n",
    "        train_features,test_features = standardize(train_features,test_features)\n",
    "        pickle.dump([train_features,test_features], open('./lystdo_kernel/ss_cache/'+feature_file+'.pkl' , 'wb'))\n",
    "\n",
    "    if is_all_features_in_single_dense:\n",
    "        leaks = np.hstack([leaks,train_features])\n",
    "        test_leaks = np.hstack([test_leaks,test_features])\n",
    "    else:\n",
    "        leaks.append(train_features)\n",
    "        test_leaks.append(test_features)\n",
    "        \n",
    "# # wrap all features up into one singe fc layer\n",
    "# if is_all_features_in_single_dense:\n",
    "#     leaks = [leaks]\n",
    "#     test_leaks = [test_leaks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leaks = np.hstack(leaks)\n",
    "test_leaks = np.hstack(test_leaks)\n",
    "\n",
    "blending_val_leaks = leaks[blending_val_idx,:]\n",
    "blending_train_leaks = leaks[blending_train_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select which leaks data is used for training and the remainings are validation data.\n",
    "'''\n",
    "\n",
    "def select_leaks(leaks, idxes, stack):\n",
    "    ret = []\n",
    "    for leak in leaks:\n",
    "        if stack:\n",
    "            ret.append(np.vstack([leak[idxes],leak[idxes]]))\n",
    "        else:\n",
    "            ret.append(leak[idxes])\n",
    "    return ret\n",
    "\n",
    "stack = True\n",
    "\n",
    "if stack:\n",
    "#     data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "#     data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "    leaks_train = select_leaks(leaks,idx_train,stack)\n",
    "    labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "#     data_1_val = data_1[idx_val]\n",
    "#     data_2_val = data_2[idx_val]\n",
    "    leaks_val = select_leaks(leaks,idx_val,stack=False)\n",
    "    labels_val = labels[idx_val]\n",
    "    \n",
    "    if use_prev_model_as_helper:\n",
    "        val_helpers = [train_helpers[0][idx_val]]\n",
    "        train_helpers = [np.vstack([train_helpers[0][idx_train],train_helpers[0][idx_train]])]\n",
    "else:\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    leaks_train = select_leaks(leaks,idx_train,stack)\n",
    "    labels_train = labels[idx_train]\n",
    "    \n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    leaks_val = select_leaks(leaks,idx_val,stack)\n",
    "    labels_val = labels[idx_val]\n",
    "    \n",
    "    if use_prev_model_as_helper:\n",
    "        val_helpers = [train_helpers[0][idx_val]]\n",
    "        train_helpers = [train_helpers[0][idx_train]]\n",
    "    \n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previous model predictions\n",
    "\n",
    "Use some blended models' prediction as next blending model's input, this works not very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading : lystdo_onlyAB_0.1704_prediction_max\n",
      "Loading : lystdo_text_corrected_0.1719_prediction_mean\n",
      "Loading : lystdo_full_features_and_poolings_0.1806_prediction_max\n",
      "Loading : lystdo_Fasttext_AllFeatures_WordCorrection_Loss_0.1609_0.1609_prediction_mean\n",
      "Loading : lystdo_original_fullfeature_prev_lstm_0.1739_prediction_mean\n",
      "Loading : lystdo_onlyAB_0.1704_prediction_mean\n",
      "Loading : lystdo_full_features_0.1744_prediction_max\n",
      "Loading : lystdo_original_fullfeature_prev_lstm_0.1739_prediction_max\n",
      "Loading : lystdo_text_corrected_0.1719_prediction_max\n",
      "Loading : lystdo_full_features_0.1744_prediction_mean\n"
     ]
    }
   ],
   "source": [
    "use_prev_model_as_helper = False\n",
    "\n",
    "helpers = [\n",
    "    'lystdo_onlyAB_0.1704_prediction_max',\n",
    "    'lystdo_text_corrected_0.1719_prediction_mean',\n",
    "    'lystdo_full_features_and_poolings_0.1806_prediction_max',\n",
    "    \n",
    "    'lystdo_Fasttext_AllFeatures_WordCorrection_Loss_0.1609_0.1609_prediction_mean',\n",
    "    'lystdo_original_fullfeature_prev_lstm_0.1739_prediction_mean',\n",
    "    'lystdo_onlyAB_0.1704_prediction_mean',\n",
    "\n",
    "    'lystdo_full_features_0.1744_prediction_max', # 14942\n",
    "    \n",
    "    'lystdo_original_fullfeature_prev_lstm_0.1739_prediction_max',\n",
    "    'lystdo_text_corrected_0.1719_prediction_max',\n",
    "    'lystdo_full_features_0.1744_prediction_mean',\n",
    "    \n",
    "#     'Howard_xgb'\n",
    "    \n",
    "#     ...QWQ\n",
    "]\n",
    "\n",
    "# 0.14963\n",
    "#     'lystdo_full_features_0.1744_prediction_max',\n",
    "#     'lystdo_onlyAB_0.1704_prediction_max',\n",
    "#     'lystdo_text_corrected_0.1719_prediction_mean',\n",
    "#     'lystdo_full_features_and_poolings_0.1806_prediction_max',\n",
    "    \n",
    "#     'lystdo_Fasttext_AllFeatures_WordCorrection_Loss_0.1609_0.1609_prediction_mean',\n",
    "#     'lystdo_original_fullfeature_prev_lstm_0.1739_prediction_mean',\n",
    "#     'lystdo_onlyAB_0.1704_prediction_mean',\n",
    "\n",
    "train_tmp = []\n",
    "test_tmp = []\n",
    "\n",
    "def read_csv(file, is_train):\n",
    "    if is_train:\n",
    "        name = './model_predictions/train/'+file+'.csv'\n",
    "    else:\n",
    "        name = './model_predictions/test/'+file+'.csv'\n",
    "    df = pd.read_csv(name)\n",
    "    df = pd.DataFrame(df['is_duplicate'].as_matrix(), columns=[file+'_is_duplicate'])\n",
    "    return df\n",
    "\n",
    "for helper in helpers:\n",
    "    print('Loading :', helper)\n",
    "    train_tmp.append(read_csv(helper,is_train=True))\n",
    "    test_tmp.append(read_csv(helper,is_train=False))\n",
    "\n",
    "train_helpers = pd.concat(train_tmp, axis=1).as_matrix()\n",
    "test_helpers = pd.concat(test_tmp, axis=1).as_matrix()\n",
    "\n",
    "val_helpers = train_helpers[val_idxes]\n",
    "blending_train_helper = train_helpers[blending_train_idx]\n",
    "blending_val_helper = train_helpers[blending_val_idx]\n",
    "\n",
    "train_helpers = np.delete(train_helpers, val_idxes, axis=0)\n",
    "\n",
    "del train_tmp\n",
    "del test_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inf_nan_to_zero(arr):\n",
    "    nan = np.isnan(arr)\n",
    "    inf = np.isinf(arr)\n",
    "    arr[nan] = 0\n",
    "    arr[inf] = 0\n",
    "    return arr\n",
    "\n",
    "# process blending training data\n",
    "\n",
    "blending_train_helper = inf_nan_to_zero(blending_train_helper)\n",
    "blending_val_helper = inf_nan_to_zero(blending_val_helper)\n",
    "\n",
    "# process all training/testing data\n",
    "\n",
    "train_helpers = inf_nan_to_zero(train_helpers)\n",
    "test_helpers = inf_nan_to_zero(test_helpers)\n",
    "val_helpers = inf_nan_to_zero(val_helpers)\n",
    "\n",
    "# process all leaks\n",
    "\n",
    "# blending_val_leaks = inf_nan_to_zero(blending_val_leaks)\n",
    "# blending_train_leaks = inf_nan_to_zero(blending_train_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# blending_train = np.hstack([blending_train_helper,blending_train_leaks])\n",
    "# blending_val = np.hstack([blending_val_helper,blending_val_leaks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.452911\tvalid-logloss:0.453067\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[20]\ttrain-logloss:0.308539\tvalid-logloss:0.311724\n",
      "[40]\ttrain-logloss:0.240279\tvalid-logloss:0.245806\n",
      "[60]\ttrain-logloss:0.201137\tvalid-logloss:0.208824\n",
      "[80]\ttrain-logloss:0.177142\tvalid-logloss:0.186616\n",
      "[100]\ttrain-logloss:0.161898\tvalid-logloss:0.172979\n",
      "[120]\ttrain-logloss:0.151945\tvalid-logloss:0.164322\n",
      "[140]\ttrain-logloss:0.145396\tvalid-logloss:0.158843\n",
      "[160]\ttrain-logloss:0.141034\tvalid-logloss:0.155287\n",
      "[180]\ttrain-logloss:0.138006\tvalid-logloss:0.153089\n",
      "[200]\ttrain-logloss:0.135804\tvalid-logloss:0.151752\n",
      "[220]\ttrain-logloss:0.134192\tvalid-logloss:0.150889\n",
      "[240]\ttrain-logloss:0.132972\tvalid-logloss:0.150368\n",
      "[260]\ttrain-logloss:0.13192\tvalid-logloss:0.149987\n",
      "[280]\ttrain-logloss:0.131025\tvalid-logloss:0.149825\n",
      "[300]\ttrain-logloss:0.13009\tvalid-logloss:0.149764\n",
      "[320]\ttrain-logloss:0.129234\tvalid-logloss:0.149746\n",
      "[340]\ttrain-logloss:0.128336\tvalid-logloss:0.149739\n",
      "[360]\ttrain-logloss:0.12721\tvalid-logloss:0.149802\n",
      "[380]\ttrain-logloss:0.12607\tvalid-logloss:0.149945\n",
      "[400]\ttrain-logloss:0.125164\tvalid-logloss:0.150055\n",
      "[420]\ttrain-logloss:0.123908\tvalid-logloss:0.150199\n",
      "Stopping. Best iteration:\n",
      "[330]\ttrain-logloss:0.128827\tvalid-logloss:0.14971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    " \n",
    "# params['eta'] = 0.1\n",
    "# params['max_depth'] = 5\n",
    "# params['subsample'] = 0.7\n",
    "# params['colsample_bytree'] = 0.5\n",
    "# params['colsample_bylevel'] = 0.5\n",
    "# params['min_child_weight'] = 10\n",
    "\n",
    "# params['base_score'] = 0.1\n",
    "\n",
    "\n",
    "d_train = xgb.DMatrix(blending_train_helper, label=y_blending_train, weight=blending_train_weight)\n",
    "d_valid = xgb.DMatrix(blending_val_helper, label=y_blending_val, weight=blending_val_weight)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 100000, watchlist, early_stopping_rounds=100, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d_test = xgb.DMatrix()\n",
    "# pred = bst.predict(d_test)\n",
    "# eval_logloss(y_all_train,pred.flatten(), all_train_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Output the blending result to csv file\n",
    "'''\n",
    "\n",
    "d_test = xgb.DMatrix(test_helpers)\n",
    "pred = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = np.arange(2345796)\n",
    "sub['is_duplicate'] = pred\n",
    "sub.to_csv('./final_hubert_blending_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LR as blending model\n",
    "\n",
    "LR is a more regular choice, so just try it.\n",
    "\n",
    "But it turns out not very well (log-loss 0.1645, which is even lower than a single model). My explaination is that: \"Our blending is actually transforming the whole problem into a new problem, the features between old training data and new training data is totally different. We need a stronger classifier to learn these new features, rather than using LR which it too weak.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16454852889914509"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class_weight = get_pos_neg_weights(y_blending_train, as_array=False)\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', \n",
    "                        C=1, \n",
    "                        intercept_scaling=1, \n",
    "                        class_weight=class_weight,\n",
    "                        random_state=8787, \n",
    "                        solver='liblinear', \n",
    "                        max_iter=100, \n",
    "                        multi_class='ovr')\n",
    "\n",
    "lr.fit(blending_train_helper, y_blending_train)\n",
    "\n",
    "pred = lr.predict_proba(blending_val_helper)\n",
    "eval_logloss(y_blending_val, pred[:,1], blending_val_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pred = lr.predict_proba(train_helpers)\n",
    "# eval_logloss(y_all_train,pred[:,1], all_train_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = lr.predict_proba(test_helpers)[:,1]\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = np.arange(2345796)\n",
    "sub['is_duplicate'] = pred\n",
    "sub.to_csv('./final_lr_blending_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly use stacking (performance sucks)\n",
    "\n",
    "As stated above, we made a huge mistake while training each model. So the direct stacking yeilds a very poor generalizability. (validation loss is 0.168, the lowest one I have ever seen.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.451229\tvalid-logloss:0.452266\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-logloss:0.192985\tvalid-logloss:0.222633\n",
      "[100]\ttrain-logloss:0.132873\tvalid-logloss:0.178054\n",
      "[150]\ttrain-logloss:0.113614\tvalid-logloss:0.168937\n",
      "[200]\ttrain-logloss:0.106972\tvalid-logloss:0.168834\n",
      "[250]\ttrain-logloss:0.104529\tvalid-logloss:0.170354\n",
      "Stopping. Best iteration:\n",
      "[171]\ttrain-logloss:0.109984\tvalid-logloss:0.168408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.2\n",
    " \n",
    "# params['eta'] = 0.1\n",
    "# params['max_depth'] = 5\n",
    "# params['subsample'] = 0.7\n",
    "# params['colsample_bytree'] = 0.5\n",
    "# params['colsample_bylevel'] = 0.5\n",
    "# params['min_child_weight'] = 10\n",
    "\n",
    "# params['base_score'] = 0.1\n",
    "\n",
    "\n",
    "d_train = xgb.DMatrix(train_helpers, label=y_train, weight=train_weight)\n",
    "d_valid = xgb.DMatrix(val_helpers, label=y_val, weight=val_weight)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 20000, watchlist, early_stopping_rounds=100, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(test_helpers)\n",
    "pred = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = np.arange(2345796)\n",
    "sub['is_duplicate'] = pred\n",
    "sub.to_csv('./final_stacking_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
