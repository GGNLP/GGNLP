{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some ideas\n",
    "\n",
    "1. 我們有已經 label 成 same question 的 data ，那我們也許可以另外 train 一個 model 可以根據 input 的句子 sequential 生成新的\"換句話說\"句子?\n",
    "2. curriculum learning\n",
    "3. Ensombling:\n",
    "\n",
    "        a. Naive RNN\n",
    "        b. xgboost\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "1. Since we already have data that labelled as same question paris. Is it possible for us to train another model which can sequetially generate a same question string depends on the input? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "embedding_matrix = pickle.load(open('../dataset/processed/embedding_matrix.pkl','rb'))\n",
    "\n",
    "df_train = pickle.load(open('../dataset/processed/processed_training_data.pkl','rb'))\n",
    "df_test = pickle.load(open('../dataset/processed/processed_testing_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[26625, 43141, 57411, 55827, 31399, 55827, 404...</td>\n",
       "      <td>[26625, 43141, 57411, 55827, 31399, 55827, 404...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[26625, 43141, 57411, 55969, 47825, 14349, 3, ...</td>\n",
       "      <td>[26625, 60703, 40671, 41866, 57411, 12894, 401...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[12206, 31554, 12357, 42227, 57411, 55391, 478...</td>\n",
       "      <td>[12206, 31554, 13085, 55391, 30025, 42228, 313...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[26684, 28375, 12357, 45770, 59618, 3, 12206, ...</td>\n",
       "      <td>[9985, 57411, 52273, 60339, 5, 43141, 36203, 3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[26647, 47944, 36110, 42124, 60109, 51346, 3, ...</td>\n",
       "      <td>[26647, 38748, 60703, 56632, 42124, 53383, 6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[3, 12357, 28375, 27360, 6043, 23904, 6028, 46...</td>\n",
       "      <td>[6, 27360, 58322, 6043, 7, 17002, 28557, 29128...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[22878, 12357, 31386, 3]</td>\n",
       "      <td>[26625, 43643, 4, 27702, 28557, 38317, 39406, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[12206, 31554, 12357, 30025, 27360, 40065, 3]</td>\n",
       "      <td>[26625, 54399, 12357, 36245, 57764, 30025, 273...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[26640, 36245, 60925, 59299, 3, 42678, 47825, 4]</td>\n",
       "      <td>[26640, 36245, 60925, 59299, 5, 42678, 47825, 6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[17083, 3, 5988, 12357, 40523, 46866, 6367, 4, 5]</td>\n",
       "      <td>[12206, 36245, 12357, 40523, 17083, 7545, 3908...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          question1  \\\n",
       "0   0  [26625, 43141, 57411, 55827, 31399, 55827, 404...   \n",
       "1   1  [26625, 43141, 57411, 55969, 47825, 14349, 3, ...   \n",
       "2   2  [12206, 31554, 12357, 42227, 57411, 55391, 478...   \n",
       "3   3  [26684, 28375, 12357, 45770, 59618, 3, 12206, ...   \n",
       "4   4  [26647, 47944, 36110, 42124, 60109, 51346, 3, ...   \n",
       "5   5  [3, 12357, 28375, 27360, 6043, 23904, 6028, 46...   \n",
       "6   6                           [22878, 12357, 31386, 3]   \n",
       "7   7      [12206, 31554, 12357, 30025, 27360, 40065, 3]   \n",
       "8   8   [26640, 36245, 60925, 59299, 3, 42678, 47825, 4]   \n",
       "9   9  [17083, 3, 5988, 12357, 40523, 46866, 6367, 4, 5]   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  [26625, 43141, 57411, 55827, 31399, 55827, 404...             0  \n",
       "1  [26625, 60703, 40671, 41866, 57411, 12894, 401...             0  \n",
       "2  [12206, 31554, 13085, 55391, 30025, 42228, 313...             0  \n",
       "3  [9985, 57411, 52273, 60339, 5, 43141, 36203, 3...             0  \n",
       "4      [26647, 38748, 60703, 56632, 42124, 53383, 6]             0  \n",
       "5  [6, 27360, 58322, 6043, 7, 17002, 28557, 29128...             1  \n",
       "6  [26625, 43643, 4, 27702, 28557, 38317, 39406, ...             0  \n",
       "7  [26625, 54399, 12357, 36245, 57764, 30025, 273...             1  \n",
       "8   [26640, 36245, 60925, 59299, 5, 42678, 47825, 6]             0  \n",
       "9  [12206, 36245, 12357, 40523, 17083, 7545, 3908...             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question(question, enc_map):\n",
    "    \n",
    "    if type(question)!=str or question==\"\":\n",
    "        return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "    \n",
    "    def map_wrod(word):\n",
    "        return enc_map[word] if word in enc_map else enc_map['<RARE>']\n",
    "         \n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "    separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>]|$)'\n",
    "    single_word = '[\\S]+' # non-empty is enough here\n",
    "    \n",
    "    words_list = re.findall(single_word+separator, question)\n",
    "    \n",
    "    return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRGL7/2O2MprxWPpMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29aabb06ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niAzb/we7MUcvE98vdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29b610b46a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008973756461945634  ratio data is longer than 30 words\n"
     ]
    }
   ],
   "source": [
    "len_counter = [0]*238 # longest question is 237 words long\n",
    "\n",
    "for i,data in df_train.iterrows():\n",
    "    q1 = data['question1']\n",
    "    q2 = data['question2']\n",
    "    \n",
    "    len_counter[len(q1)-2] += 1\n",
    "    len_counter[len(q2)-2] += 1\n",
    "        \n",
    "\n",
    "plt.plot(len_counter)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(len_counter[0:40])\n",
    "plt.show()\n",
    "\n",
    "print(sum(len_counter[30:])/sum(len_counter), ' ratio data is longer than 30 words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so, use 30 words as length of model input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_to_list_and_clip(data, clip_st_ed=False):\n",
    "    \n",
    "    clip_len = 30\n",
    "    \n",
    "    if clip_st_ed:\n",
    "        data['question1'] = data['question1'][1:-1]\n",
    "        data['question2'] = data['question2'][1:-1]\n",
    "    \n",
    "    if len(data['question1'])<clip_len:\n",
    "        data['question1'] = data['question1'] + [enc_map['<ED>']] * (30-len(data['question1']))\n",
    "    else:\n",
    "        data['question1'] = data['question1'][:30]\n",
    "        \n",
    "    if len(data['question2'])<clip_len:\n",
    "        data['question2'] = data['question2'] + [enc_map['<ED>']] * (30-len(data['question2']))\n",
    "    else:\n",
    "        data['question2'] = data['question2'][:30]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_transformed = df_train.apply(change_to_list_and_clip, axis=1)\n",
    "pickle.dump(df_transformed, open('../dataset/processed/df_train_hubertLin_version.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_transformed = pickle.load(open('../dataset/processed/df_train_hubertLin_version.pkl', 'rb'))\n",
    "\n",
    "diff_questions = df_transformed.where(df_transformed['is_duplicate']==0).dropna().reset_index(drop=True)\n",
    "same_questions = df_transformed.where(df_transformed['is_duplicate']==1).dropna().reset_index(drop=True)\n",
    "\n",
    "pickle.dump(diff_questions, open('../dataset/processed/df_train_diff_hubertLin_version.pkl', 'wb'))\n",
    "pickle.dump(same_questions, open('../dataset/processed/df_train_same_hubertLin_version.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_test.apply(change_to_list_and_clip, axis=1)\n",
    "pickle.dump(df_transformed, open('../dataset/processed/df_test_hubertLin_version.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "embedding_matrix = pickle.load(open('../dataset/processed/embedding_matrix.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def holdout(df, ratio=0.025): # we have 400,000 training data, 10000 for validation is enough\n",
    "    def gen_suffle_idx(arr):\n",
    "        random.shuffle(arr)\n",
    "        return arr\n",
    "    \n",
    "    holdout_size = int(len(df)*ratio)\n",
    "    idxes = gen_suffle_idx(np.arange(len(df)))\n",
    "    holdout_idxes = idxes[:holdout_size]\n",
    "    train_idxes = idxes[holdout_size:]\n",
    "    \n",
    "    df_train = df.iloc[train_idxes].dropna().reset_index(drop=True)\n",
    "    df_val   = df.iloc[holdout_idxes].dropna().reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, GRU, Reshape, Dense, merge, Dropout, BatchNormalization, LSTM, MaxoutDense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.layers.noise import GaussianNoise\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def naive_model():\n",
    "\n",
    "    # hyper-parameters that should be passed as function argument\n",
    "    \n",
    "    EMBEDDING_SIZE = 300\n",
    "    vocab_size = len(enc_map)\n",
    "    vocab_dim = EMBEDDING_SIZE # in my case is 300\n",
    "    text_length = 30 # how many words are we going to feed in one time\n",
    "    clipnorm = 1\n",
    "    \n",
    "    lstm_output = 128\n",
    "\n",
    "#     gru = GRU(256, dropout=0.2)\n",
    "    gru = LSTM(lstm_output, dropout=0.5, recurrent_dropout=0.5)\n",
    "    \n",
    "    # embed our encoded question to embedded vector\n",
    "\n",
    "    encoded_question_input1 = Input(shape=(text_length,))\n",
    "\n",
    "    x = Embedding(output_dim = vocab_dim, \n",
    "                  input_dim = vocab_size, \n",
    "                  init = 'glorot_uniform',       # to be honest, I don't know what is this\n",
    "                  input_length = text_length, \n",
    "                  weights = [embedding_matrix]   # our embedding_matrix\n",
    "                 )(encoded_question_input1)\n",
    "    x = GaussianNoise(0.1)(x)\n",
    "    text_embedded1 = Reshape((1,vocab_dim*text_length))(x)\n",
    "    gru1 = gru(text_embedded1)\n",
    "\n",
    "    \n",
    "    # embed our encoded question to embedded vector\n",
    "\n",
    "    encoded_question_input2 = Input(shape=(text_length,))\n",
    "\n",
    "    x = Embedding(output_dim = vocab_dim, \n",
    "                  input_dim = vocab_size, \n",
    "                  init = 'glorot_uniform',       # to be honest, I don't know what is this\n",
    "                  input_length = text_length, \n",
    "                  weights = [embedding_matrix]   # our embedding_matrix\n",
    "                 )(encoded_question_input2)\n",
    "    x = GaussianNoise(0.1)(x)\n",
    "    text_embedded2 = Reshape((1,vocab_dim*text_length))(x)\n",
    "    gru2 = gru(text_embedded2)\n",
    "    \n",
    "\n",
    "    # feed to RNN model\n",
    "    x = merge([gru1, gru2], mode='concat', concat_axis=-1)\n",
    "\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "#     x = Dense(60, activation='relu')(x)\n",
    "    x = MaxoutDense(lstm_output//2)(x) # ex: MaxoutDense(output_dim=1,nb_feature=3,input_dim=1)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # compile the model\n",
    "    \n",
    "    model = Model(input=[encoded_question_input1, encoded_question_input2], output=out)\n",
    "    # choose objective and optimizer\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-3))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(output_dim=300, input_length=30, embeddings_initializer=\"glorot_uniform\", input_dim=61203, weights=[array([[-...)`\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(output_dim=300, input_length=30, embeddings_initializer=\"glorot_uniform\", input_dim=61203, weights=[array([[-...)`\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:529: UserWarning: The `MaxoutDense` layer is deprecated and will be removed after 06/2017.\n",
      "  warnings.warn('The `MaxoutDense` layer is deprecated '\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "model = naive_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on all data directly. Overcome the imbalance problem by directly assign weights for each class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pos_label_ratio = 0.1746\n",
    "\n",
    "weights = {\n",
    "    0: 1.309028344,\n",
    "    1: 0.472001959\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = pickle.load(open('../dataset/processed/df_train_hubertLin_version.pkl', 'rb'))\n",
    "\n",
    "df_transformed, df_validation = holdout(df_transformed, ratio=0.025)\n",
    "\n",
    "X_train = [np.vstack(df_transformed['question1']), np.vstack(df_transformed['question2'])]\n",
    "y_train = np.vstack(df_transformed['is_duplicate'])\n",
    "\n",
    "X_val = [np.vstack(df_validation['question1']), np.vstack(df_validation['question2'])]\n",
    "y_val = np.vstack(df_validation['is_duplicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pos_label_ratio = 0.1746\n",
    "\n",
    "pos_labels = df_transformed['is_duplicate']==1\n",
    "pos_label_ratio = sum(pos_labels)/len(pos_labels)\n",
    "weights = {\n",
    "    0: (1-test_set_pos_label_ratio) / (1-pos_label_ratio),\n",
    "    1: test_set_pos_label_ratio/pos_label_ratio\n",
    "}\n",
    "\n",
    "pos_labels = y_val==1\n",
    "pos_label_ratio = sum(pos_labels)/len(pos_labels)\n",
    "validation_weights = np.repeat(weights[1], len(y_val))\n",
    "validation_weights[y_val.reshape(len(y_val))==0] = weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "50/50 [==============================] - 31s - loss: 0.7475 - val_loss: 0.5179\n",
      "Epoch 2/1000\n",
      "50/50 [==============================] - 27s - loss: 0.5189 - val_loss: 0.4525\n",
      "Epoch 3/1000\n",
      "50/50 [==============================] - 27s - loss: 0.4603 - val_loss: 0.4401\n",
      "Epoch 4/1000\n",
      "50/50 [==============================] - 27s - loss: 0.4482 - val_loss: 0.4338\n",
      "Epoch 5/1000\n",
      "50/50 [==============================] - 27s - loss: 0.4327 - val_loss: 0.4467\n",
      "Epoch 6/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4456 - val_loss: 0.4270\n",
      "Epoch 7/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4360 - val_loss: 0.4236\n",
      "Epoch 8/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4346 - val_loss: 0.4151\n",
      "Epoch 9/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4278 - val_loss: 0.4098\n",
      "Epoch 10/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4230 - val_loss: 0.4077\n",
      "Epoch 11/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4064 - val_loss: 0.3965\n",
      "Epoch 12/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4097 - val_loss: 0.4070\n",
      "Epoch 13/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4014 - val_loss: 0.3849\n",
      "Epoch 14/1000\n",
      "50/50 [==============================] - 28s - loss: 0.4062 - val_loss: 0.3802\n",
      "Epoch 15/1000\n",
      "50/50 [==============================] - 28s - loss: 0.3892 - val_loss: 0.3820\n",
      "Epoch 16/1000\n",
      "50/50 [==============================] - 28s - loss: 0.3919 - val_loss: 0.3915\n",
      "Epoch 17/1000\n",
      "50/50 [==============================] - 28s - loss: 0.3942 - val_loss: 0.3948\n",
      "Epoch 18/1000\n",
      "50/50 [==============================] - 28s - loss: 0.3902 - val_loss: 0.3923\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def gen_batch(X,y):\n",
    "    size = len(y)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        rnd_idxes = [random.randint(0,size-1) for i in range(batch_size)]\n",
    "        \n",
    "        yield [X[0][rnd_idxes], X[1][rnd_idxes]], y[rnd_idxes]\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "]\n",
    "    \n",
    "try:\n",
    "#     model.fit(X_train, \n",
    "#               y_train,\n",
    "#               class_weight=weights,\n",
    "#               epochs=1000, \n",
    "#               batch_size=512,\n",
    "#               shuffle=True,\n",
    "#               validation_data=(X_val,y_val,validation_weights), \n",
    "#               callbacks=callbacks,\n",
    "#               verbose=1)\n",
    "    \n",
    "    model.fit_generator(gen_batch(X_train,y_train),\n",
    "                        steps_per_epoch=50,\n",
    "                        class_weight=weights,\n",
    "                        epochs=1000,\n",
    "                        validation_data=(X_val,y_val,validation_weights), \n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('\\nEarly stop by user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss= 0.349204152869\n",
      "roc_auc= 0.833908597103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "pred = model.predict(X_val)\n",
    "\n",
    "pred[pred==1] = 0.999999\n",
    "pred[pred==0] = 0.000001\n",
    "\n",
    "print('log_loss=',log_loss(y_val, pred, sample_weight=validation_weights))\n",
    "print('roc_auc=',roc_auc_score(y_val, pred, sample_weight=validation_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commonly overfit around loss ~ 0.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on batch , uses unsampling/downsampling to gaurantee the pos/neg labels are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_questions = pickle.load(open('../dataset/processed/df_train_diff_hubertLin_version.pkl', 'rb'))\n",
    "same_questions = pickle.load(open('../dataset/processed/df_train_same_hubertLin_version.pkl', 'rb'))\n",
    "\n",
    "diff_questions, diff_val = holdout(diff_questions)\n",
    "same_questions, same_val = holdout(same_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404290 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      " 73728/404290 [====>.........................] - ETA: 309s - loss: 0.5912\n",
      "Early stop by user\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "def gen_suffle_idx(arr):\n",
    "    random.shuffle(arr)\n",
    "    return arr\n",
    "\n",
    "def data_gen():\n",
    "    a = np.arange(batch_size)\n",
    "    while True:\n",
    "        \n",
    "        rand = [random.randint(0,len(same_questions)-1) for i in range(batch_size//2)]\n",
    "        pos_selected = same_questions.ix[rand]\n",
    "        rand = [random.randint(0,len(diff_questions)-1) for i in range(batch_size//2)]\n",
    "        neg_selected = diff_questions.ix[rand]\n",
    "        \n",
    "        selected  = pos_selected.append(neg_selected).reset_index(drop=True)\n",
    "        selected = selected.ix[gen_suffle_idx(a)]\n",
    "\n",
    "#         batch_size = len(sanity_check)\n",
    "#         a = np.arange(batch_size)\n",
    "#         selected = sanity_check.ix[gen_suffle_idx(a)]\n",
    "        \n",
    "        q1 = np.array([data['question1'] for i,data in selected.iterrows()])\n",
    "        q2 = np.array([data['question2'] for i,data in selected.iterrows()])\n",
    "        \n",
    "        # randomly swap two training question pair\n",
    "        swap_idxes = [random.randint(0,len(selected)-1) for i in range(batch_size//2)]\n",
    "        for i in swap_idxes:\n",
    "            q1[i] , q2[i] = q2[i] , q1[i]\n",
    "        \n",
    "        y = np.array([data['is_duplicate'] for i,data in selected.iterrows()])\n",
    "\n",
    "        yield [q1, q2], y\n",
    "\n",
    "def gen_validation(size):\n",
    "    \n",
    "    a = np.arange(size)\n",
    "    \n",
    "    rand = [random.randint(0,len(same_val)-1) for i in range(size//2)]\n",
    "    pos_selected = same_val.ix[rand]\n",
    "    rand = [random.randint(0,len(diff_val)-1) for i in range(size//2)]\n",
    "    neg_selected = diff_val.ix[rand]\n",
    "    \n",
    "    selected  = pos_selected.append(neg_selected).reset_index(drop=True)\n",
    "    selected = selected.ix[gen_suffle_idx(a)]\n",
    "\n",
    "    q1 = np.array([data['question1'] for i,data in selected.iterrows()])\n",
    "    q2 = np.array([data['question2'] for i,data in selected.iterrows()])\n",
    "\n",
    "    y = np.array([data['is_duplicate'] for i,data in selected.iterrows()])\n",
    "\n",
    "    return [q1, q2], y, class_weights\n",
    "    \n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n",
    "]\n",
    "    \n",
    "try:\n",
    "    validation_data = gen_validation(10000)\n",
    "    model.fit_generator(data_gen(), 50, epochs=1000, validation_data=validation_data, callbacks=callbacks)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nEarly stop by user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss 約 converge 在 0.49~0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    q = diff_questions.ix[i]\n",
    "    x1 = np.array([q['question1']])\n",
    "    x2 = np.array([q['question2']])\n",
    "    print(model.predict([x1,x2])[0][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def predict(i,q):\n",
    "    \n",
    "#     x1 = np.array([list(q['question1'])])\n",
    "#     x2 = np.array([list(q['question2'])])\n",
    "#     return model.predict([x1,x2])[:,1]\n",
    "\n",
    "# df_test.head()\n",
    "# for i in range(10):\n",
    "#     df = same_questions\n",
    "#     print(dec_question(df.ix[i]['question1'], dec_map))\n",
    "#     print(dec_question(df.ix[i]['question2'], dec_map))\n",
    "#     print(predict(i,df.ix[i]))\n",
    "#     print(df.ix[i]['is_duplicate'])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../model/HubertLin_naive_LSTM120.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### customized testcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question_pair(question_pair_list, enc_map, capsulate_st_ed=False):\n",
    "\n",
    "    def enc_question(question):\n",
    "\n",
    "        if type(question)!=str or question==\"\":\n",
    "            if capsulate_st_ed:\n",
    "                return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def map_wrod(word):\n",
    "            if word in enc_map:\n",
    "                return enc_map[word]\n",
    "            elif word in rare_words:\n",
    "                idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                return enc_map['<RARE' + str(idx) + '>']\n",
    "            else:\n",
    "                rare_words.append(word)\n",
    "                return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "        # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "        separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "        single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "        words_list = re.findall(single_word+separator, question)\n",
    "\n",
    "        if capsulate_st_ed:\n",
    "            return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "        else:\n",
    "            return [map_wrod(word) for word in words_list]\n",
    "        \n",
    "    # This array is a local cache that identify those non-encoded rare words. \n",
    "    # We'll assign identical index (in this question pair) for each of rare words\n",
    "    rare_words = []\n",
    "    \n",
    "    return enc_question(question_pair_list[0]) , enc_question(question_pair_list[1])\n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]\n",
    "\n",
    "def clip_length(data, clip_st_ed=False):\n",
    "    \n",
    "    clip_len = 30\n",
    "    \n",
    "    if clip_st_ed:\n",
    "        data[0] = eval(data[0])[1:-1]\n",
    "        data[1] = eval(data[1])[1:-1]\n",
    "    \n",
    "    if len(data[0])<clip_len:\n",
    "        data[0] = data[0] + [enc_map['<ED>']] * (30-len(data[0]))\n",
    "    else:\n",
    "        data[0] = data[0][:30]\n",
    "        \n",
    "    if len(data[1])<clip_len:\n",
    "        data[1] = data[1] + [enc_map['<ED>']] * (30-len(data[1]))\n",
    "    else:\n",
    "        data[1] = data[1][:30]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Which', 'is', 'the', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "['Where', 'was', 'training', '<RARE0>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "\n",
      "Predicting is same question proba = [[ 0.15064879]]\n"
     ]
    }
   ],
   "source": [
    "a = df_test.ix[100]\n",
    "q1 = dec_question(a['question1'], dec_map)\n",
    "q2 = dec_question(a['question2'], dec_map)\n",
    "print(q1)\n",
    "print(q2)\n",
    "\n",
    "pred = model.predict([np.array([a['question1']]), np.array([a['question2']])])\n",
    "print('\\nPredicting is same question proba =', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'your', 'favorite', 'food', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "['Which', 'is', 'your', 'favorite', 'one', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "Predicting is same question proba = [[ 0.58142167]]\n"
     ]
    }
   ],
   "source": [
    "q1 = 'I am a good man'\n",
    "q2 = 'A good man is me'\n",
    "\n",
    "q1 = 'What is your favorite food'\n",
    "q2 = 'Which is your favorite one'\n",
    "\n",
    "sp1 = q1.split(' ')\n",
    "sp2 = q2.split(' ')\n",
    "\n",
    "enc1, enc2 = enc_question_pair([q1,q2], enc_map)\n",
    "enc1, enc2 = clip_length([enc1,enc2])\n",
    "print(dec_question(enc1,dec_map))\n",
    "print(dec_question(enc2,dec_map))\n",
    "\n",
    "pred = model.predict([np.array([enc1]), np.array([enc2])])\n",
    "print('Predicting is same question proba =', pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# try:\n",
    "#     model==None\n",
    "# except:\n",
    "#     model = load_model('./model/HubertLin_naive_2_GRU256_same.model')\n",
    "\n",
    "df_test = pickle.load(open('../dataset/processed/df_test_hubertLin_version.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "partition_size = 1000\n",
    "\n",
    "def predict(i,q):\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        print(i*partition_size, '/', len(df_test))\n",
    "    \n",
    "    x1 = np.array(list(q['question1']))\n",
    "    x2 = np.array(list(q['question2']))\n",
    "    return model.predict([x1,x2])\n",
    "\n",
    "partition_len = len(df_test)//partition_size +1\n",
    "result = [predict(i,df_test.iloc[i*partition_size:(i+1)*partition_size]) for i in range(partition_len)]\n",
    "con = np.concatenate(result)\n",
    "df_result = pd.DataFrame({'test_id':np.arange(len(con)),'is_duplicate':con.reshape(len(con))}, columns=['test_id','is_duplicate'])\n",
    "df_result.to_csv('../result/prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction success\n"
     ]
    }
   ],
   "source": [
    "if len(df_result)!=2345796:\n",
    "    print('Your result prediction count is not fit to the testing data length 2345796 , yours:', len(df_result))\n",
    "else:\n",
    "    print('Prediction success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
