{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some ideas\n",
    "\n",
    "1. Since we already have data that labelled as same question paris. Is it possible for us to train another model which can sequetially generate a same question string depends on the input? \n",
    "2. curriculum learning\n",
    "3. Ensombling:\n",
    "\n",
    "        a. Naive RNN\n",
    "        b. xgboost\n",
    "        c. randomforest\n",
    "        \n",
    "4. Feature engineering\n",
    "\n",
    "        a. question length (May not be very powerful)\n",
    "        b. similarity metric score (Directly count it and feed it as training feature. For exampole: cosine similarity LOL)\n",
    "        c. I don't think TF-IDF would work, but still worth trying if we have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "\n",
    "duplicated_qid_pair = pickle.load(open('../dataset/processed/positive_question_id_pairs.pkl', 'rb'))\n",
    "qid_question_dict = pickle.load(open('../dataset/processed/qid_question_wrods_list_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question(question, enc_map):\n",
    "    \n",
    "    if type(question)!=str or question==\"\":\n",
    "        return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "    \n",
    "    def map_wrod(word):\n",
    "        return enc_map[word] if word in enc_map else enc_map['<RARE>']\n",
    "         \n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "    separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>]|$)'\n",
    "    single_word = '[\\S]+' # non-empty is enough here\n",
    "    \n",
    "    words_list = re.findall(single_word+separator, question)\n",
    "    \n",
    "    return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip training questions\n",
    "\n",
    "Normalize the length of all question to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in qid_question_dict:\n",
    "    q_len = len(qid_question_dict[key])\n",
    "    if q_len>=30:\n",
    "        qid_question_dict[key] = qid_question_dict[key][:30]\n",
    "    else:\n",
    "        qid_question_dict[key] = qid_question_dict[key] + ['<ED>'] * (30-q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(qid_question_dict, open('../dataset/processed/qid_question_wrods_list_dict_clipped.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_validation_samples(df, enc_map):\n",
    "    \n",
    "    def parse_word_list(question):\n",
    "        \n",
    "        if type(question)!=str:\n",
    "            return []\n",
    "\n",
    "        # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "        separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "        single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "        return re.findall(single_word+separator, question)\n",
    "    \n",
    "    # encode single question pair\n",
    "    def enc_question_pair(pair):\n",
    "\n",
    "        # This array is a local cache that identify those non-encoded rare words. \n",
    "        # We'll assign identical index (in this question pair) for each of rare words\n",
    "        rare_words = []\n",
    "\n",
    "        def enc_question(question):\n",
    "\n",
    "            def map_word(word):\n",
    "                if word in enc_map:\n",
    "                    return enc_map[word]\n",
    "                elif word in rare_words:\n",
    "                    idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                    return enc_map['<RARE' + str(idx) + '>']\n",
    "                else:\n",
    "                    rare_words.append(word)\n",
    "                    return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "            word_list = parse_word_list(question)\n",
    "            word_list = [map_word(word) for word in word_list]\n",
    "            \n",
    "            # clip the question length to 30\n",
    "            q_len = len(word_list)\n",
    "            if q_len>=30:\n",
    "                word_list = word_list[:30]\n",
    "            else:\n",
    "                word_list = word_list + [enc_map['<ED>']] * (30-q_len)\n",
    "            \n",
    "            return embedding_matrix[np.array(word_list)]\n",
    "        \n",
    "        return [enc_question(pair[0]) , enc_question(pair[1])]\n",
    "    \n",
    "    return X,y\n",
    "    \n",
    "    X = np.array([enc_question_pair([series['question1'],series['question2']]) for i,series in df.iterrows()])\n",
    "    y = np.array([series['is_duplicate'] for i,series in df.iterrows()])\n",
    "\n",
    "    return [X[:,0], X[:,1]], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pickle.load(open('../dataset/processed/validation_df.pkl','rb'))\n",
    "val_data = process_validation_samples(df_val, enc_map)\n",
    "pickle.dump(val_data, open('../dataset/processed/validation_data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and encode testing data (this is very slow)\n",
    "\n",
    "I still parses question length feature, although I haven't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def change_to_list_and_clip(data, clip_st_ed=False):\n",
    "    \n",
    "    global counter\n",
    "    counter+=1\n",
    "    if counter%10000==0:\n",
    "        print('Changed',counter,'data')\n",
    "    \n",
    "    clip_len = 30\n",
    "    data['q1_len'] = len(data['question1'])\n",
    "    data['q2_len'] = len(data['question2'])\n",
    "\n",
    "    if clip_st_ed:\n",
    "        data['question1'] = data['question1'][1:-1]\n",
    "        data['question2'] = data['question2'][1:-1]\n",
    "\n",
    "    if len(data['question1'])<clip_len:\n",
    "        data['question1'] = data['question1'] + [enc_map['<ED>']] * (30-data['q1_len'])\n",
    "    else:\n",
    "        data['question1'] = data['question1'][:30]\n",
    "        \n",
    "    if len(data['question2'])<clip_len:\n",
    "        data['question2'] = data['question2'] + [enc_map['<ED>']] * (30-data['q2_len'])\n",
    "    else:\n",
    "        data['question2'] = data['question2'][:30]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "df_test = pickle.load(open('../dataset/processed/processed_testing_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed 10000 data\n",
      "Changed 20000 data\n",
      "Changed 30000 data\n",
      "Changed 40000 data\n",
      "Changed 50000 data\n",
      "Changed 60000 data\n",
      "Changed 70000 data\n",
      "Changed 80000 data\n",
      "Changed 90000 data\n",
      "Changed 100000 data\n",
      "Changed 110000 data\n",
      "Changed 120000 data\n",
      "Changed 130000 data\n",
      "Changed 140000 data\n",
      "Changed 150000 data\n",
      "Changed 160000 data\n",
      "Changed 170000 data\n",
      "Changed 180000 data\n",
      "Changed 190000 data\n",
      "Changed 200000 data\n",
      "Changed 210000 data\n",
      "Changed 220000 data\n",
      "Changed 230000 data\n",
      "Changed 240000 data\n",
      "Changed 250000 data\n",
      "Changed 260000 data\n",
      "Changed 270000 data\n",
      "Changed 280000 data\n",
      "Changed 290000 data\n",
      "Changed 300000 data\n",
      "Changed 310000 data\n",
      "Changed 320000 data\n",
      "Changed 330000 data\n",
      "Changed 340000 data\n",
      "Changed 350000 data\n",
      "Changed 360000 data\n",
      "Changed 370000 data\n",
      "Changed 380000 data\n",
      "Changed 390000 data\n",
      "Changed 400000 data\n",
      "Changed 410000 data\n",
      "Changed 420000 data\n",
      "Changed 430000 data\n",
      "Changed 440000 data\n",
      "Changed 450000 data\n",
      "Changed 460000 data\n",
      "Changed 470000 data\n",
      "Changed 480000 data\n",
      "Changed 490000 data\n",
      "Changed 500000 data\n",
      "Changed 510000 data\n",
      "Changed 520000 data\n",
      "Changed 530000 data\n",
      "Changed 540000 data\n",
      "Changed 550000 data\n",
      "Changed 560000 data\n",
      "Changed 570000 data\n",
      "Changed 580000 data\n",
      "Changed 590000 data\n",
      "Changed 600000 data\n",
      "Changed 610000 data\n",
      "Changed 620000 data\n",
      "Changed 630000 data\n",
      "Changed 640000 data\n",
      "Changed 650000 data\n",
      "Changed 660000 data\n",
      "Changed 670000 data\n",
      "Changed 680000 data\n",
      "Changed 690000 data\n",
      "Changed 700000 data\n",
      "Changed 710000 data\n",
      "Changed 720000 data\n",
      "Changed 730000 data\n",
      "Changed 740000 data\n",
      "Changed 750000 data\n",
      "Changed 760000 data\n",
      "Changed 770000 data\n",
      "Changed 780000 data\n",
      "Changed 790000 data\n",
      "Changed 800000 data\n",
      "Changed 810000 data\n",
      "Changed 820000 data\n",
      "Changed 830000 data\n",
      "Changed 840000 data\n",
      "Changed 850000 data\n",
      "Changed 860000 data\n",
      "Changed 870000 data\n",
      "Changed 880000 data\n",
      "Changed 890000 data\n",
      "Changed 900000 data\n",
      "Changed 910000 data\n",
      "Changed 920000 data\n",
      "Changed 930000 data\n",
      "Changed 940000 data\n",
      "Changed 950000 data\n",
      "Changed 960000 data\n",
      "Changed 970000 data\n",
      "Changed 980000 data\n",
      "Changed 990000 data\n",
      "Changed 1000000 data\n",
      "Changed 1010000 data\n",
      "Changed 1020000 data\n",
      "Changed 1030000 data\n",
      "Changed 1040000 data\n",
      "Changed 1050000 data\n",
      "Changed 1060000 data\n",
      "Changed 1070000 data\n",
      "Changed 1080000 data\n",
      "Changed 1090000 data\n",
      "Changed 1100000 data\n",
      "Changed 1110000 data\n",
      "Changed 1120000 data\n",
      "Changed 1130000 data\n",
      "Changed 1140000 data\n",
      "Changed 1150000 data\n",
      "Changed 1160000 data\n",
      "Changed 1170000 data\n",
      "Changed 1180000 data\n",
      "Changed 1190000 data\n",
      "Changed 1200000 data\n",
      "Changed 1210000 data\n",
      "Changed 1220000 data\n",
      "Changed 1230000 data\n",
      "Changed 1240000 data\n",
      "Changed 1250000 data\n",
      "Changed 1260000 data\n",
      "Changed 1270000 data\n",
      "Changed 1280000 data\n",
      "Changed 1290000 data\n",
      "Changed 1300000 data\n",
      "Changed 1310000 data\n",
      "Changed 1320000 data\n",
      "Changed 1330000 data\n",
      "Changed 1340000 data\n",
      "Changed 1350000 data\n",
      "Changed 1360000 data\n",
      "Changed 1370000 data\n",
      "Changed 1380000 data\n",
      "Changed 1390000 data\n",
      "Changed 1400000 data\n",
      "Changed 1410000 data\n",
      "Changed 1420000 data\n",
      "Changed 1430000 data\n",
      "Changed 1440000 data\n",
      "Changed 1450000 data\n",
      "Changed 1460000 data\n",
      "Changed 1470000 data\n",
      "Changed 1480000 data\n",
      "Changed 1490000 data\n",
      "Changed 1500000 data\n",
      "Changed 1510000 data\n",
      "Changed 1520000 data\n",
      "Changed 1530000 data\n",
      "Changed 1540000 data\n",
      "Changed 1550000 data\n",
      "Changed 1560000 data\n",
      "Changed 1570000 data\n",
      "Changed 1580000 data\n",
      "Changed 1590000 data\n",
      "Changed 1600000 data\n",
      "Changed 1610000 data\n",
      "Changed 1620000 data\n",
      "Changed 1630000 data\n",
      "Changed 1640000 data\n",
      "Changed 1650000 data\n",
      "Changed 1660000 data\n",
      "Changed 1670000 data\n",
      "Changed 1680000 data\n",
      "Changed 1690000 data\n",
      "Changed 1700000 data\n",
      "Changed 1710000 data\n",
      "Changed 1720000 data\n",
      "Changed 1730000 data\n",
      "Changed 1740000 data\n",
      "Changed 1750000 data\n",
      "Changed 1760000 data\n",
      "Changed 1770000 data\n",
      "Changed 1780000 data\n",
      "Changed 1790000 data\n",
      "Changed 1800000 data\n",
      "Changed 1810000 data\n",
      "Changed 1820000 data\n",
      "Changed 1830000 data\n",
      "Changed 1840000 data\n",
      "Changed 1850000 data\n",
      "Changed 1860000 data\n",
      "Changed 1870000 data\n",
      "Changed 1880000 data\n",
      "Changed 1890000 data\n",
      "Changed 1900000 data\n",
      "Changed 1910000 data\n",
      "Changed 1920000 data\n",
      "Changed 1930000 data\n",
      "Changed 1940000 data\n",
      "Changed 1950000 data\n",
      "Changed 1960000 data\n",
      "Changed 1970000 data\n",
      "Changed 1980000 data\n",
      "Changed 1990000 data\n",
      "Changed 2000000 data\n",
      "Changed 2010000 data\n",
      "Changed 2020000 data\n",
      "Changed 2030000 data\n",
      "Changed 2040000 data\n",
      "Changed 2050000 data\n",
      "Changed 2060000 data\n",
      "Changed 2070000 data\n",
      "Changed 2080000 data\n",
      "Changed 2090000 data\n",
      "Changed 2100000 data\n",
      "Changed 2110000 data\n",
      "Changed 2120000 data\n",
      "Changed 2130000 data\n",
      "Changed 2140000 data\n",
      "Changed 2150000 data\n",
      "Changed 2160000 data\n",
      "Changed 2170000 data\n",
      "Changed 2180000 data\n",
      "Changed 2190000 data\n",
      "Changed 2200000 data\n",
      "Changed 2210000 data\n",
      "Changed 2220000 data\n",
      "Changed 2230000 data\n",
      "Changed 2240000 data\n",
      "Changed 2250000 data\n",
      "Changed 2260000 data\n",
      "Changed 2270000 data\n",
      "Changed 2280000 data\n",
      "Changed 2290000 data\n",
      "Changed 2300000 data\n",
      "Changed 2310000 data\n",
      "Changed 2320000 data\n",
      "Changed 2330000 data\n",
      "Changed 2340000 data\n",
      "Wall time: 1h 11min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "counter = 0\n",
    "df_transformed = df_test.apply(change_to_list_and_clip, axis=1)\n",
    "pickle.dump(df_transformed, open('../dataset/processed/df_test_hubertLin_version.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "embedding_matrix = pickle.load(open('../dataset/processed/embedding_matrix.pkl','rb'))\n",
    "\n",
    "train_pairs = pickle.load(open('../dataset/processed/positive_question_id_pairs.pkl', 'rb'))\n",
    "qid_question_dict = pickle.load(open('../dataset/processed/qid_question_wrods_list_dict_clipped.pkl','rb'))\n",
    "\n",
    "val_data = pickle.load(open('../dataset/processed/validation_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on all data directly. Overcome the imbalance problem by directly assign weights for each class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_pos_label_ratio = 0.1746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove validation data from training data\n",
    "def holdout(qid_pair_list, ratio=0.025):\n",
    "    \n",
    "    def gen_suffle_idx(arr):\n",
    "        random.shuffle(arr)\n",
    "        return arr\n",
    "    \n",
    "    holdout_size = int(len(qid_pair_list)*ratio)\n",
    "    idxes = gen_suffle_idx(np.arange(len(qid_pair_list)))\n",
    "    holdout_idxes = idxes[:holdout_size]\n",
    "    train_idxes = idxes[holdout_size:]\n",
    "    \n",
    "    train = qid_pair_list[train_idxes]\n",
    "    val   = qid_pair_list[holdout_idxes]\n",
    "    \n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The validation data is heldout in the data augmenting phase, no need to holdout here\n",
    "\n",
    "# train_pairs, val_pairs = holdout(train_pairs, ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate validation data's class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the class weights to simulate the class disribution of testing samples\n",
    "#\n",
    "# These numbers and formula is got from here: \n",
    "#     https://www.kaggle.com/lystdo/quora-question-pairs/lstm-with-word2vec-embeddings#175198\n",
    "\n",
    "test_set_pos_label_ratio = 0.1746\n",
    "\n",
    "X_val = val_data[0]\n",
    "y_val = val_data[1]\n",
    "validation_pos_ratio = sum(y_val)/len(y_val)\n",
    "\n",
    "val_weights = {\n",
    "    0: (1-test_set_pos_label_ratio) / (1-validation_pos_ratio),\n",
    "    1: test_set_pos_label_ratio/validation_pos_ratio\n",
    "}\n",
    "\n",
    "validation_weights = np.repeat(val_weights[1], len(y_val))\n",
    "validation_weights[y_val==0] = val_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras.python.keras.layers import Input, GRU, Reshape, Dense, Dropout, BatchNormalization, LSTM, GaussianDropout, SpatialDropout1D\n",
    "from tensorflow.contrib.keras.python.keras.layers.merge import Concatenate\n",
    "from tensorflow.contrib.keras.python.keras.models import Model\n",
    "from tensorflow.contrib.keras.python.keras.optimizers import RMSprop, Nadam\n",
    "from tensorflow.contrib.keras.python.keras.layers.wrappers import TimeDistributed\n",
    "from tensorflow.contrib.keras.python.keras.layers.noise import GaussianNoise\n",
    "from tensorflow.contrib.keras.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "# from keras.layers import Input, GRU, Reshape, Dense, Dropout, BatchNormalization, LSTM, GaussianDropout, Bidirectional\n",
    "# from keras.layers.merge import Concatenate\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import RMSprop, Nadam\n",
    "# # from keras.layers.wrappers import TimeDistributed\n",
    "# from keras.layers.noise import GaussianNoise\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "def naive_model():\n",
    "\n",
    "    # hyper-parameters that should be passed as function argument\n",
    "    \n",
    "    EMBEDDING_SIZE = 300\n",
    "    vocab_size = len(enc_map)\n",
    "    vocab_dim = EMBEDDING_SIZE # in my case is 300\n",
    "    text_length = 30 # how many words are we going to feed in one time\n",
    "    clipnorm = 1\n",
    "    \n",
    "    lstm_output = 168\n",
    "\n",
    "#     q1_len = Input(shape=(1,))\n",
    "#     q2_len = Input(shape=(1,))\n",
    "    \n",
    "    gru = LSTM(lstm_output, dropout=0.3, recurrent_dropout=0.3)\n",
    "#     tdd = TimeDistributed(Dense(lstm_output)) # TimeDistributedDense(lstm_output)\n",
    "    noise = GaussianNoise(0.3)\n",
    "    \n",
    "    # embed our encoded question to embedded vector\n",
    "\n",
    "    encoded_question_input1 = Input(shape=(text_length,vocab_dim))\n",
    "    x = noise(encoded_question_input1)\n",
    "    text_embedded1 = Reshape((1,vocab_dim*text_length))(x)\n",
    "    gru1 = gru(text_embedded1)\n",
    "    \n",
    "    # embed our encoded question to embedded vector\n",
    "\n",
    "    encoded_question_input2 = Input(shape=(text_length,vocab_dim))\n",
    "    x = noise(encoded_question_input2)\n",
    "    text_embedded2 = Reshape((1,vocab_dim*text_length))(x)\n",
    "    gru2 = gru(text_embedded2)\n",
    "    \n",
    "#     # FC without RNN, hope this patial model can deal with question with same words\n",
    "#     raw_concat = Concatenate(axis=-1)([text_embedded1, text_embedded2])\n",
    "#     fc_raw_concat = Dense(lstm_output//2, activation='relu')(raw_concat)\n",
    "#     fc_raw_concat = Reshape((lstm_output//2,))(fc_raw_concat)\n",
    "\n",
    "    # fully connnected\n",
    "    \n",
    "    x = Concatenate(axis=-1)([gru1, gru2])\n",
    "    x = GaussianDropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(lstm_output//2, activation='relu')(x)\n",
    "#     x = Concatenate(axis=-1)([x, fc_raw_concat])\n",
    "#     x = MaxoutDense(lstm_output//2)(x) # ex: MaxoutDense(output_dim=1,nb_feature=3,input_dim=1)\n",
    "#     x = merge([x, q1_len, q2_len], mode='concat', concat_axis=-1)\n",
    "    x = GaussianDropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # compile the model\n",
    "    \n",
    "    model = Model(inputs=[encoded_question_input1, encoded_question_input2], outputs=out)\n",
    "    # choose objective and optimizer\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-3))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Nadam(lr=2e-3))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = naive_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the list of words to list of embedded arrays\n",
    "def enc_question_pairs(qid_pairs, qid_question_dict, enc_map, swap=False):\n",
    "\n",
    "    # encode single question pair\n",
    "    def enc_question_pair(pair):\n",
    "\n",
    "        # This array is a local cache that identify those non-encoded rare words. \n",
    "        # We'll assign identical index (in this question pair) for each of rare words\n",
    "        rare_words = []\n",
    "\n",
    "        def enc_question(qid):\n",
    "\n",
    "            def map_word(word):\n",
    "                if word in enc_map:\n",
    "                    return enc_map[word]\n",
    "                elif word in rare_words:\n",
    "                    idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                    return enc_map['<RARE' + str(idx) + '>']\n",
    "                else:\n",
    "                    rare_words.append(word)\n",
    "                    return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "            question = qid_question_dict[qid]\n",
    "            return [map_word(word) for word in question]\n",
    "        \n",
    "        if swap:\n",
    "            return [enc_question(pair[1]) , enc_question(pair[0])]\n",
    "        else:\n",
    "            return [enc_question(pair[0]) , enc_question(pair[1])]\n",
    "    \n",
    "\n",
    "    return np.array([enc_question_pair(pair) for pair in qid_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate training/validation data\n",
    "\n",
    "Pseudo code:\n",
    "\n",
    "1. Random assign a vector for <RARE> tags, this vector values only used in this batch\n",
    "2. Generate negative samples by random assigning qid pairs. (The number of negative samples is decided by \"pos_ratio\" parameter)\n",
    "3. Encode all question pairs.\n",
    "        The same RARE word in a single question pair is gauranteed to be mapped to same vector.   \n",
    "4. Shuffle the order of all question pairs.\n",
    "5. Embed the question pairs.\n",
    "'''\n",
    "def gen_batch_data(pos_pairs, pos_ratio, swap=False):\n",
    "    \n",
    "    global embedding_matrix\n",
    "    \n",
    "    def random_assign_rare_vectors():\n",
    "        rare_idxes = [enc_map['<RARE'+str(i)+'>'] for i in range(30)]\n",
    "        for i in rare_idxes:\n",
    "            embedding_matrix[i] = np.random.rand(300)\n",
    "            \n",
    "    \n",
    "    def gen_shuffle_idxes(num):\n",
    "        a = np.arange(num)\n",
    "        random.shuffle(a)\n",
    "        return a\n",
    "    \n",
    "    def gen_negative_qid_pairs(num):\n",
    "        \n",
    "        '''\n",
    "        TODO:\n",
    "            Maybe we still need to check if generated question pair belongs to duplicated question samples.\n",
    "            Although I think this is not very necessay, but we still can give it a try if we have time.\n",
    "        '''\n",
    "        \n",
    "        id_list = list(qid_question_dict.keys())\n",
    "        rnd = lambda: id_list[random.randint(0,len(id_list)-1)]\n",
    "        return [ [rnd(),rnd()] for i in range(num)]\n",
    "    \n",
    "    # gaurantee <RARE> would be different in each batch\n",
    "    random_assign_rare_vectors()\n",
    "    \n",
    "    # generate negative samples to match the given pos:neg ratio\n",
    "    \n",
    "    pos_count = len(pos_pairs)\n",
    "    neg_count = int( ((1-pos_ratio)/pos_ratio) * pos_count )\n",
    "    \n",
    "    neg_pairs = gen_negative_qid_pairs(neg_count)\n",
    "    \n",
    "    pos = enc_question_pairs(pos_pairs, qid_question_dict, enc_map, swap=swap)\n",
    "    neg = enc_question_pairs(neg_pairs, qid_question_dict, enc_map)\n",
    "    X = np.vstack([pos,neg])\n",
    "    \n",
    "    # create y\n",
    "    \n",
    "    y = np.zeros(X.shape[0])\n",
    "    y[:pos.shape[0]] = 1\n",
    "    \n",
    "    # shuffle\n",
    "    \n",
    "    shuffle_idxes = gen_shuffle_idxes(y.shape[0])\n",
    "    X = X[shuffle_idxes]\n",
    "    y = y[shuffle_idxes]\n",
    "    \n",
    "    # embed X\n",
    "    \n",
    "    embed_q1 = embedding_matrix[X[:,0,:]]\n",
    "    embed_q2 = embedding_matrix[X[:,1,:]]\n",
    "    \n",
    "    return [embed_q1,embed_q2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the class weights to simulate the class disribution of testing samples\n",
    "#\n",
    "# These numbers and formula is got from here: \n",
    "#     https://www.kaggle.com/lystdo/quora-question-pairs/lstm-with-word2vec-embeddings#175198\n",
    "\n",
    "test_set_pos_label_ratio = 0.1746\n",
    "training_pos_ratio = 0.3\n",
    "\n",
    "weights = {\n",
    "    0: (1-test_set_pos_label_ratio) / (1-training_pos_ratio),\n",
    "    1: test_set_pos_label_ratio/training_pos_ratio\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "30/30 [==============================] - 15s - loss: 0.6760 - val_loss: 0.5117\n",
      "Epoch 2/1000\n",
      "30/30 [==============================] - 12s - loss: 0.4754 - val_loss: 0.4715\n",
      "Epoch 3/1000\n",
      "30/30 [==============================] - 12s - loss: 0.4171 - val_loss: 0.4765\n",
      "Epoch 4/1000\n",
      "30/30 [==============================] - 12s - loss: 0.3899 - val_loss: 0.4778\n",
      "Epoch 5/1000\n",
      " 4/30 [===>..........................] - ETA: 6s - loss: 0.3498\n",
      "Early stop by user\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def batch_generator(pairs,switch_position=False):\n",
    "    \n",
    "    pos_count = int(batch_size*training_pos_ratio)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        rnd_idxes = [random.randint(0,pairs.shape[0]-1) for i in range(pos_count)]\n",
    "        X, y = gen_batch_data(pairs[rnd_idxes], training_pos_ratio, swap=switch_position)\n",
    "        \n",
    "        yield X, y\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n",
    "]\n",
    "    \n",
    "try:\n",
    "#     model.fit(X_train, \n",
    "#               y_train,\n",
    "#               class_weight=weights,\n",
    "#               epochs=1000, \n",
    "#               batch_size=512,\n",
    "#               shuffle=True,\n",
    "#               validation_data=(X_val,y_val,validation_weights), \n",
    "#               callbacks=callbacks,\n",
    "#               verbose=1)\n",
    "    \n",
    "    model.fit_generator(batch_generator(train_pairs, switch_position=False),\n",
    "                        steps_per_epoch=30,\n",
    "                        class_weight=weights,\n",
    "                        epochs=1000,\n",
    "                        validation_data=(X_val,y_val,validation_weights), \n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('\\nEarly stop by user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the ROC_AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss= 0.534604601134\n",
      "roc_auc= 0.62890797396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "pred = model.predict(X_val)\n",
    "\n",
    "# pred_bin = np.zeros(len(pred))\n",
    "# pred_bin[(pred>0.5).reshape(len(pred))] = 0.999999\n",
    "# pred_bin[(pred<=0.5).reshape(len(pred))] = 0.000001\n",
    "\n",
    "pred[pred>0.999] = 0.999999\n",
    "pred[pred<=0.001] = 0.000001\n",
    "\n",
    "print('log_loss=',log_loss(y_val, pred, sample_weight=validation_weights))\n",
    "\n",
    "print('roc_auc=',roc_auc_score(y_val, pred, sample_weight=validation_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'HubertLin_naive_LSTM_dynamic_training'\n",
    "model.save('../model/'+model_name+'.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customized testcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('../model/HubertLin_naive_LSTM_alot_dropouts.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question_pair(question_pair_list, enc_map, capsulate_st_ed=False):\n",
    "\n",
    "    def enc_question(question):\n",
    "\n",
    "        if type(question)!=str or question==\"\":\n",
    "            if capsulate_st_ed:\n",
    "                return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def map_wrod(word):\n",
    "            if word in enc_map:\n",
    "                return enc_map[word]\n",
    "            elif word in rare_words:\n",
    "                idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                return enc_map['<RARE' + str(idx) + '>']\n",
    "            else:\n",
    "                rare_words.append(word)\n",
    "                return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "        # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "        separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "        single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "        words_list = re.findall(single_word+separator, question)\n",
    "\n",
    "        if capsulate_st_ed:\n",
    "            return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "        else:\n",
    "            return [map_wrod(word) for word in words_list]\n",
    "        \n",
    "    # This array is a local cache that identify those non-encoded rare words. \n",
    "    # We'll assign identical index (in this question pair) for each of rare words\n",
    "    rare_words = []\n",
    "    \n",
    "    return enc_question(question_pair_list[0]) , enc_question(question_pair_list[1])\n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]\n",
    "\n",
    "def clip_length(data, clip_st_ed=False):\n",
    "    \n",
    "    clip_len = 30\n",
    "    \n",
    "    if clip_st_ed:\n",
    "        data[0] = eval(data[0])[1:-1]\n",
    "        data[1] = eval(data[1])[1:-1]\n",
    "    \n",
    "    if len(data[0])<clip_len:\n",
    "        data[0] = data[0] + [enc_map['<ED>']] * (30-len(data[0]))\n",
    "    else:\n",
    "        data[0] = data[0][:30]\n",
    "        \n",
    "    if len(data[1])<clip_len:\n",
    "        data[1] = data[1] + [enc_map['<ED>']] * (30-len(data[1]))\n",
    "    else:\n",
    "        data[1] = data[1][:30]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a good man <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "A good man is me <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.54771608]]\n",
      "\n",
      "The <RARE0> is totally fucking awful <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "The <RARE0> is the best thing I have ever seen <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.1880715]]\n",
      "\n",
      "The <RARE0> is the best thing I have ever seen <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "The <RARE0> is totally fucking awful <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.15263924]]\n",
      "\n",
      "How <RARE0> can be used in cooking <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Can we find <RARE1> on the moon <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.26314893]]\n",
      "\n",
      "A bird is flying in the sky <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "My bird is flying over the ocean <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.43076953]]\n",
      "\n",
      "What is your favorite food <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Which is your favorite one <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.67051756]]\n",
      "\n",
      "<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "<RARE6> <RARE7> <RARE8> <RARE9> <RARE10> <RARE11> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.54163289]]\n",
      "\n",
      "<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.53129935]]\n",
      "\n",
      "what a <RARE0> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "what a <RARE0> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.41723025]]\n",
      "\n",
      "what a <RARE0> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "what a <RARE1> program I have never seen such <RARE0> code ever <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Predicting is same question proba = [[ 0.42482105]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl','rb'))\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))\n",
    "\n",
    "testcases = [\n",
    "    {\n",
    "        'q1': 'I am a good man',\n",
    "        'q2': 'A good man is me'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'The <RARE0> is totally fucking awful',\n",
    "        'q2': 'The <RARE0> is the best thing I have ever seen'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'The <RARE0> is the best thing I have ever seen',\n",
    "        'q2': 'The <RARE0> is totally fucking awful',\n",
    "    },\n",
    "    {\n",
    "        'q1': 'How <RARE0> can be used in cooking',\n",
    "        'q2': 'Can we find <RARE1> on the moon'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'A bird is flying in the sky',\n",
    "        'q2': 'My bird is flying over the ocean'\n",
    "    },\n",
    "    {\n",
    "        'q1': 'What is your favorite food',\n",
    "        'q2': 'Which is your favorite one'\n",
    "    },\n",
    "    {\n",
    "        'q1': '<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> ',\n",
    "        'q2': '<RARE6> <RARE7> <RARE8> <RARE9> <RARE10> <RARE11> ',\n",
    "    },\n",
    "    {\n",
    "        'q1': '<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> ',\n",
    "        'q2': '<RARE0> <RARE1> <RARE2> <RARE3> <RARE4> <RARE5> ',\n",
    "    },\n",
    "    {\n",
    "        'q1': ' what a <RARE0> program I have never seen such beatiful code ever',\n",
    "        'q2': ' what a <RARE0> program I have never seen such beatiful code ever',\n",
    "    },\n",
    "    {\n",
    "        'q1': ' what a <RARE0> program I have never seen such beatiful code ever',\n",
    "        'q2': ' what a <RARE1> program I have never seen such beatiful code ever',\n",
    "    },\n",
    "]\n",
    "\n",
    "for testcase in testcases:\n",
    "    \n",
    "    enc1, enc2 = enc_question_pair([testcase['q1'],testcase['q2']], enc_map)\n",
    "    enc1, enc2 = clip_length([enc1,enc2])\n",
    "    print(' '.join(dec_question(enc1,dec_map)))\n",
    "    print(' '.join(dec_question(enc2,dec_map)))\n",
    "\n",
    "    emb1 = embedding_matrix[np.array([enc1])]\n",
    "    emb2 = embedding_matrix[np.array([enc2])]\n",
    "\n",
    "    pred = model.predict([emb1,emb2])\n",
    "    print('Predicting is same question proba =', pred)\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to predict some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pickle.load(open('../dataset/processed/df_test_hubertLin_version.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'customer', 'lifetime', '<RARE0>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "['What', 'is', 'customer', 'lifetime', 'value', 'in', '<RARE1>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>', '<ED>']\n",
      "\n",
      "Predicting is same question proba = [[ 0.60896122]]\n"
     ]
    }
   ],
   "source": [
    "a = df_test.ix[186461]\n",
    "q1 = dec_question(a['question1'], dec_map)\n",
    "q2 = dec_question(a['question2'], dec_map)\n",
    "print(q1)\n",
    "print(q2)\n",
    "\n",
    "emb1 = embedding_matrix[np.array([a['question1']])]\n",
    "emb2 = embedding_matrix[np.array([a['question2']])]\n",
    "pred = model.predict([emb1, emb2])\n",
    "print('\\nPredicting is same question proba =', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "try:\n",
    "    model==None\n",
    "except:\n",
    "    model = load_model('../model/'+model_name+'.model')\n",
    "\n",
    "df_test = pickle.load(open('../dataset/processed/df_test_hubertLin_version.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 2345796\n",
      "100000 / 2345796\n",
      "200000 / 2345796\n",
      "300000 / 2345796\n",
      "400000 / 2345796\n",
      "500000 / 2345796\n",
      "600000 / 2345796\n",
      "700000 / 2345796\n",
      "800000 / 2345796\n",
      "900000 / 2345796\n",
      "1000000 / 2345796\n",
      "1100000 / 2345796\n",
      "1200000 / 2345796\n",
      "1300000 / 2345796\n",
      "1400000 / 2345796\n",
      "1500000 / 2345796\n",
      "1600000 / 2345796\n",
      "1700000 / 2345796\n",
      "1800000 / 2345796\n",
      "1900000 / 2345796\n",
      "2000000 / 2345796\n",
      "2100000 / 2345796\n",
      "2200000 / 2345796\n",
      "2300000 / 2345796\n",
      "Wall time: 48min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "partition_size = 10000\n",
    "\n",
    "def predict(i,q):\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        print(i*partition_size, '/', len(df_test))\n",
    "    \n",
    "    x1 = np.array(list(q['question1']))\n",
    "    x2 = np.array(list(q['question2']))\n",
    "    \n",
    "    x1 = embedding_matrix[x1]\n",
    "    x2 = embedding_matrix[x2]\n",
    "    \n",
    "    return model.predict([x1,x2])\n",
    "\n",
    "partition_len = len(df_test)//partition_size +1\n",
    "result = [predict(i,df_test.iloc[i*partition_size:(i+1)*partition_size]) for i in range(partition_len)]\n",
    "con = np.concatenate(result)\n",
    "df_result = pd.DataFrame({'test_id':np.arange(len(con)),'is_duplicate':con.reshape(len(con))}, columns=['test_id','is_duplicate'])\n",
    "df_result.to_csv('../result/prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction success\n"
     ]
    }
   ],
   "source": [
    "if len(df_result)!=2345796:\n",
    "    print('Your result prediction count is not fit to the testing data length 2345796 , yours:', len(df_result))\n",
    "else:\n",
    "    print('Prediction success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can a vacuum cleaner concentrate suck your eye out if it is pressed against your <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Could a vacuum cleaner suck get your eye out if directly pressed on the <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.813005\n",
      "\n",
      "Is web development just building <RARE0> best you get a web developer job if you know how to make a <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "Is web development just building <RARE0> Can you get a backend web developer job if you know how year make a <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.966372\n",
      "\n",
      "How do I overcome my shyness with <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "How do you overcome being <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.865856\n",
      "\n",
      "If I like a comment to a computer post by a friend on <RARE0> who will be able to see this <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "I want the public to see my posts on Facebook but only my friends to comment and <RARE2> How can I do <RARE3> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.884993\n",
      "\n",
      "What die good gifts for a foreign visitor to bring when <RARE0> invited to <RARE1> home in Australia for the first <RARE2> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "What are good gifts for a foreign visitor to bring when <RARE0> invited to <RARE1> home in Malta for the yes <RARE2> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.881261\n",
      "\n",
      "How can I speak English fluently and <RARE0> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "How can I speak english naturally <RARE1> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED> <ED>\n",
      "0.832783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "over = df_test[df_result['is_duplicate']>0.8]\n",
    "\n",
    "c = 0\n",
    "for i,s in over.iterrows():\n",
    "    print(' '.join(dec_question(s['question1'],dec_map)))\n",
    "    print(' '.join(dec_question(s['question2'],dec_map)))\n",
    "    print(df_result.ix[i]['is_duplicate'])\n",
    "    print('')\n",
    "    c+=1\n",
    "    if c>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
