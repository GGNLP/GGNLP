{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "Since many question pairs has same qid involve, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../dataset/raw/train.csv', delimiter=',')\n",
    "df_test = pd.read_csv('../dataset/raw/test.csv', delimiter=',')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all duplicated questions\n",
    "\n",
    "We'll construct two data structures:\n",
    "1. A dictionary records { qid : question_text } pair. \n",
    "\n",
    "         The question_text is only splited as a list of words, not yet transformed into encoded form. This gives us chances to do training more flexible.\n",
    "         \n",
    "2. A list of duplicated qid pairs \n",
    "\n",
    "        Recording only qid saves us data loading time (since we'll try to augment and enumerate a huge amount of duplicated question pairs).\n",
    "\n",
    "Note:\n",
    "\n",
    "I didn't record any single non-duplicated question relation. \n",
    "I decided to random assign any question pairs to be non-duplicated question pairs. This might causes some issue that similar to too-much-upsampling case, but this extremely increases the variety of non-duplicated question samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../dataset/raw/train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max qid =  537932\n"
     ]
    }
   ],
   "source": [
    "def get_max_qid(df):\n",
    "    max_qid = 0\n",
    "    for idx,frame in df.iterrows():\n",
    "        qid1 = int(frame['qid1'])\n",
    "        qid2 = int(frame['qid2'])\n",
    "        if qid1>max_qid:\n",
    "            max_qid = qid1\n",
    "        elif qid2>max_qid:\n",
    "            max_qid = qid2\n",
    "    print('Max qid = ', max_qid)\n",
    "    return max_qid\n",
    "\n",
    "max_qid = get_max_qid(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming groups of duplicated question pairs.\n",
    "\n",
    "EX: \n",
    "```\n",
    "if A==B and B==C:\n",
    "    group A,B,C as a group, then we can enumerate all combinations, including A==C as a new sample\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_questions(df):\n",
    "    group_id = 0\n",
    "    group_list = np.repeat(-1, max_qid)\n",
    "    \n",
    "    for idx,frame in df.iterrows():\n",
    "        qid1 = int(frame['qid1'])\n",
    "        qid2 = int(frame['qid2'])\n",
    "        \n",
    "        if int(frame['is_duplicate'])==1:\n",
    "            # if both has no group, add new group\n",
    "            if group_list[qid1]==-1 and group_list[qid2]==-1:\n",
    "                group_list[qid1] = group_id\n",
    "                group_list[qid2] = group_id\n",
    "                group_id += 1\n",
    "\n",
    "            # if both has group, join the group \n",
    "            elif group_list[qid1]!=-1 and group_list[qid2]!=-1 :\n",
    "                idxes_to_be_joined = np.where(group_list==group_list[qid2])[0]\n",
    "                group_list[idxes_to_be_joined] = group_list[qid1]\n",
    "\n",
    "            # only q1 has group , than add q2 to q1's group\n",
    "            elif  group_list[qid1]!=-1:\n",
    "                group_list[qid2] = group_list[qid1]\n",
    "\n",
    "            # only q2 has group , than add q1 to q2's group\n",
    "            elif  group_list[qid2]!=-1:\n",
    "                group_list[qid1] = group_list[qid2]\n",
    "                \n",
    "    return group_list\n",
    "    \n",
    "group_ids = group_questions(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149650"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(group_ids!=-1) # means these questions has group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the group and store it as a dictionary\n",
    "group_dict = {}\n",
    "for i in range(np.max(group_ids)+1):\n",
    "    group_members = np.where(group_ids==i)[0]\n",
    "    if len(group_members)>0:\n",
    "        group_dict[i] = group_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def enumerate_all_positive_cases(group_dict):\n",
    "    \n",
    "    def enumerate_inside_group(group):\n",
    "        return list(itertools.combinations(group, 2))\n",
    "    \n",
    "    return np.vstack(enumerate_inside_group(group_dict[group_id]) for group_id in group_dict)\n",
    "\n",
    "def duplicate_all(df):\n",
    "    \n",
    "    def get_qid_set():\n",
    "        ids = set()\n",
    "        for i,series in df.iterrows():\n",
    "            if series['qid1'] not in ids:\n",
    "                ids.add(series['qid1'])\n",
    "            if series['qid2'] not in ids:\n",
    "                ids.add(series['qid2'])\n",
    "        return ids\n",
    "    \n",
    "    id_set = get_qid_set()\n",
    "    return [[i,i] for i in id_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enumerate all cases of duplicated question pairs from each group\n",
    "enumerate_pairs = enumerate_all_positive_cases(group_dict)\n",
    "\n",
    "# The question pairs with itself is also a sample of duplicated question pair\n",
    "# duplicate_pairs = duplicate_all(df_train) \n",
    "\n",
    "all_pos_pairs = enumerate_pairs\n",
    "# all_pos_pairs = np.vstack([enumerate_pairs,duplicate_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The duplicate question pair count grows from 149263 to 228548\n"
     ]
    }
   ],
   "source": [
    "print('The duplicate question pair count grows from {} to {}'.format(len(df_train[df_train['is_duplicate']==1]),len(all_pos_pairs))) # The total duplicated samples count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove validation set from training set\n",
    "\n",
    "We should gaurantee that the questions in validation set never appears in the training set. Since we enumerate all possible combinations of positive question pairs, using questions in training set as validation set is very weird and risky.\n",
    "\n",
    "我們要保證 validation set 裡面的 data 與 training set 裡面的 data 互不交及，否則 training set 裡面包含者 validation set 的資訊是危險而且詭異的做法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos_rate_in_training_set():\n",
    "    dup = np.array(df_train['is_duplicate'])\n",
    "    pos_ratio = np.sum(dup) / dup.shape[0]\n",
    "    return pos_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since we are very probable to use a single question for several times, we should remove validation samples directly at this point\n",
    "# Fro example, Q_a == Q_b and Q_b == Q_c ,\n",
    "# In my method, we'll generate a new data Q_a == Q_c\n",
    "# If we move this Q_a == Q_c sample to validation set,\n",
    "#     it is very weird that our training set already has this kind of information (can be recognized from the Q_a == Q_b == Q_c relation).\n",
    "\n",
    "import random\n",
    "\n",
    "validation_size = 20000 # an approximation, final result can be slightly more than this number\n",
    "\n",
    "def split_val(pos_pairs, val_pos_ratio):\n",
    "    \n",
    "    # an estimation of how many data should be split from training set\n",
    "    split_pos_size = int(validation_size * val_pos_ratio)\n",
    "    \n",
    "    qids = pos_pairs.flatten()\n",
    "    \n",
    "    # totally remove those selected qids from training set\n",
    "    val_bools = np.repeat(False,pos_pairs.shape[0])\n",
    "    while(np.sum(val_bools)<split_pos_size):\n",
    "        rnd_qid = random.randint(0,len(qids)-1)\n",
    "        val_single_bool = np.bitwise_or(pos_pairs[:,0]==rnd_qid,pos_pairs[:,1]==rnd_qid)\n",
    "        val_bools = np.bitwise_or(val_bools, val_single_bool)\n",
    "    val_idxes = np.where(val_bools)[0]\n",
    "    \n",
    "    val = pos_pairs[val_idxes]\n",
    "    train = np.delete(pos_pairs, val_idxes, axis=0)\n",
    "    \n",
    "    return train, val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pos_ratio = get_pos_rate_in_training_set()\n",
    "train_pos_pairs, val_pos_pairs = split_val(all_pos_pairs, val_pos_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training positive pairs: 221150\n",
      "Validation positive pairs: 7398\n"
     ]
    }
   ],
   "source": [
    "print('Training positive pairs:', train_pos_pairs.shape[0])\n",
    "print('Validation positive pairs:', val_pos_pairs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(train_pos_pairs, open('../dataset/processed/train_positive_qid_pairs.pkl', 'wb'))\n",
    "pickle.dump(val_pos_pairs, open('../dataset/processed/validation_positive_qid_pairs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record non-duplicate question pairs\n",
    "\n",
    "Not considering qid this time.\n",
    "\n",
    "我是覺得 testing data 裡面也有包含 training data 的 qid ，而且 negative pair 不像 positive pair 那樣有連鎖性關係，所以在這邊就不處理了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_duplicate = df_train[df_train['is_duplicate']==0]\n",
    "non_dup_question_pairs = np.array([[series['qid1'],series['qid2']] for i,series in non_duplicate.iterrows()])\n",
    "\n",
    "val_pos_count = val_pos_pairs.shape[0]\n",
    "val_total_count = val_pos_count / val_pos_ratio\n",
    "val_neg_count = int(val_total_count - val_pos_count)\n",
    "\n",
    "val_neg_idxes = [random.randint(0,len(non_dup_question_pairs)-1) for i in range(val_neg_count)]\n",
    "\n",
    "val_neg_pairs = non_dup_question_pairs[val_neg_idxes]\n",
    "train_neg_pairs = np.delete(non_dup_question_pairs, val_neg_idxes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauranteed non-duplicated question pair length is  255027 \n",
      "\n",
      "Training negative pairs: 242730\n",
      "Validation negative pairs: 12640\n"
     ]
    }
   ],
   "source": [
    "print('Gauranteed non-duplicated question pair length is ', len(non_dup_question_pairs), '\\n')\n",
    "\n",
    "print('Training negative pairs:', train_neg_pairs.shape[0])\n",
    "print('Validation negative pairs:', val_neg_pairs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(train_pos_pairs, open('../dataset/processed/train_negative_qid_pairs.pkl', 'wb'))\n",
    "pickle.dump(val_pos_pairs, open('../dataset/processed/validation_negative_qid_pairs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation final size: 20038\n"
     ]
    }
   ],
   "source": [
    "print('Validation final size:', val_neg_pairs.shape[0] + val_pos_pairs.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse original training DataFrame to words list and store it\n",
    "\n",
    "Note: \n",
    "\n",
    "Not encoded yet, we need to map rare words to same `<RARE_X>` special token in each question pair. This should be done in training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../dataset/raw/train.csv', delimiter=',')\n",
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_wrod_list(question):\n",
    "    \n",
    "    if type(question)!=str:\n",
    "        return []\n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "    special_chars = '[\\s\\'!\"\\?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]'\n",
    "    pre_separator = '(?='+special_chars+')'\n",
    "    post_separator = '(?='+special_chars+'|$)'\n",
    "    single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "    return re.findall(special_chars+single_word+post_separator, question)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_qid_question_dict(df, parse=False):\n",
    "    res = {}\n",
    "    for i,frame in df.iterrows():\n",
    "        \n",
    "        qid1 = int(frame['qid1'])\n",
    "        if qid1 not in res:\n",
    "            if parse:\n",
    "                res[qid1] = parse_wrod_list(frame['question1'])\n",
    "            else:\n",
    "                res[qid1] = frame['question1']\n",
    "        \n",
    "        qid2 = int(frame['qid2'])\n",
    "        if qid2 not in res:\n",
    "            if parse:\n",
    "                res[qid2] = parse_wrod_list(frame['question2'])\n",
    "            else:\n",
    "                res[qid2] = frame['question2']\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# i = 20\n",
    "# rnd = [random.randint(0,len(df_train)-1) for ii in range(i)]\n",
    "# for r in rnd:\n",
    "#     print(df_train.ix[r]['question1'])\n",
    "#     print(parse_wrod_list(df_train.ix[r]['question1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gen_all_pos_df(qid_dict, all_pos_pairs):\n",
    "#     all_series = []\n",
    "#     column_names = ['qid1','qid2','question1','question2', 'is_duplicate']\n",
    "#     for i,pair in enumerate(all_pos_pairs):\n",
    "#         series = pd.Series([ pair[0], pair[1], qid_dict[pair[0]], qid_dict[pair[1]], 1 ], name=i)\n",
    "#         all_series.append(series)\n",
    "#     ret = pd.DataFrame(all_series)\n",
    "#     ret.columns = column_names\n",
    "#     return ret\n",
    "\n",
    "# all_pos_df = gen_all_pos_df(qid_dict, all_pos_pairs)\n",
    "# all_pos_df.head(10)\n",
    "# pickle.dump(all_pos_df, open('../dataset/processed/enumerate_all_positive_training_data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_question_dict = gen_qid_question_dict(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(training_question_dict, open('../dataset/processed/qid_question_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
