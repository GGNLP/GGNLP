{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition\n",
    "\n",
    "**<font color='red'>This kernel is equivalent to \"../release_kernel/Name_Entity_Recognition.ipynb\", there has fully documented idea and comments.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity test:\n",
      "[I, am, a, good, programmer, !]\n"
     ]
    }
   ],
   "source": [
    "print('Sanity test:')\n",
    "doc = nlp(u'I am a good programmer!')\n",
    "print([t for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hubert -> \n",
      "Hubert -> PERSON\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('Is Hubert a good programmer?')\n",
    "doc2 = nlp('Hubert is eating bugs!')\n",
    "\n",
    "print(doc1[1].text, '->', doc1[1].ent_type_) # can't recognize entity type\n",
    "print(doc2[0].text, '->', doc2[0].ent_type_) # can recognize entity type is a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags\n",
    "\n",
    "some hyper paramters can be tuned to generate different data\n",
    "\n",
    "#### is_substitute_proper_noun\n",
    "\n",
    "Now, for cases like : \"is Taiwan a part of China?\" => \"is country a part of country\"\n",
    "\n",
    "This simplifies question a lot, but loses a large portion of information. \n",
    "\n",
    "This is kind of \"over cleaning\" issue. May be normalize it using lemma is enough ?\n",
    "\n",
    "#### is_remove_stopwords\n",
    "\n",
    "Removing stopwords seems can reduce some noise. But at the same time, we loses some informations.\n",
    "\n",
    "#### is_remove_special_chars_first\n",
    "\n",
    "for cases like \"I have 3 years (36 months) experiance in coding.\", if we remove '(' early, my program will recognize '3 years' and '36 months' are contiguous numbers, and reduce them into a single number.\n",
    "\n",
    "EX:\n",
    "```\n",
    "'I have 3 years (36 months) experiance in coding.'\n",
    "\n",
    "(if flag True)   =>  \n",
    "['i', 'have', 'number', 'experiance', 'in', 'write', 'code'] # the \"number\" is yield from \"3 years 36 months\"\n",
    "\n",
    "(if flag False) =>  \n",
    "['i', 'have', 'number', 'number', 'experiance', 'in', 'write', 'code'] # the \"number\"s are yield from \"3 years\" and \"36 months\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_substitute_proper_noun = True # not yet done, a little complicated than it seems\n",
    "is_remove_stopwords = False # not yet tested\n",
    "is_remove_punctuations_first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if is_remove_stopwords:\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qid_dict = pickle.load(open('../augmented/qid_question_dict.pkl', 'rb'))\n",
    "df_test = pd.read_csv('../quora-question-pairs/test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pos_pairs = pickle.load(open('../augmented/train_positive_qid_pairs.pkl', 'rb'))\n",
    "train_neg_pairs = pickle.load(open('../augmented/train_negative_qid_pairs.pkl', 'rb'))\n",
    "\n",
    "val_pos_pairs = pickle.load(open('../augmented/validation_positive_qid_pairs.pkl', 'rb'))\n",
    "val_neg_pairs = pickle.load(open('../augmented/validation_negative_qid_pairs.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up question text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = {\n",
    "    'quoted': 'quoted_item',\n",
    "    'non-ascii': 'non_ascii_word',\n",
    "    'undefined': 'something'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_string(text):\n",
    "    \n",
    "    def pad_str(s):\n",
    "        return ' '+s+' '\n",
    "    \n",
    "    # Empty question\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "    \n",
    "    # preventing first and last word being ignored by regex\n",
    "    # and convert first word in question to lower case\n",
    "    \n",
    "    text = ' ' + text[0].lower() + text[1:] + ' '\n",
    "    \n",
    "    # replace all first char after either [.!?)\"'] with lowercase\n",
    "    # don't mind if we lowered a proper noun, it won't be a big problem\n",
    "    \n",
    "    def lower_first_char(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        return matched_string[:-1] + matched_string[-1].lower()\n",
    "    \n",
    "    text = re.sub(\"(?<=[\\.\\?\\)\\!\\'\\\"])[\\s]*.\",lower_first_char , text)\n",
    "    \n",
    "    # Replace weird chars in text\n",
    "    \n",
    "    text = re.sub(\"’\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"`\", \"'\", text) # special single quote\n",
    "    text = re.sub(\"“\", '\"', text) # special double quote\n",
    "    text = re.sub(\"？\", \"?\", text) \n",
    "    text = re.sub(\"…\", \" \", text) \n",
    "    text = re.sub(\"é\", \"e\", text) \n",
    "    \n",
    "    # Clean shorthands\n",
    "    \n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\W|^)([0-9]+)[kK](\\W|$)\", r\"\\1\\g<2>000\\3\", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    \n",
    "    # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with word \"number\"\n",
    "    \n",
    "    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    \n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    # all numbers should separate from words, this might be too aggressive\n",
    "    \n",
    "    def pad_number(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        return pad_str(matched_string)\n",
    "    text = re.sub('[0-9]+', pad_number, text)\n",
    "    \n",
    "    # add padding to punctuations and special chars, we still need them later\n",
    "    \n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    def pad_pattern(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        return pad_str(matched_string)\n",
    "    text = re.sub('[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\]', pad_pattern, text) \n",
    "        \n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii word with special word\n",
    "    \n",
    "    # indian dollar\n",
    "    \n",
    "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    \n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" III \", \" 3 \", text)\n",
    "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # typos identified with my eyes\n",
    "    \n",
    "    text = re.sub(r\" quikly \", \" quickly \", text)\n",
    "    text = re.sub(r\" unseccessful \", \" unsuccessful \", text)\n",
    "    text = re.sub(r\" demoniti[\\S]+ \", \" demonetization \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" demoneti[\\S]+ \", \" demonetization \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" addmision \", \" admission \", text)\n",
    "    text = re.sub(r\" insititute \", \" institute \", text)\n",
    "    text = re.sub(r\" connectionn \", \" connection \", text)\n",
    "    text = re.sub(r\" permantley \", \" permanently \", text)\n",
    "    text = re.sub(r\" sylabus \", \" syllabus \", text)\n",
    "    text = re.sub(r\" sequrity \", \" security \", text)\n",
    "    text = re.sub(r\" undergraduation \", \" undergraduate \", text) # not typo, but GloVe can't find it\n",
    "    text = re.sub(r\"(?=[a-zA-Z])ig \", \"ing \", text)\n",
    "    text = re.sub(r\" latop\", \" laptop\", text)\n",
    "    text = re.sub(r\" programmning \", \" programming \", text)  \n",
    "    text = re.sub(r\" begineer \", \" beginner \", text)  \n",
    "    text = re.sub(r\" qoura \", \" Quora \", text)\n",
    "    text = re.sub(r\" wtiter \", \" writer \", text)  \n",
    "    text = re.sub(r\" litrate \", \" literate \", text)  \n",
    "\n",
    "      \n",
    "    # for words like A-B-C-D or \"A B C D\", \n",
    "    # if A,B,C,D individuaally has vector in glove:\n",
    "    #     it can be treat as separate words\n",
    "    # else:\n",
    "    #     replace it as a special word, A_B_C_D is enough, we'll deal with that word later\n",
    "    #\n",
    "    # Testcase: 'a 3-year-old 4 -tier car'\n",
    "    \n",
    "    def dash_dealer(pattern):\n",
    "        matched_string = pattern.group(0)\n",
    "        splited = matched_string.split('-')\n",
    "        splited = [sp.strip() for sp in splited if sp!=' ' and sp!='']\n",
    "        joined = ' '.join(splited)\n",
    "        parsed = nlp(joined)\n",
    "        for token in parsed:\n",
    "            # if one of the token is not common word, then join the word into one single word\n",
    "            if not token.has_vector or token.text in SPECIAL_TOKENS.values():\n",
    "                return '_'.join(splited)\n",
    "        # if all tokens are common words, then split them\n",
    "        return joined\n",
    "\n",
    "    text = re.sub(\"[a-zA-Z0-9\\-]*-[a-zA-Z0-9\\-]*\", dash_dealer, text)\n",
    "    \n",
    "    # try to see if sentence between quotes is meaningful\n",
    "    # rule:\n",
    "    #     if exist at least one word is \"not number\" and \"length longer than 2\" and \"it can be identified by SpaCy\":\n",
    "    #         then consider the string is meaningful\n",
    "    #     else:\n",
    "    #         replace the string with a special word, i.e. quoted_item\n",
    "    # Testcase:\n",
    "    # i am a good (programmer)      -> i am a good programmer\n",
    "    # i am a good (programmererer)  -> i am a good quoted_item\n",
    "    # i am \"i am a\"                 -> i am quoted_item\n",
    "    # i am \"i am a programmer\"      -> i am i am a programmer\n",
    "    # i am \"i am a programmererer\"  -> i am quoted_item\n",
    "    \n",
    "    def quoted_string_parser(pattern):\n",
    "        string = pattern.group(0)\n",
    "        parsed = nlp(string[1:-1])\n",
    "        is_meaningful = False\n",
    "        for token in parsed:\n",
    "            # if one of the token is meaningful, we'll consider the full string is meaningful\n",
    "            if len(token.text)>2 and not token.text.isdigit() and token.has_vector:\n",
    "                is_meaningful = True\n",
    "            elif token.text in SPECIAL_TOKENS.values():\n",
    "                is_meaningful = True\n",
    "            \n",
    "        if is_meaningful:\n",
    "            return string\n",
    "        else:\n",
    "            return pad_str(string[0]) + SPECIAL_TOKENS['quoted'] + pad_str(string[-1])\n",
    "\n",
    "    text = re.sub('\\\".*\\\"', quoted_string_parser, text)\n",
    "    text = re.sub(\"\\'.*\\'\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\(.*\\)\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\[.*\\]\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\{.*\\}\", quoted_string_parser, text)\n",
    "    text = re.sub(\"\\<.*\\>\", quoted_string_parser, text)\n",
    "\n",
    "    text = re.sub('[\\(\\)\\[\\]\\{\\}\\<\\>\\'\\\"]', pad_pattern, text) \n",
    "    \n",
    "    # the single 's' in this stage is 99% of not clean text, just kill it\n",
    "    text = re.sub(' s ', \" \", text)\n",
    "    \n",
    "    # reduce extra spaces into single spaces\n",
    "    text = re.sub('[\\s]+', \" \", text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clean_all_training_set():\n",
    "    for k in qid_dict:\n",
    "        if k%100000==0:\n",
    "            print('Processed {} out of {}'.format(k,len(qid_dict)))\n",
    "        qid_dict[k] = clean_string(qid_dict[k])\n",
    "\n",
    "# clean_all_training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process all questions in qid_dict using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def run_spacy_on_all_questions():\n",
    "    \n",
    "    spacy_obj_dict = {}\n",
    "    total_len = len(qid_dict)\n",
    "    for i,k in enumerate(qid_dict):\n",
    "        if i%50000==0:\n",
    "            print('Processed {} out of {}'.format(i,total_len))\n",
    "\n",
    "        # some questions are null\n",
    "        if type(qid_dict[k])!= str:\n",
    "            qid_dict[k] = ''\n",
    "\n",
    "        spacy_obj_dict[k] = nlp(qid_dict[k])\n",
    "        \n",
    "# run_spacy_on_all_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace proper nouns in sentence to related types\n",
    "\n",
    "i.e. \n",
    "\n",
    "    'Google'  -> 'organization'\n",
    "    'Spanish' -> 'language'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dir(list(spacy_obj_dict[1].noun_chunks)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spacy_obj_dict[1][-1].tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ENTITY_ENUM = {\n",
    "    '': '',\n",
    "    'PERSON': 'person',\n",
    "    'NORP': 'nationality',\n",
    "    'FAC': 'facility',\n",
    "    'ORG': 'organization',\n",
    "    'GPE': 'country',\n",
    "    'LOC': 'location',\n",
    "    'PRODUCT': 'product',\n",
    "    'EVENT': 'event',\n",
    "    'WORK_OF_ART': 'artwork',\n",
    "    'LANGUAGE': 'language',\n",
    "    'DATE': 'date',\n",
    "    'TIME': 'time',\n",
    "#     'PERCENT': 'percent',\n",
    "#     'MONEY': 'money',\n",
    "#     'QUANTITY': 'quantity',\n",
    "#     'ORDINAL': 'ordinal',\n",
    "#     'CARDINAL': 'cardinal',\n",
    "    'PERCENT': 'number',\n",
    "    'MONEY': 'number',\n",
    "    'QUANTITY': 'number',\n",
    "    'ORDINAL': 'number',\n",
    "    'CARDINAL': 'number',\n",
    "    'LAW': 'law'\n",
    "}\n",
    "\n",
    "NUMERIC_TYPES = set([\n",
    "    'DATE',\n",
    "    'TIME',\n",
    "    'PERCENT',\n",
    "    'MONEY',\n",
    "    'QUANTITY',\n",
    "    'ORDINAL',\n",
    "    'CARDINAL',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea\n",
    "\n",
    "For example, for a word A, \n",
    "\n",
    "if SpaCy identifies A in question1 as a person, \n",
    "\n",
    "if SpaCy identifies A in question2 as a person, \n",
    "\n",
    "if SpaCy identifies A in question3 as an organization, \n",
    "\n",
    "A is more probable be a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if the second type count is extremely low, consider it as noise and the word has no second type\n",
    "threshold_of_second_type = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vote_dict = {}\n",
    "word_ent_type_dict = {}\n",
    "word_ent_type_second_dict = {}\n",
    "\n",
    "def create_vote_result():\n",
    "\n",
    "    # construct vote dictionary\n",
    "\n",
    "    for qid in spacy_obj_dict:\n",
    "\n",
    "        if qid%100000==0:\n",
    "            print('Processing {} / {}'.format(qid,len(spacy_obj_dict)))\n",
    "\n",
    "        for token in spacy_obj_dict[qid]:\n",
    "\n",
    "            if token.lower_ not in vote_dict:\n",
    "                vote_dict[token.lower_] = {}\n",
    "\n",
    "            if token.ent_type_ not in vote_dict[token.lower_]:\n",
    "                vote_dict[token.lower_][token.ent_type_] = 0\n",
    "\n",
    "            # if the token has_vector is True, maybe we shouldn't record its \n",
    "\n",
    "            vote_dict[token.lower_][token.ent_type_] += 1 # TODO: not sure if storing in lowercase form is safe ?\n",
    "\n",
    "\n",
    "    # vote for what should the type be\n",
    "\n",
    "    for key in vote_dict:\n",
    "\n",
    "        # non-type has lower priority\n",
    "        if '' in vote_dict[key]:\n",
    "            vote_dict[key][''] = vote_dict[key][''] - 0.1\n",
    "\n",
    "        ents = list(vote_dict[key].keys())\n",
    "        bi_list = [\n",
    "            ents,\n",
    "            [vote_dict[key][ent] for ent in ents]\n",
    "        ]\n",
    "\n",
    "        # if several ent_type_ have same count, just let it go, making them share same ent_type_ is enough\n",
    "        # TODO: if have time, can design a better metric to deal with second graded type\n",
    "        sorted_idx = np.argsort(bi_list[1])\n",
    "\n",
    "        if sorted_idx.shape[0]>1:\n",
    "            best_idx = sorted_idx[-1]\n",
    "            second_idx = sorted_idx[-2]\n",
    "            word_ent_type_dict[key] = bi_list[0][best_idx]\n",
    "            if bi_list[1][second_idx]>threshold_of_second_type:\n",
    "                word_ent_type_second_dict[key] = bi_list[0][second_idx]\n",
    "        else:\n",
    "            best_idx = sorted_idx[-1]\n",
    "            word_ent_type_dict[key] = bi_list[0][best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create_vote_result()\n",
    "\n",
    "# pickle.dump(vote_dict, open('./ent_type_vote_dict.pkl','wb'))\n",
    "# pickle.dump(word_ent_type_dict, open('./word_ent_type_dict.pkl','wb'))\n",
    "# pickle.dump(word_ent_type_second_dict, open('./word_ent_type_second_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vote_dict = pickle.load(open('./ent_type_vote_dict.pkl','rb'))\n",
    "word_ent_type_dict = pickle.load(open('./word_ent_type_dict.pkl','rb'))\n",
    "word_ent_type_second_dict = pickle.load(open('./word_ent_type_second_dict.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# APIs to access entity type\n",
    "\n",
    "def token_type_lookup(token, report_detail=False):\n",
    "    \n",
    "    if type(token)==str:\n",
    "        token = nlp(token)[0]\n",
    "        \n",
    "    key = token.lower_\n",
    "    \n",
    "    try:\n",
    "        if report_detail:\n",
    "            print(ENTITY_ENUM[word_ent_type_dict[key]], ' <= ', {ENTITY_ENUM[ent_t] : vote_dict[key][ent_t] for ent_t in vote_dict[key]} )\n",
    "\n",
    "        return word_ent_type_dict[key]\n",
    "    \n",
    "    except KeyError:\n",
    "        return ''\n",
    "\n",
    "def is_token_has_second_type(token):\n",
    "    \n",
    "    if type(token)==str:\n",
    "        token = nlp(token)[0]\n",
    "        \n",
    "    key = token.lower_\n",
    "    \n",
    "    try:\n",
    "        return key in word_ent_type_second_dict\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "def token_second_type_lookup(token, report_detail=False):\n",
    "    \n",
    "    if type(token)==str:\n",
    "        token = nlp(token)[0]\n",
    "        \n",
    "    key = token.lower_\n",
    "    \n",
    "    try:\n",
    "        if report_detail:\n",
    "            print(ENTITY_ENUM[word_ent_type_second_dict[key]], ' <= ', {ENTITY_ENUM[ent_t] : vote_dict[key][ent_t] for ent_t in vote_dict[key]} )\n",
    "\n",
    "        return word_ent_type_second_dict[key]\n",
    "    except KeyError:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms\n",
      "  <=  {'': 1235.9, 'organization': 101, 'date': 13, 'facility': 26, 'number': 2, 'person': 7, 'country': 13, 'artwork': 2, 'product': 4, 'language': 3}\n",
      "\n",
      "taiwanese\n",
      "nationality  <=  {'nationality': 25}\n",
      "\n",
      "google\n",
      "organization  <=  {'': 1304.9, 'organization': 1877, 'country': 234, 'location': 50, 'person': 588, 'facility': 65, 'product': 87, 'nationality': 11, 'law': 3, 'artwork': 28, 'date': 3}\n",
      "\n",
      "tensorflow\n",
      "  <=  {'organization': 5, '': 24.9, 'country': 5, 'person': 5, 'product': 1}\n",
      "\n",
      "mac\n",
      "  <=  {'': 457.9, 'person': 143, 'organization': 104, 'artwork': 3, 'date': 4, 'product': 11, 'facility': 4, 'country': 1, 'location': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testcases = [\n",
    "    'ms',\n",
    "    'taiwanese',\n",
    "    'google',\n",
    "    'tensorflow',\n",
    "    'mac'\n",
    "]\n",
    "\n",
    "for testcase in testcases:\n",
    "    print(testcase)\n",
    "    token_type_lookup(testcase, report_detail=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person  <=  {'': 92.9, 'person': 132, 'location': 2, 'country': 3, 'organization': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PERSON'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_lookup('Node', report_detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isn't this vey easy ? Why you write these dirty codes?\n",
    "\n",
    "here's a fast example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hubert -> \n",
      "Hubert -> PERSON\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('Is Hubert a good programmer?')\n",
    "doc2 = nlp('Hubert is eating bugs!')\n",
    "\n",
    "print(doc1[1].text, '->', doc1[1].ent_type_) # can't recognize entity type\n",
    "print(doc2[0].text, '->', doc2[0].ent_type_) # can recognize entity type is a person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What !? \n",
    "\n",
    "Yep, spaCy is powerful, but it doesn't always work. Its entity type prediction is noisy. So, here comes my solution:\n",
    "\n",
    "Let's go through the whole dataset first. Then we'll come back and tell you what this proper noun is in most cases.\n",
    "\n",
    "### Benefit\n",
    "\n",
    "1. Very stable \n",
    "2. The result is good for those proper nouns which appear many times in the training set\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. The word has very different meaning in different cases might suck. EX:\n",
    "\n",
    "        \"Can't you google that ? \" => ['can', 'not', 'you', 'organization', 'that']\n",
    "        \"I want to work in Google\" => ['i', 'want', 'to', 'work', 'in', 'organization']\n",
    "\n",
    "2. If a word A is a proper noun, but spaCy can't recognize its entity type in most cases. We have no clue to identify if the word is a very common word or a very rare word.\n",
    "\n",
    "        As a solution, I also recorded the second highest voted entity type. \n",
    "        If a word is abolutely a proper noun, we can choose to use its second highest entity type.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "is the context been over-cleaned ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Testcases\n",
    "# process_question_with_spacy(nlp('meet you on 2017-02-31')) # => meet you on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exception_list =  set(['need']) # spaCy identifies need's lamma as 'ne', which is not we want\n",
    "numeric_types = set(['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'])\n",
    "\n",
    "'''\n",
    "Pseudo code:\n",
    "\n",
    "for each token:\n",
    "\n",
    "    if token is in exception_list:\n",
    "        use its original form (i.e. \"need\" can be parsed as \"ne\" ... we absolutely will use \"need\")\n",
    "        \n",
    "    elif token is in SPECIAL_TOKENS:\n",
    "        if token's entity type can be identified:\n",
    "            use the ent_type_ detected\n",
    "        else:\n",
    "            use the special token directly\n",
    "        \n",
    "    elif token is a special character or not cleaned ' ' or 's':\n",
    "        skip it\n",
    "        \n",
    "    # cuz we don't want a single proper noun being expended into several words\n",
    "    # ex: \n",
    "    # 'meet you on 2017-02-31'\n",
    "    # (O) => meet you on date\n",
    "    # (X) => meet you on date date date\n",
    "    elif ( the current ent_type_ and previous ent_type_ are both numeric types in NUMERIC_TYPES ) or ( the current ent_type_ == previous ent_type_ ):\n",
    "        skip it\n",
    "        \n",
    "    elif ( token is labeled with an ent_type_ in word_ent_type_dict ):\n",
    "        replace the token with ent_type_ specified in word_ent_type_dict\n",
    "        \n",
    "    elif ( the transform only changes uppercases to lowercases ) and ( the changes happened not only on the first char ):\n",
    "        then it is a proper noun (in most cases) , we'll try to pick the largest ent_type_ in word_ent_type_dict which is not ''\n",
    "        \n",
    "    # i.e. the token's ent_type_ cannot be found, is either a very common word or a very rare word\n",
    "    else:\n",
    "        if token's lemma == '-PRON-':\n",
    "            it's a weird case, just use its original form\n",
    "        elif token's lemma has_vector is true: # i.e. can be identified by GloVe\n",
    "            use the lemma form\n",
    "        elif the token has second ent_type_ usable:\n",
    "            use the second ent_type_\n",
    "        elif the token itself has_vector is true:\n",
    "            use the token's raw text\n",
    "        # have no idea what happenned, most cases are \"very rare proper noun\" or \"type\"\n",
    "        else:\n",
    "            use \"something as a replacement\"\n",
    "            \n",
    "'''\n",
    "\n",
    "def process_question_with_spacy(spacy_obj, debug=False, show_fail=False, idx=None):\n",
    "\n",
    "    def not_alpha_or_digit(token):\n",
    "        ch = token.text[0]\n",
    "        return not (ch.isalpha() or ch.isdigit())\n",
    "    \n",
    "    result_word_list = []\n",
    "    res = ''\n",
    "    \n",
    "    # for continuous entity type string, we need only single term. \n",
    "    # EX: \"2017-01-01\"\n",
    "    # => \"time time time\" (X)\n",
    "    # => \"time\" (O)\n",
    "    previous_ent_type = None\n",
    "    \n",
    "    is_a_word_parsed_fail = False\n",
    "    fail_words = []\n",
    "    \n",
    "    for token in spacy_obj:\n",
    "        \n",
    "        global_ent_type = token_type_lookup(token)\n",
    "        \n",
    "        # problematic token, use its base form\n",
    "        if token.text in exception_list:\n",
    "            \n",
    "            previous_ent_type = None\n",
    "            result_word_list.append(token.text)\n",
    "        \n",
    "        # special kind of tokens\n",
    "        elif token.text in SPECIAL_TOKENS.values():\n",
    "            \n",
    "            # we have no choice but use the entity type detected by spaCy directly, but it still might fail\n",
    "            if token.ent_type_!='':\n",
    "                previous_ent_type = token.ent_type_\n",
    "                result_word_list.append(token.ent_type_)\n",
    "            \n",
    "            else:\n",
    "                previous_ent_type = None\n",
    "                result_word_list.append(token.text)\n",
    "                \n",
    "            \n",
    "        # skip none words tokens\n",
    "        elif not_alpha_or_digit(token) or token.text==' ' or token.text=='s':\n",
    "            previous_ent_type = None\n",
    "            if debug: print(token.text, ' : remove punc or special chars')\n",
    "            \n",
    "            \n",
    "        # if the \"remove stop word\" flag is set to True\n",
    "        elif is_remove_stopwords and token.lemma_ in is_remove_stopwords:\n",
    "            previous_ent_type = None\n",
    "            if debug: print(token.text, ' : remove stop word')\n",
    "        \n",
    "        \n",
    "        # contiguous same type, skip it\n",
    "        elif global_ent_type==previous_ent_type or token.ent_type_==previous_ent_type:\n",
    "            if debug: print('contiguous same type')\n",
    "        elif global_ent_type in NUMERIC_TYPES and previous_ent_type in NUMERIC_TYPES:\n",
    "            if debug: print('contiguous numeric')\n",
    "        elif token.ent_type_ in NUMERIC_TYPES and previous_ent_type in NUMERIC_TYPES:\n",
    "            if debug: print('contiguous numeric')\n",
    "                \n",
    "        \n",
    "        # number without an ent_type_\n",
    "        elif token.text.isdigit():\n",
    "            \n",
    "            if debug: print(token.text, 'force to be number')\n",
    "                \n",
    "            if previous_ent_type in NUMERIC_TYPES:\n",
    "                pass\n",
    "            else:\n",
    "                previous_ent_type = 'CARDINAL' # any number type would be okay\n",
    "                result_word_list.append('number')\n",
    "\n",
    "    \n",
    "        # replace proper nouns into name entities. \n",
    "        # EX:\n",
    "        # Original : Taiwan is next to China\n",
    "        # Result   : country is next to country \n",
    "        elif global_ent_type!='':\n",
    "            \n",
    "            result_word_list.append(ENTITY_ENUM[global_ent_type])\n",
    "            previous_ent_type = global_ent_type\n",
    "            if debug: print(token.text, ' : sub ent_type:', ENTITY_ENUM[global_ent_type])\n",
    "            \n",
    "            \n",
    "        # Identify if a word is proper noun or not, if it is a proper noun, we'll try to use second highest rated ent_type_\n",
    "        #\n",
    "        # A proper noun has following special patterns:\n",
    "        #     1. its lemma_ (base form) returned by spaCy is just its lowercase form\n",
    "        #     2. if one of its character except the first character is uppercase, it is a propernoun (in most cases)\n",
    "        # except the special cases like \"I LOVE YOU\", we cal say that if (1.) and (2.), then the token is proper noun\n",
    "        #\n",
    "        # for cases like \"Tensorflow\", we have no good rule to identify it is a proper noun or not ... let's just move on\n",
    "        elif token.lower_==token.lemma_ and token.text[1:]!=token.lemma_[1:] and is_token_has_second_type(token):\n",
    "            second_type = token_second_type_lookup(token)\n",
    "            result_word_list.append(ENTITY_ENUM[second_type])\n",
    "            if debug: print(token.text, ' : use second ent_type:', ENTITY_ENUM[second_type])\n",
    "            previous_ent_type = second_type\n",
    "        \n",
    "        \n",
    "        # words arrive here are either \"extremely common\" or \"extremely rare and has no method to deal with\"\n",
    "        else:\n",
    "            # A weird behavior of SpaCy, it substitutes [I, my, they] into '-PRON-', which mean pronoun (代名詞)\n",
    "            # More detail in : https://github.com/explosion/spaCy/issues/962\n",
    "            if token.lemma_=='-PRON-':\n",
    "                result_word_list.append(token.lower_)\n",
    "                res = token.lower\n",
    "                previous_ent_type = None\n",
    "            \n",
    "            # the lemma can be identified by GloVe\n",
    "            elif nlp(token.lemma_)[0].has_vector:\n",
    "                result_word_list.append(token.lemma_)\n",
    "                res = token.lemma_\n",
    "                previous_ent_type = None\n",
    "            \n",
    "            # the lemma cannot be identified, very probably a proper noun\n",
    "            elif is_token_has_second_type(token):\n",
    "                second_type = token_second_type_lookup(token)\n",
    "                result_word_list.append(ENTITY_ENUM[second_type])\n",
    "                res = ENTITY_ENUM[second_type]\n",
    "                previous_ent_type = second_type\n",
    "                if debug: print(token.text, ' : use second ent_type in else :', ENTITY_ENUM[second_type])\n",
    "            \n",
    "            # the lemma is not in glove and Spacy can't identify if it is a proper noun, last try, \n",
    "            #      if the word itself can be identified by GloVe or not\n",
    "            elif nlp(token.lower_)[0].has_vector:\n",
    "                result_word_list.append(token.lower_)\n",
    "                res = token.lower_\n",
    "                previous_ent_type = None\n",
    "                if debug: print(token.text, ' : the token itself can be identified :', token.lower_)\n",
    "            elif token.has_vector:\n",
    "                result_word_list.append(token.text)\n",
    "                res = token.text\n",
    "                previous_ent_type = None\n",
    "                if debug: print(token.text, ' : the token itself can be identified :', token.text)\n",
    "                \n",
    "            # Damn, I have totally no idea what's going on\n",
    "            # You got to deal with it by yourself\n",
    "            # In my case, I use fasttext to deal with it\n",
    "            else:\n",
    "                is_a_word_parsed_fail = True\n",
    "                fail_words.append(token.text)\n",
    "                previous_ent_type = None\n",
    "                \n",
    "                #  Question:\n",
    "                #  can we replace all this kind of word into \"something\" ?\n",
    "                result_word_list.append(SPECIAL_TOKENS['undefined'])\n",
    "                if debug: print(token.text, ' : can\\'t identify, replace with \"something\"')\n",
    "                \n",
    "    \n",
    "    if show_fail and is_a_word_parsed_fail:\n",
    "        if idx!=None:\n",
    "            print('At qid=', idx)\n",
    "        print('Fail words: ', fail_words)\n",
    "        print('Before:', spacy_obj.text)\n",
    "        print('After: ', ' '.join(result_word_list))\n",
    "        print('====================================================================')\n",
    "    \n",
    "    return np.array(result_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show part of the result\n",
    "\n",
    "you can see most of them are typo and rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for k in range(1,300):\n",
    "#     process_question_with_spacy(spacy_obj_dict[k], show_fail=True, idx=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_on_all_trainin_set():\n",
    "    qid_spacy_cleaned_question_dict = {}\n",
    "    qid_spacy_cleaned_word_lists_dict = {}\n",
    "\n",
    "    for k in spacy_obj_dict:\n",
    "\n",
    "        if i%50000==0:\n",
    "            print('Processed {} out of {}'.format(i,total_len))\n",
    "\n",
    "        word_list = process_question_with_spacy(spacy_obj_dict[k])\n",
    "        qid_spacy_cleaned_word_lists_dict[k] = word_list\n",
    "        qid_spacy_cleaned_question_dict[k] = ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# run_on_all_trainin_set()\n",
    "\n",
    "# # reduce questions to word lists\n",
    "# word_lists = list(qid_spacy_cleaned_word_lists_dict.values())\n",
    "# all_words = set(np.unique(np.hstack(word_lists)))\n",
    "\n",
    "# pickle.dump(qid_spacy_cleaned_word_lists_dict, open('./qid_spacy_cleaned_word_lists_dict.pkl','wb')) # used to get word2vec vectors\n",
    "# pickle.dump(qid_spacy_cleaned_question_dict, open('./qid_spacy_cleaned_question_dict.pkl','wb')) # used to get tf-idf result\n",
    "\n",
    "# # uses too much memory\n",
    "\n",
    "# del spacy_obj_dict\n",
    "# del qid_spacy_cleaned_question_dict\n",
    "# del qid_spacy_cleaned_word_lists_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a method to process any input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_new_string(s):\n",
    "    s = clean_string(s)\n",
    "    s = nlp(s)\n",
    "    s = process_question_with_spacy(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm Hubert, I am new to Kaggle and this is my first Kernel ! Hope you like It ~ \n",
      "=> hi i be hubert i be new to something and this be my number kernel hope you like it\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "spaCy excels at large-scale information extraction tasks. It's written from the ground up in carefully memory-managed Cython. Independent research has confirmed that spaCy is the fastest in the world. If your application needs to process entire web dumps, spaCy is the library you want to be using.\n",
      "=> spacy excel at large scale information extraction task it write from the ground up in carefully memory manage cython independent research have confirm that spacy be the fast in the world if your application need to process entire web dump spacy be the library you want to be use\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Google began in January 1996 as a research project by Larry Page and Sergey Brin when they were both PhD students at Stanford University in Stanford, California.[6]\n",
      "=> organization begin in date as a research project by person and person when they be both artwork student at organization in organization country quoted_item\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "3-year-old child can also be a good-programmer\n",
      "=> number child can also be a good programmer\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "what \"Пока\" means ?\n",
      "=> what non_ascii_word mean\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "what Пока means ?\n",
      "=> what non_ascii_word mean\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "IS Taiwan a Good place to Live ? \n",
      "=> be country a good place to live\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "How to being hired by google?\n",
      "=> how to be hire by organization\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "I ran 3mm\\/sec\n",
      "=> i run number sec\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Is trump a billionaire? How do we know for sure? Why do most people (Mark Cuban excepted) just take him at his word on this?\n",
      "=> be person a billionaire how do we know for sure why do most people mark person except just take him at his word on this\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Is it better to have a career in C, C++ or in Node.js?\n",
      "=> be it good to have a career in c c or in person js\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Which is better intel i5 (6th gen) or i7 (5th gen)?\n",
      "=> which be good organization i number number or i number number th gen\n",
      "\n",
      "========================================================================================================\n",
      "\n",
      "Which is correct: '2 dozen of eggs cost 30 rupees' or '2 dozen of eggs costs 30 rupees'?\n",
      "=> which be correct number of egg cost number or number of egg cost number\n",
      "\n",
      "========================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testcases = [\n",
    "    \n",
    "    # testcases writen by me or randomly copy-paste from websites\n",
    "    \n",
    "    'Hi! I\\'m Hubert, I am new to Kaggle and this is my first Kernel ! Hope you like It ~ ',\n",
    "    \"spaCy excels at large-scale information extraction tasks. It's written from the ground up in carefully memory-managed Cython. Independent research has confirmed that spaCy is the fastest in the world. If your application needs to process entire web dumps, spaCy is the library you want to be using.\",\n",
    "    'Google began in January 1996 as a research project by Larry Page and Sergey Brin when they were both PhD students at Stanford University in Stanford, California.[6]',\n",
    "    \n",
    "    # some weird testcases\n",
    "    \n",
    "    '3-year-old child can also be a good-programmer',\n",
    "    'what \"Пока\" means ?',\n",
    "    'what Пока means ?',\n",
    "    'IS Taiwan a Good place to Live ? ',\n",
    "    'How to being hired by google?',\n",
    "    'I ran 3mm\\/sec',\n",
    "    \n",
    "    # some testcases random pick from training set\n",
    "    \n",
    "    # Mark is detected as not a proper noun ... Poor Mark\n",
    "    'Is trump a billionaire? How do we know for sure? Why do most people (Mark Cuban excepted) just take him at his word on this?',\n",
    "    'Is it better to have a career in C, C++ or in Node.js?',\n",
    "    'Which is better intel i5 (6th gen) or i7 (5th gen)?',\n",
    "    \"Which is correct: '2 dozen of eggs cost 30 rupees' or '2 dozen of eggs costs 30 rupees'?\",\n",
    "]\n",
    "\n",
    "for testcase in testcases:\n",
    "    print(testcase)\n",
    "    print('=> ' + ' '.join(process_new_string(testcase)))\n",
    "    print('\\n========================================================================================================\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 out of 404290\n",
      "Processed 50000 out of 404290\n",
      "Processed 100000 out of 404290\n",
      "Processed 150000 out of 404290\n",
      "Processed 200000 out of 404290\n",
      "Processed 250000 out of 404290\n",
      "Processed 300000 out of 404290\n",
      "Processed 350000 out of 404290\n",
      "Processed 400000 out of 404290\n",
      "CPU times: user 16min 52s, sys: 224 ms, total: 16min 53s\n",
      "Wall time: 16min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SET = 'TRAIN' # or 'TEST' for test set\n",
    "record_all_words = False\n",
    "\n",
    "if SET == 'TRAIN':\n",
    "    df = pd.read_csv('../../dataset/quora-question-pairs/train.csv', delimiter=',')\n",
    "    file_name = './train_processed.txt'\n",
    "else:\n",
    "    df = pd.read_csv('../../dataset/quora-question-pairs/test.csv', delimiter=',')\n",
    "    file_name = './test_processed.txt'\n",
    "\n",
    "total_len = len(df)\n",
    "\n",
    "# although I want to store it in pickle, but the file is too large to store in memory QQ\n",
    "with open(file_name, 'w') as f:\n",
    "    for i,series in df.iterrows():\n",
    "\n",
    "        if i%10000==0:\n",
    "            print('Processed {} out of {}'.format(i,total_len))\n",
    "            \n",
    "        q1 = process_new_string(series['question1'])\n",
    "        q2 = process_new_string(series['question2'])\n",
    "\n",
    "        # write two questions in two lines\n",
    "        f.write(' '.join(q1) + '\\n')\n",
    "        f.write(' '.join(q2) + '\\n')\n",
    "        \n",
    "        \n",
    "        if record_all_words:\n",
    "            for word in q1:\n",
    "                all_words.add(word)\n",
    "            for word in q2:\n",
    "                all_words.add(word)\n",
    "            \n",
    "        del q1\n",
    "        del q2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_words, open('./all_words.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store into types can be processed by fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write to plain text, each line a word\n",
    "with open('./all_words.txt', 'w') as f:\n",
    "    for word in all_words:\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execute fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('./fasttext/fastText/fasttext print-word-vectors fasttext/wiki.en.bin < ./all_words.txt > ./embedded_vectors.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read the fasttext result and construct map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vec_map = {}\n",
    "\n",
    "with open('./embedded_vectors.vec','r') as f:\n",
    "    \n",
    "    vec_dim = 300\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        splited = line.split(' ')[:-1] # remove '\\n'\n",
    "        vec = np.array([float(sp) for sp in splited[-vec_dim:]])\n",
    "        if len(splited[:-vec_dim])>1:\n",
    "            print(splited[:-vec_dim])\n",
    "        word = ' '.join(splited[:-vec_dim])\n",
    "        word_vec_map[word] = vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(word_vec_map, open('./word_embedding_vector_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract TF-IDF feature\n",
    "\n",
    "p.s. the stored type is sparse_matrix, which need to transform into np_array again by method \"to_array()\", or, there will be errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print('Start fitting data into TfidfVectorizer ... ')\n",
    "\n",
    "question_list = list(qid_spacy_cleaned_question_dict.values())\n",
    "model = TfidfVectorizer(ngram_range=(1,1)) # TODO : ngram is a powerful setting, but not sure how and what value should we use.\n",
    "model.fit(question_list)\n",
    "\n",
    "# transform each question in dict and store it\n",
    "print('Start process each string ... ')\n",
    "qid_tfidf_sparse_matrix_dict = {}\n",
    "for qid in qid_spacy_cleaned_question_dict:\n",
    "    qid_tfidf_sparse_matrix_dict[qid] = model.transform([question_list[qid]])\n",
    "\n",
    "pickle.dump(qid_tfidf_sparse_matrix_dict, open('./qid_tfidf_sparse_matrix_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get spacy doc to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_dict = {}\n",
    "\n",
    "for i,k in enumerate(spacy_obj_dict):\n",
    "    if i%10000==0:\n",
    "        print('Processed {} out of {}'.format(i,total_len))\n",
    "        \n",
    "    doc2vec_dict[k] = spacy_obj_dict[k].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,(a,b) in enumerate(train_pos_pairs):\n",
    "    if i>20:\n",
    "        break\n",
    "    try:\n",
    "        print(spacy_obj_dict[a].similarity(spacy_obj_dict[b]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.986684250232\n",
    "0.924999549419\n",
    "0.929068045365\n",
    "0.730742430956\n",
    "0.846615892744\n",
    "0.640389531856\n",
    "0.878855919191\n",
    "0.855952899574\n",
    "0.738678928032\n",
    "0.996409017233\n",
    "0.946655548642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similarity(qid_list):\n",
    "    \n",
    "    ret_list = np.zeros(len(qid_list))\n",
    "    \n",
    "    for qid1, qid2 in qid_list:\n",
    "\n",
    "        try:\n",
    "            ret_list[i] = spacy_obj_dict[qid1].similarity(spacy_obj_dict[qid2])\n",
    "        except:\n",
    "            print('Missed qid pair: ', qid1, qid2, '.\\t Aborting ... ')\n",
    "            ret_list[i] = -1\n",
    "            break\n",
    "            \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_pos_similarity_list = get_similarity(val_pos_pairs)\n",
    "val_neg_similarity_list = get_similarity(val_neg_pairs)\n",
    "\n",
    "train_pos_similarity_list = get_similarity(train_pos_pairs)\n",
    "train_neg_similarity_list = get_similarity(train_neg_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
