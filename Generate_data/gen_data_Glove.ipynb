{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideas not yet done\n",
    "\n",
    "1. if a,b are labeled as same question pair, we can easily gen new training data by\n",
    "\n",
    "        a. for c!=a  =>  c!=b\n",
    "        b. for c==b && d==a  =>  c==d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../dataset/raw/train.csv', delimiter=',')\n",
    "df_test = pd.read_csv('../dataset/raw/test.csv', delimiter=',')\n",
    "df_train_size = 404290\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_questions = np.concatenate([np.array(df_train['question1']), np.array(df_train['question2'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse words\n",
    "\n",
    "Identify the words which are worth for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    # Clean the text\n",
    "    text = re.sub(\"what's\", \"what is \", text)\n",
    "    text = re.sub(\"\\'s\", \" \", text)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"cannot \", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am \", text)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\" e.g.\", \" eg \", text)\n",
    "    text = re.sub(\" b.g.\", \" bg \", text)\n",
    "    text = re.sub(\"(\\d+)(kK)\", \"\\g<1>000\", text)\n",
    "    text = re.sub(\"e-mail\", \"email\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def parse_single_words(question, idx=None):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "\n",
    "        Use regex parse all single words without special characters and non-ascii terms\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        List of string which is identified as a single word without special characters\n",
    "\n",
    "    Testcases:\n",
    "\n",
    "        # test first ten questions in dataset\n",
    "        for i in range(10):\n",
    "            print(i)\n",
    "            print(all_questions[i])\n",
    "            print(parse_single_words(all_questions[i]))\n",
    "            \n",
    "        # another good testcase\n",
    "        print(parse_single_words(\" '我是Hubert Lin, 這 code? 看起來還不錯ㄅ ? ha-ha-ha' \"))\n",
    "\n",
    "    '''\n",
    "    \n",
    "    ascii_only = False\n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < > -\n",
    "    separator_include_ascii = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|[^\\x00-\\x7F])'\n",
    "    separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-])'\n",
    "    \n",
    "    single_word = '[a-zA-Z0-9]+[a-zA-Z0-9_]*' # rule is at least one eng character or number\n",
    "    \n",
    "    try:\n",
    "        question = clean_string(question)\n",
    "        if not ascii_only:\n",
    "            words = re.findall( single_word + separator_include_ascii + '|[^\\x00-\\x7F]+' , question)\n",
    "\n",
    "        elif ascii_only:\n",
    "            words = re.findall( single_word + separator , question)\n",
    "    except:\n",
    "        idx = idx if idx<df_train_size else idx-df_train_size\n",
    "        print('Got execption while handling' , idx, '-th question: ', question)\n",
    "        print('There are two question pairs has empty string, got to deal with that')\n",
    "        words = []\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got execption while handling 105780 -th question:  nan\n",
      "There are two question pairs has empty string, got to deal with that\n",
      "Got execption while handling 201841 -th question:  nan\n",
      "There are two question pairs has empty string, got to deal with that\n"
     ]
    }
   ],
   "source": [
    "# Can't do this since return values are lists with different lengths, \n",
    "# this will cause error while np tries to transform all results into np array\n",
    "#\n",
    "# parse_single_words = np.vectorize(parse_single_words)\n",
    "# all_questions_parsed = parse_single_words(all_questions)\n",
    "\n",
    "all_questions_parsed = [parse_single_words(question, idx=i) for i,question in enumerate(all_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten_word_vec = np.concatenate(all_questions_parsed)\n",
    "unique_word_vec, unique_word_count = np.unique(flatten_word_vec, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109755  unique words\n",
      "35648 words occurs more (or equal) than 5 times\n"
     ]
    }
   ],
   "source": [
    "# Remove noise words, whom occurs less than threshold\n",
    "# More words may increase performance, but increases size of word-embedding matrix\n",
    "occurrence_threshold = 5\n",
    "selected_words = unique_word_vec[unique_word_count>=occurrence_threshold]\n",
    "\n",
    "print(len(unique_word_vec), ' unique words')\n",
    "print(len(selected_words), 'words occurs more (or equal) than', occurrence_threshold , 'times')\n",
    "\n",
    "json.dump(list(selected_words), open('../dataset/processed/selected_words.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0dev | 2f | 4d | 797 | 182cm | 002 | 6yr | 820s | 4mm | 1550nm | 1th | 1180 | 1m3 | A1c | 1985 | AFBC | 2000 | AA | 2cgpa | 8300 | 1267 | ANSYS | 7segment | 11808 | 37a | 12071 | 80000m | 5lbs | 3121 | AFFECTIVE | 63000gs | 200000ph | AGO | 692 | 100000m | ASTRONOMY | 7mo | 20cm | 800rpm | AUTOCAD | 7500u | 7engine | 291 | 5500U | 8440p | 09 | 1334 | 4167 | 5cm | 38000'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "trash_words = unique_word_vec[np.logical_and(unique_word_count>3, unique_word_count<5)]\n",
    "' | '.join([unique_word_vec[random.randint(0,len(trash_words)-1)] for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate encode/decode mapping\n",
    "\n",
    "Encode the question into integers lists, this can reduce time consumption while we do word embedding on training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_enc_dec_mapping(words):\n",
    "    \"\"\"\n",
    "    enc_map: voc --encode--> id\n",
    "    dec_map: id --decode--> voc\n",
    "    \"\"\"\n",
    "    \n",
    "    enc_map, dec_map = {}, {}\n",
    "    \n",
    "    # add special control characters\n",
    "    # <ST>    : start of question\n",
    "    # <ED>    : end of question\n",
    "    # <RARE>  : rare words occurrence is lower than threshold\n",
    "    # <EMPTY> : no word in such position\n",
    "    special_characters = [ '<ST>', '<ED>', '<RARE>', '<EMPTY>']\n",
    "    sp_len = len(special_characters)\n",
    "    for i,sp in enumerate(special_characters):\n",
    "        enc_map[sp] = i\n",
    "        dec_map[i] = sp\n",
    "    \n",
    "    # add our words into mapping\n",
    "    for i,word in enumerate(words):\n",
    "        enc_map[word] = sp_len+i\n",
    "        dec_map[sp_len+i] = word\n",
    "    \n",
    "    return enc_map, dec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_map, dec_map = build_enc_dec_mapping(selected_words)\n",
    "\n",
    "# save enc/decoding map to disk\n",
    "# json.dump(enc_map, open('dataset/processed/enc_map.json', 'w'))\n",
    "# json.dump(dec_map, open('dataset/processed/dec_map.json', 'w'))\n",
    "pickle.dump(enc_map, open('../dataset/processed/enc_map.pkl', 'wb'))\n",
    "pickle.dump(dec_map, open('../dataset/processed/dec_map.pkl', 'wb'))\n",
    "vocab_size = len(dec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question(question, enc_map):\n",
    "    \n",
    "    if type(question)!=str or question==\"\":\n",
    "        return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "    \n",
    "    def map_wrod(word):\n",
    "        return enc_map[word] if word in enc_map else enc_map['<RARE>']\n",
    "         \n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "    separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "    single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "    \n",
    "    words_list = re.findall(single_word+separator, question)\n",
    "    \n",
    "    return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 34035, 35445, 27101, 1]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl', 'rb'))\n",
    "enc_question(\"two-year-old\", enc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  glove is a good library\n",
      "Encoded:  [0, 22216, 24245, 14111, 22258, 24979, 1]\n",
      "Decoded:  ['<ST>', 'glove', 'is', 'a', 'good', 'library', '<ED>']\n"
     ]
    }
   ],
   "source": [
    "# testcase\n",
    "question = 'glove is a good library'\n",
    "enc = enc_question(question, enc_map)\n",
    "dec = dec_question(enc, dec_map)\n",
    "\n",
    "print('Original question: ', question)\n",
    "print('Encoded: ', enc)\n",
    "print('Decoded: ', dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode original training data frame and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_df(df, enc_map):\n",
    "    \n",
    "    def process_frame(frame):\n",
    "        frame['question1'] = enc_question(frame['question1'], enc_map)\n",
    "        frame['question2'] = enc_question(frame['question2'], enc_map)\n",
    "        return frame\n",
    "    \n",
    "    return df.apply(process_frame, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 13732, 24245, 33281, 32261, 16641, 32261, ...</td>\n",
       "      <td>[0, 13732, 24245, 33281, 32261, 16641, 32261, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 13732, 24245, 33281, 32345, 27057, 7357, 2...</td>\n",
       "      <td>[0, 13732, 35366, 22643, 23380, 33281, 6593, 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 6226, 16744, 6303, 23621, 33281, 31961, 27...</td>\n",
       "      <td>[0, 6226, 16744, 6691, 31961, 15774, 23622, 16...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 13767, 14758, 6303, 25805, 34670, 2, 6226,...</td>\n",
       "      <td>[0, 5093, 33281, 29916, 35126, 2, 24245, 19780...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 13747, 27137, 19720, 23551, 34985, 2, 2, 2...</td>\n",
       "      <td>[0, 13747, 21442, 35366, 32790, 23551, 30656, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          question1  \\\n",
       "0   0  [0, 13732, 24245, 33281, 32261, 16641, 32261, ...   \n",
       "1   1  [0, 13732, 24245, 33281, 32345, 27057, 7357, 2...   \n",
       "2   2  [0, 6226, 16744, 6303, 23621, 33281, 31961, 27...   \n",
       "3   3  [0, 13767, 14758, 6303, 25805, 34670, 2, 6226,...   \n",
       "4   4  [0, 13747, 27137, 19720, 23551, 34985, 2, 2, 2...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  [0, 13732, 24245, 33281, 32261, 16641, 32261, ...             0  \n",
       "1  [0, 13732, 35366, 22643, 23380, 33281, 6593, 2...             0  \n",
       "2  [0, 6226, 16744, 6691, 31961, 15774, 23622, 16...             0  \n",
       "3  [0, 5093, 33281, 29916, 35126, 2, 24245, 19780...             0  \n",
       "4  [0, 13747, 21442, 35366, 32790, 23551, 30656, ...             0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = preprocess_df(df_train, enc_map)\n",
    "\n",
    "# remove these two useless columns\n",
    "del df_preprocessed['qid1']\n",
    "del df_preprocessed['qid2']\n",
    "# df_preprocessed.to_csv('../dataset/processed/processed_training_data.csv', index=False)\n",
    "pickle.dump(df_preprocessed, open('../dataset/processed/processed_training_data.pkl', 'wb'))\n",
    "\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 6226, 19835, 33281, 12365, 10330, 22937, 6...</td>\n",
       "      <td>[0, 13767, 19438, 8587, 17288, 18329, 25335, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 11817, 6303, 22725, 14111, 22564, 33775, 1...</td>\n",
       "      <td>[0, 6226, 26367, 18389, 19835, 22564, 33775, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 13732, 16617, 24245, 33281, 15908, 35015, ...</td>\n",
       "      <td>[0, 13732, 35474, 31037, 26217, 33512, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 13747, 21633, 26847, 2, 1]</td>\n",
       "      <td>[0, 13732, 21635, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 6226, 2, 32194, 2, 1]</td>\n",
       "      <td>[0, 6226, 33291, 16744, 6303, 32194, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  [0, 6226, 19835, 33281, 12365, 10330, 22937, 6...   \n",
       "1        1  [0, 11817, 6303, 22725, 14111, 22564, 33775, 1...   \n",
       "2        2  [0, 13732, 16617, 24245, 33281, 15908, 35015, ...   \n",
       "3        3                     [0, 13747, 21633, 26847, 2, 1]   \n",
       "4        4                          [0, 6226, 2, 32194, 2, 1]   \n",
       "\n",
       "                                           question2  \n",
       "0  [0, 13767, 19438, 8587, 17288, 18329, 25335, 1...  \n",
       "1  [0, 6226, 26367, 18389, 19835, 22564, 33775, 2...  \n",
       "2       [0, 13732, 35474, 31037, 26217, 33512, 2, 1]  \n",
       "3                            [0, 13732, 21635, 2, 1]  \n",
       "4         [0, 6226, 33291, 16744, 6303, 32194, 2, 1]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = preprocess_df(df_test, enc_map)\n",
    "# df_preprocessed.to_csv('./dataset/processed/processed_testing_data.csv', index=False)\n",
    "pickle.dump(df_preprocessed, open('../dataset/processed/processed_testing_data.pkl', 'wb'))\n",
    "\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embedding matrix for GloVe\n",
    "\n",
    "Warning:\n",
    "\n",
    "Don't run this if not necessary. This takes about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embedding_matrix(w2v_path, dec_map, lang_dim):    \n",
    "    \n",
    "    out_vocab = []\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    print('Loading from GloVe file on hard disk ... ')\n",
    "    f = open(w2v_path, 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except Exception as e:\n",
    "            if e==KeyboardInterrupt:\n",
    "                raise KeyBoardInterrupt\n",
    "            print('exception word: ', values[0], ' ', values[1], ' ', values[2])\n",
    "            continue\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    print('Load complete, start embedding ... ')\n",
    "    embedding_matrix = np.random.rand(len(dec_map), lang_dim)\n",
    "    for idx, wd in dec_map.items():\n",
    "        if wd in embeddings_index.keys():\n",
    "            embedding_matrix[idx] = embeddings_index[wd]\n",
    "        else:\n",
    "            out_vocab.append(wd)\n",
    "    print('\\nNot in pre-trained vocabulary list:\\n', ','.join(out_vocab))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from GloVe file on hard disk ... \n",
      "exception word:  .   .   .\n",
      "exception word:  at   name@domain.com   0.0061218\n",
      "exception word:  .   .   .\n",
      "exception word:  to   name@domain.com   0.33865\n",
      "exception word:  .   .   0.035974\n",
      "exception word:  .   .   .\n",
      "exception word:  email   name@domain.com   0.33529\n",
      "exception word:  or   name@domain.com   0.48374\n",
      "exception word:  contact   name@domain.com   0.016426\n",
      "exception word:  Email   name@domain.com   0.37344\n",
      "exception word:  on   name@domain.com   0.037295\n",
      "exception word:  At   Killerseats.com   -0.13854\n",
      "exception word:  by   name@domain.com   0.6882\n",
      "exception word:  in   mylot.com   -0.18148\n",
      "exception word:  emailing   name@domain.com   0.39173\n",
      "exception word:  Contact   name@domain.com   0.14933\n",
      "exception word:  at   name@domain.com   0.44321\n",
      "exception word:  •   name@domain.com   -0.13288\n",
      "exception word:  at   Amazon.com   -0.5275\n",
      "exception word:  is   name@domain.com   -0.1197\n",
      "Load complete, start embedding ... \n",
      "\n",
      "Not in pre-trained vocabulary list:\n",
      " <ST>,<ED>,<RARE>,<EMPTY>,000Rs,10000INR,10000g,1000Rs,1000rs,1300D,1300d,15000gs,15000rs,160R,160r,1800_,19pencils,1lac,20000g,2000INR,2000Rs,2000rs,200ah,25000INR,251_,2750U,2K17,2fscript,30000g,30000rs,300grain,401000,50000g,5000gs,5005U,500Rs,500rs,5200U,6100U,6200U,6500U,6600000,66000g,7000gs,7Ghz,8009315079,816G,816g,820M,840M,8462852,8LPA,920M,940M,940MX,A2As,AADHAR,AIATS,AIIB,AIITS,ANTHE,ASEAM,Aachar,Aborignals,Achanakmar,Addepar,Aghoris,Ahok,Aiims,AirPods,Airpods,Ambhani,Ambrane,Anandiben,Ananthagiri,Angular2,AngularJs,Anushasan,Arthakranti,Ashwathama,Athavada,Atsume,Awdhesh,BBBJTC,BHIM,BIPC,BNBR,BREXIT,BYJU,Baahubali,Bakchod,Befikre,Bhakts,Biligiri,Biswapati,Blackcore,Blockchain,Bodnick,Bombil,Boruto,Brexit,Bulleya,Byju,C2090,C2150,C2180,C4040,CKWL,CPH4,CRYPTOL,Calexit,Cas9,Cgl,Chakravyuha,Champcash,Chandrakar,Chromecast,Ciaz,ClassPass,Codechef,Codeforces,Codevita,Coinbase,CrackVerbal,Crypt7,D3300,D5300,DCEU,DE0,DISEC,Daesh,DeepMind,Dehshat,Demonetisation,Demonetization,Deolo,DigitalOcean,DoorDash,Dream11,Drishyam,Drumpf,Dubsmash,Durpin,EXTC,EagerPanda,EduShastra,Elitmus,Eslands,Eyjafjallaj,FTRE,Fiitjee,Filepile,FinViz,Freecharge,Freeletics,Fullstack,FundsIndia,G360H,GSoc,GTBIT,Gateforum,Gathbandhan,Golang,Groupme,Gumasta,Gurugram,HAWALA,HackerEarth,HackerRank,Haptik,Healthkart,Homejoy,Hotstar,IGDTUW,IIEST,IIITD,IIPMR,INSOFE,IRNSS,Ibirit,Indeen,Indiape,Instacart,JECRC,JN0,Jact,Jeenkart,JoSAA,JoSSA,Kaabil,Kabali,Kakigarden,Kamchatca,Kaththi,Kaufmich,Kerbi,Khalilabad,Khowai,Kibana,Kombdi,Kongka,Kwid,LBSNAA,LHMC,LNGEDNA,LNMIIT,Laxmikanth,LeEco,Lenskart,Letsbuydrink,LikeALittle,ListenOnRepeat,Lycanroc,M20X,M452dw,M506n,M506x,M553x,M604dn,M604n,M605n,M605x,M606x,M712dn,M750n,MD101HN,MDlive,MH370,MT6752,Macbok,Maibong,Majboori,Mamry,Mangalyaan,Maqsad,Masaan,Massachisetts,Meritnation,Mi4i,Minance,MobiKwik,Musigma,MyJio,NA2O,NAVIC,NICMAR,NIELIT,NITian,NSEJS,NSITians,Nallamala,Ncert,NoFap,Nowazardan,Nowzaradan,Nutils,OKRs,Obscurial,OddEvenDobara,Odoo,Olacab,Olacabs,OneCoin,OneDrive,OnePlus,Oneplus,Ooaltah,OpenAI,Orthofill,PAYTM,PESSAT,PESU,PGDC,PGDIE,Paaus,Pakalu,Paksh,Paribhasha,Passowrds,Patreon,PayTM,PayU,PayUMoney,Payme0,Phogat,Pocketshopy,PokeCoins,Precent,Prega,Premam,Proavisor,Proze,Pythagurus,Q164,QUORA,Qoura,Quorans,RLWL,RSWL,Radhakishan,Rajyasabha,Ranogajec,Rapastinel,Ravindrababu,Ravula,ReactJS,Reddice,Redmi,Romedy,Roundchat,RuPay,Rulinskas,Rupay,Rutterdam,S7582,SAPUI5,SGSITS,SHAREit,SIMPs,STARBOY,S_,Sairat,Sandeepani,Satakarni,Scripbox,Sherkiy,Sherperd,Shimply,Shivaay,Shivsmarak,Shortfeeds,Sivalik,Smilevo,Snapchats,Somatra,Sooryavansham,Stemcentrx,Suragel,Susunia,Swacch,Swachh,Swiggy,T00J,T1530,T795,TISSNET,TQWL,TSPSC,TaxiForSure,TeachRipon,Telnor,TensorFlow,Tensorflow,Testbook,Theranos,ThunderQuote,Tirifjorden,TopProspect,Toptal,Truecaller,UCEED,UPPCS,USACO,UberBlack,UberGo,UberX,Undertale,Universalhunt,Upwork,Urjit,V151,VSSUT,Vailfivia,Vajiram,Vaporizergiant,Virali,VoLte,WHATSAPP,WN722N,Wynk,Xchrisone,Xiomi,Y55L,ZenFone,Zenfone,Zerodha,Zhihu,Zingat,Zondle,aFreeChat,abstractism,abundancestockadvisors,addanyproject,addmefast,adhaar,afcat,aiats,allbestlist,allgenericmedicine,amcat,ampann,amulyam,angular2,appjasmine,applock,aurangzeb,autoencoder,autowini,ayurvedically,benter,bestmytest,bhaang,bleuflamme,bookfi,boruto,brexit,buddybuild,buttermyresume,caccount,carnotaurus,carrierbuilder,cazpro,cherenkov,chsl,churchofsatan,circ11,circ4,circ5,circ6,clickbait,clickbuzzer,codeHS,codeacademy,codecademy,codeforces,codevita,codingparks,coep,conservativespirit,copilotsearch,cos2x,cos9,cpec,cph4,crypt8,cubesmith,cyrius,dangal,dellhi,demonetisation,demonetise,demonetised,demonetising,demonetize,demonetizing,demonitisation,demonitization,demonitize,demonitizing,deplorables,dfrac1,dosham,dubsmash,eLitmus,eatalo,edulous,elitmus,elivered,ennikkum,ethereum,fangdd,feedpost,fiitJEE,filmybaba,fitclown,flipagram,flixcart,fmovies,freebookcity,freecharge,friendzones,gearbest,gements,gojee,graffittibooks,grammarly,groupcommerce,gtbit,h_a_t,harbat,healthprofs,healthwarehouse,higoto,hotstar,housebuildup,iMeetzu,iPill,iVectorize,iimjobs,iinie,iisc,illetarate,incognitovpn,int_,intercast,intragency,iocl,isturb,itinindia,jallikattu,jhak,jobdreaming,kabali,keenmobi,keepsafe,kickasstorrents,kitchkarma,krazybutterfly,kvpy,laravel,lifebydesign,lim_,limerice,limits_,linkis,majboori,mangalik,materialtree,medels,mediahuman,meldonium,mhchem,mi4i,microservices,midnightexpressions,milaanj,motvitation,mouthswab,munjal,musturbating,myntra,mypromovideos,n3540,n3700,nanodegree,neered,neerings,neet2,negotiablesales,niammy,nightwears,nnering,noetics,nofap,nuco,nvalues,oneplus,orderyourchoice,osell,paytm,pcmc,percentilers,permantley,polyhouse,proadvisor,programmnig,qoura,quorans,redmi,reunificaton,rs1000,rs500,rs60000,rzea,saavn,sakaldiwipi,sanyogita,sasikala,scapetovacations,schizoids,scikit,scind,screenedrenters,senctioned,serieusly,shatri,shiuld,shivaay,sinA,sitesmatrix,snapchats,snapstreak,socialtrade,soundmagic,storeguide,strarving,studeing,subquantic,subsurfaces,sum_,syntel,techmahindra,testmysite,thedartmouth,tqwl,trackimei,tricks4me,truecaller,tterlinschrift,udemy,udemydownloader,unacademy,uncollapse,undergraduation,unifunds,unpop,untearable,unwaiveringly,upwork,urjit,usekit,usertesting,venmo,vmmc,voLTE,voot,wbJEE,whattsapp,wheystar,xtrade,xxxyyyzzz,y2mate,ygeia3,yureka,zenfone,zerocensorship,zomato,­,í‘,ƫҿϞɮ☉,ɐŧ,ɖ∀ཡز∂,ɽφʉʛƕ,ΙΧΣ,एक,का,नाम,बेहराल,​,​​,‎₹,‏,‪,‬,₹,⚪,〗,さあひる,不好意思,也是醉了,几,多少,横向卷轴,游戏,窩艾尼,﻿\n"
     ]
    }
   ],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl', 'rb'))\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "embedding_matrix = generate_embedding_matrix('../dataset/raw/glove.840B.300d.txt', dec_map, EMBEDDING_SIZE)\n",
    "\n",
    "pickle.dump(embedding_matrix, open('../dataset/processed/embedding_matrix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "↑↑↑↑↑ WOW ↑↑↑↑↑\n",
    "\n",
    "(TensorFlow is in this list LOL)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  4,  9, 16, 25, 36, 49, 64], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(9)\n",
    "np.square(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do a short experiment see if our embedding matrix works or not\n",
    "\n",
    "Does vector : \" London -> Paris \" contains semantic relationship and can approximate relationship of \" England -> France \" ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7265 9282 4135\n",
      "France   dist =  5.64297987282\n",
      "Paris   dist =  6.27527457426\n",
      "England   dist =  7.1538742503\n",
      "French   dist =  7.24948883275\n",
      "Lyon   dist =  7.3931232693\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "print(enc_map['London'], enc_map['Paris'], enc_map['England'])\n",
    "\n",
    "London = embedding_matrix[enc_map['London']]\n",
    "Paris = embedding_matrix[enc_map['Paris']]\n",
    "England = embedding_matrix[enc_map['England']]\n",
    "target = Paris - London + England\n",
    "\n",
    "distances = np.zeros(embedding_matrix.shape[0])\n",
    "for i,vec in enumerate(embedding_matrix):\n",
    "    distances[i] = sqrt(np.sum(np.square(vec-target)))\n",
    "\n",
    "bests = np.argsort(distances)[:5]\n",
    "for best in bests:\n",
    "    print(dec_map[best], '  dist = ', distances[best])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this embedding matrix afterward (Example in Keras)\n",
    "\n",
    "Note:\n",
    "\n",
    "Since some weird words is not embeddable, so we random assign a random value vector to related word. Although this workaround might have some problems such as : the random assigned vector is probably being very close to other non-related word vector, which makes our model mis-understanding the meaning of such word. But at least our model has ability to identify if two words are same or not.\n",
    "\n",
    "翻譯蒟蒻 (translation) :\n",
    "\n",
    "因為我們選定的不少字不存在於 embedding file 裡面，而我們的處理方法是直接隨機給予一個 embedding vector ，這雖然會讓我們的 model 誤認為這些字與某些其他字有相關性，但至少這樣的做法讓我們的 model 有能力辨認兩個字是不是一樣的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a short example how to embed encoded text into word vector\n",
    "\n",
    "def model():\n",
    "    \n",
    "    '''This model example is for Keras'''\n",
    "\n",
    "    # hyper-parameters that should be passed as function argument\n",
    "    \n",
    "    vocab_size = len(enc_map)\n",
    "    vocab_dim = EMBEDDING_SIZE # in my case is 300\n",
    "    text_length = 1 # how many words are we going to feed in one time\n",
    "\n",
    "    \n",
    "    '----------------------------------------------------------------------------'\n",
    "\n",
    "    \n",
    "    # embed our encoded question to embedded vector\n",
    "\n",
    "    encoded_question_input = Input(shape=(text_length,))\n",
    "\n",
    "    x = Embedding(output_dim = vocab_dim, \n",
    "                  input_dim = vocab_size, \n",
    "                  init = 'glorot_uniform',       # to be honest, I don't know what is this\n",
    "                  input_length = text_length, \n",
    "                  weights = [embedding_matrix]   # our embedding_matrix\n",
    "                 )(encoded_question_input)\n",
    "\n",
    "    text_embedded = Reshape((vocab_dim*text_length,))(x)\n",
    "\n",
    "    # maybe other input source\n",
    "\n",
    "    other_input = . . . \n",
    "    other_input_dim = . . .\n",
    "\n",
    "    # feed to RNN model\n",
    "\n",
    "    x = merge([text_embedded, other_input], mode='concat', concat_axis=-1)\n",
    "    x = Reshape((1, vocab_dim*text_length + other_input_dim ))(x)\n",
    "    x = GRU(128)(x)\n",
    "    out = Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # compile the model\n",
    "    \n",
    "    model = Model(input=[encoded_question_input, other_input], output=out)\n",
    "    # choose objective and optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=1e-3, clipnorm=clipnorm))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
