{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideas not yet done\n",
    "\n",
    "1. if a,b are labeled as same question pair, we can easily gen new training data by\n",
    "\n",
    "        a. for c!=a  =>  c!=b\n",
    "        b. for c==b && d==a  =>  c==d\n",
    "        \n",
    "2. Features:\n",
    "        \n",
    "        a. question length\n",
    "        b. how common the words in question is (need a metric to measure)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../dataset/raw/train.csv', delimiter=',')\n",
    "df_test = pd.read_csv('../dataset/raw/test.csv', delimiter=',')\n",
    "df_train_size = 404290\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_questions = np.concatenate([np.array(df_train['question1']), np.array(df_train['question2'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse words\n",
    "\n",
    "Identify the words which are worth for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    # Clean the text\n",
    "    text = re.sub(\"what's\", \"what is \", text)\n",
    "    text = re.sub(\"\\'s\", \" \", text)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"cannot \", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am \", text)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\" e.g.\", \" eg \", text)\n",
    "    text = re.sub(\" b.g.\", \" bg \", text)\n",
    "    text = re.sub(\"(\\d+)(kK)\", \"\\g<1>000\", text)\n",
    "    text = re.sub(\"e-mail\", \"email\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def parse_single_words(question, idx=None):\n",
    "    \n",
    "    '''\n",
    "    Usage:\n",
    "\n",
    "        Use regex parse all single words without special characters and non-ascii terms\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        List of string which is identified as a single word without special characters\n",
    "\n",
    "    Testcases:\n",
    "\n",
    "        # test first ten questions in dataset\n",
    "        for i in range(10):\n",
    "            print(i)\n",
    "            print(all_questions[i])\n",
    "            print(parse_single_words(all_questions[i]))\n",
    "            \n",
    "        # another good testcase\n",
    "        print(parse_single_words(\" '我是Hubert Lin, 這 code? 看起來還不錯ㄅ ? ha-ha-ha' \"))\n",
    "\n",
    "    '''\n",
    "    \n",
    "    ascii_only = False\n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < > -\n",
    "    separator_include_ascii = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|[^\\x00-\\x7F])'\n",
    "    separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-])'\n",
    "    \n",
    "    single_word = '[a-zA-Z0-9]+[a-zA-Z0-9_]*' # rule is at least one eng character or number\n",
    "    \n",
    "    try:\n",
    "        question = clean_string(question)\n",
    "        if not ascii_only:\n",
    "            words = re.findall( single_word + separator_include_ascii + '|[^\\x00-\\x7F]' , question)\n",
    "\n",
    "        elif ascii_only:\n",
    "            words = re.findall( single_word + separator , question)\n",
    "    except:\n",
    "        idx = idx if idx<df_train_size else idx-df_train_size\n",
    "        print('Got execption while handling' , idx, '-th question: ', question)\n",
    "        print('There are two question pairs has empty string, got to deal with that')\n",
    "        words = []\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got execption while handling 105780 -th question:  nan\n",
      "There are two question pairs has empty string, got to deal with that\n",
      "Got execption while handling 201841 -th question:  nan\n",
      "There are two question pairs has empty string, got to deal with that\n"
     ]
    }
   ],
   "source": [
    "# Can't do this since return values are lists with different lengths, \n",
    "# this will cause error while np tries to transform all results into np array\n",
    "#\n",
    "# parse_single_words = np.vectorize(parse_single_words)\n",
    "# all_questions_parsed = parse_single_words(all_questions)\n",
    "\n",
    "all_questions_parsed = [parse_single_words(question, idx=i) for i,question in enumerate(all_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten_word_vec = np.concatenate(all_questions_parsed)\n",
    "unique_word_vec, unique_word_count = np.unique(flatten_word_vec, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if word is in GloVe , if not , do some simple transformation. If this still doesn't work, trash the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from GloVe file on hard disk ... \n",
      "exception word:  .   .   .\n",
      "exception word:  at   name@domain.com   0.0061218\n",
      "exception word:  .   .   .\n",
      "exception word:  to   name@domain.com   0.33865\n",
      "exception word:  .   .   0.035974\n",
      "exception word:  .   .   .\n",
      "exception word:  email   name@domain.com   0.33529\n",
      "exception word:  or   name@domain.com   0.48374\n",
      "exception word:  contact   name@domain.com   0.016426\n",
      "exception word:  Email   name@domain.com   0.37344\n",
      "exception word:  on   name@domain.com   0.037295\n",
      "exception word:  At   Killerseats.com   -0.13854\n",
      "exception word:  by   name@domain.com   0.6882\n",
      "exception word:  in   mylot.com   -0.18148\n",
      "exception word:  emailing   name@domain.com   0.39173\n",
      "exception word:  Contact   name@domain.com   0.14933\n",
      "exception word:  at   name@domain.com   0.44321\n",
      "exception word:  •   name@domain.com   -0.13288\n",
      "exception word:  at   Amazon.com   -0.5275\n",
      "exception word:  is   name@domain.com   -0.1197\n"
     ]
    }
   ],
   "source": [
    "def load_glove():\n",
    "    embeddings_index = {}\n",
    "\n",
    "    print('Loading from GloVe file on hard disk ... ')\n",
    "    f = open('../dataset/raw/glove.840B.300d.txt', 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except Exception as e:\n",
    "            if e==KeyboardInterrupt:\n",
    "                raise KeyBoardInterrupt\n",
    "            print('exception word: ', values[0], ' ', values[1], ' ', values[2])\n",
    "            continue\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "    \n",
    "glove = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_keys_lowercases = {key.lower():key for key in glove}\n",
    "# approximately_mapping = {}\n",
    "\n",
    "def is_glove_embeddable(word):\n",
    "    \n",
    "    if word in glove:\n",
    "        return word\n",
    "    \n",
    "    if word.lower() in glove_keys_lowercases:\n",
    "        return glove_keys_lowercases[word.lower()]\n",
    "    \n",
    "    # Wanna deal with 'un-' or 'in-' prefix cases here, but seems not so neccesary\n",
    "    \n",
    "#     for key in glove.keys():\n",
    "#         if word in key\n",
    "#             approximately_mapping\n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110183  unique words\n",
      "47142 words occurs more (or equal) than 3 times\n"
     ]
    }
   ],
   "source": [
    "# Remove noise words, whom occurs less than threshold\n",
    "# More words may increase performance, but increases size of word-embedding matrix\n",
    "occurrence_threshold = 3\n",
    "frequent_words = unique_word_vec[unique_word_count>=occurrence_threshold]\n",
    "\n",
    "# let the GloVe do removing weird words for us\n",
    "glove_handlable_words = np.array([is_glove_embeddable(word) for word in frequent_words])\n",
    "\n",
    "selected_indexes = glove_handlable_words!=np.array([None])\n",
    "selected_words = frequent_words[selected_indexes]\n",
    "embedding_words_list = glove_handlable_words[selected_indexes]\n",
    "\n",
    "print(len(unique_word_vec), ' unique words')\n",
    "print(len(selected_words), 'words occurs more (or equal) than', occurrence_threshold , 'times')\n",
    "\n",
    "json.dump(list(selected_words), open('../dataset/processed/selected_words.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1030 | 130k | 14443 | 20__ | 156cm | 12v | 11yrs | 11301 | 10170 | 1023 | 17th | 2185 | 11pm | 150ibs | 1611 | 10pm | 1m | 13000mAh | 10months | 20MP | 24075 | 18yr | 196lbs | 15k | 1700th | 1055 | 16GB | 12H143 | 19pencils | 11class | 13cm | 130K | 11b | 1976 | 100CE | 1903 | 12inch | 12801 | 140M | 1yr | 11 | 100kg | 0w | 00am | 12203 | 2160p | 1547 | 1vs1 | 23400 | 0n7'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "trash_words = frequent_words[glove_handlable_words==np.array([None])]\n",
    "' | '.join([unique_word_vec[random.randint(0,len(trash_words)-1)] for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate encode/decode mapping\n",
    "\n",
    "Encode the question into integers lists, this can reduce time consumption while we do word embedding on training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_enc_dec_mapping(words):\n",
    "    \"\"\"\n",
    "    enc_map: voc --encode--> id\n",
    "    dec_map: id --decode--> voc\n",
    "    \"\"\"\n",
    "    \n",
    "    enc_map, dec_map = {}, {}\n",
    "    cannot_encode_words = []\n",
    "    \n",
    "    # add special control characters\n",
    "    # <ST>    : start of question\n",
    "    # <ED>    : end of question\n",
    "    # <RARE>  : rare words occurrence is lower than threshold\n",
    "    # <EMPTY> : no word in such position\n",
    "    rare_characters = ['<RARE'+str(i)+'>' for i in range(100)] # 100 is just a number that I think large enough\n",
    "    special_characters = [ '<ST>', '<ED>', '<EMPTY>'] + rare_characters\n",
    "    sp_len = len(special_characters)\n",
    "    for i,sp in enumerate(special_characters):\n",
    "        enc_map[sp] = i\n",
    "        dec_map[i] = sp\n",
    "    \n",
    "    # add our words into mapping\n",
    "    for i,word in enumerate(words):\n",
    "        if word in glove or is_glove_embeddable(word):\n",
    "            enc_map[word] = sp_len+i\n",
    "            dec_map[sp_len+i] = word\n",
    "            \n",
    "        else:\n",
    "            cannot_encode_words.append(word)\n",
    "            \n",
    "    print(' | '.join(cannot_encode_words))\n",
    "    \n",
    "    return enc_map, dec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enc_map, dec_map = build_enc_dec_mapping(selected_words)\n",
    "\n",
    "# save enc/decoding map to disk\n",
    "# json.dump(enc_map, open('dataset/processed/enc_map.json', 'w'))\n",
    "# json.dump(dec_map, open('dataset/processed/dec_map.json', 'w'))\n",
    "pickle.dump(enc_map, open('../dataset/processed/enc_map.pkl', 'wb'))\n",
    "pickle.dump(dec_map, open('../dataset/processed/dec_map.pkl', 'wb'))\n",
    "vocab_size = len(dec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc_question_pair(question_pair_list, enc_map, capsulate_st_ed=False):\n",
    "\n",
    "    def enc_question(question):\n",
    "\n",
    "        if type(question)!=str or question==\"\":\n",
    "            if capsulate_st_ed:\n",
    "                return [enc_map['<ST>'], enc_map['<ED>']]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def map_wrod(word):\n",
    "            if word in enc_map:\n",
    "                return enc_map[word]\n",
    "            elif word in rare_words:\n",
    "                idx = np.where(np.array(rare_words)==word)[0][0]\n",
    "                return enc_map['<RARE' + str(idx) + '>']\n",
    "            else:\n",
    "                rare_words.append(word)\n",
    "                return enc_map['<RARE' + str(len(rare_words)-1) + '>']\n",
    "\n",
    "        # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "        separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "        single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "        words_list = re.findall(single_word+separator, question)\n",
    "\n",
    "        if capsulate_st_ed:\n",
    "            return [enc_map['<ST>']] + [map_wrod(word) for word in words_list] + [enc_map['<ED>']] \n",
    "        else:\n",
    "            return [map_wrod(word) for word in words_list]\n",
    "        \n",
    "    # This array is a local cache that identify those non-encoded rare words. \n",
    "    # We'll assign identical index (in this question pair) for each of rare words\n",
    "    rare_words = []\n",
    "    \n",
    "    return enc_question(question_pair_list[0]) , enc_question(question_pair_list[1])\n",
    "    \n",
    "    \n",
    "def dec_question(question, dec_map):\n",
    "    return [dec_map[enc_value] for enc_value in question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  glove is a good library howww awesomeeeee this work is awesomeeeee\n",
      "Encoded:  [30119, 32666, 19889, 30182, 33650, 3, 4] [44304, 46811, 32666, 4]\n",
      "Decoded:  ['glove', 'is', 'a', 'good', 'library', '<RARE0>', '<RARE1>'] ['this', 'work', 'is', '<RARE1>']\n"
     ]
    }
   ],
   "source": [
    "# testcase\n",
    "question1 = 'glove is a good library howww awesomeeeee'\n",
    "question2 = 'this work is awesomeeeee'\n",
    "enc1, enc2 = enc_question_pair([question1, question2], enc_map)\n",
    "dec1 = dec_question(enc1, dec_map)\n",
    "dec2 = dec_question(enc2, dec_map)\n",
    "\n",
    "print('Original question: ', question1, question2)\n",
    "print('Encoded: ', enc1, enc2)\n",
    "print('Decoded: ', dec1, dec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embedding matrix for GloVe\n",
    "\n",
    "Warning:\n",
    "\n",
    "Don't run this if not necessary. This takes about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_embedding_matrix(glove, words_list, dec_map, lang_dim):    \n",
    "    \n",
    "    out_vocab = []\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    print('Load complete, start embedding ... ')\n",
    "    embedding_matrix = np.random.rand(len(dec_map), lang_dim)\n",
    "    for idx,word in enumerate(words_list):\n",
    "        if word in glove.keys():\n",
    "            embedding_matrix[idx] = glove[word]\n",
    "        else:\n",
    "            out_vocab.append(word)\n",
    "            \n",
    "    if len(out_vocab)>0:\n",
    "        print('\\nNot in pre-trained vocabulary list:\\n', ','.join(out_vocab))\n",
    "    else:\n",
    "        print('\\nAll words are embedded successfully')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load complete, start embedding ... \n",
      "\n",
      "All words are embedded successfully\n"
     ]
    }
   ],
   "source": [
    "dec_map = pickle.load(open('../dataset/processed/dec_map.pkl', 'rb'))\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "embedding_matrix = generate_embedding_matrix(glove, embedding_words_list, dec_map, EMBEDDING_SIZE)\n",
    "\n",
    "pickle.dump(embedding_matrix, open('../dataset/processed/embedding_matrix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode original training data frame and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_df(df, enc_map):\n",
    "    \n",
    "    def process_frame(frame):\n",
    "        question_pair = [frame['question1'], frame['question2']]\n",
    "        frame['question1'] , frame['question2'] = enc_question_pair(question_pair, enc_map)\n",
    "        return frame\n",
    "    \n",
    "    return df.apply(process_frame, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[19354, 32666, 44207, 42906, 23164, 42906, 304...</td>\n",
       "      <td>[19354, 32666, 44207, 42906, 23164, 42906, 304...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[19354, 32666, 44207, 43024, 36339, 10433, 3, ...</td>\n",
       "      <td>[19354, 46871, 30660, 31592, 44207, 9371, 3022...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[8861, 23291, 8972, 31895, 44207, 42532, 36339...</td>\n",
       "      <td>[8861, 23291, 9512, 42532, 22036, 31896, 23164...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[19400, 20710, 8972, 34717, 45992, 3, 8861, 23...</td>\n",
       "      <td>[7257, 44207, 39954, 46574, 5, 32666, 27075, 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[19372, 36439, 27001, 31811, 46391, 39195, 3, ...</td>\n",
       "      <td>[19372, 29136, 46871, 43570, 31811, 40880, 6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          question1  \\\n",
       "0   0  [19354, 32666, 44207, 42906, 23164, 42906, 304...   \n",
       "1   1  [19354, 32666, 44207, 43024, 36339, 10433, 3, ...   \n",
       "2   2  [8861, 23291, 8972, 31895, 44207, 42532, 36339...   \n",
       "3   3  [19400, 20710, 8972, 34717, 45992, 3, 8861, 23...   \n",
       "4   4  [19372, 36439, 27001, 31811, 46391, 39195, 3, ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  [19354, 32666, 44207, 42906, 23164, 42906, 304...             0  \n",
       "1  [19354, 46871, 30660, 31592, 44207, 9371, 3022...             0  \n",
       "2  [8861, 23291, 9512, 42532, 22036, 31896, 23164...             0  \n",
       "3  [7257, 44207, 39954, 46574, 5, 32666, 27075, 2...             0  \n",
       "4      [19372, 29136, 46871, 43570, 31811, 40880, 6]             0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = preprocess_df(df_train, enc_map)\n",
    "\n",
    "# remove these two useless columns\n",
    "del df_preprocessed['qid1']\n",
    "del df_preprocessed['qid2']\n",
    "# df_preprocessed.to_csv('../dataset/processed/processed_training_data.csv', index=False)\n",
    "pickle.dump(df_preprocessed, open('../dataset/processed/processed_training_data.pkl', 'wb'))\n",
    "\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[8861, 27137, 44207, 17454, 14563, 31023, 970,...</td>\n",
       "      <td>[19400, 26642, 12141, 23989, 25254, 34103, 208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[16669, 8972, 30757, 19889, 30559, 44818, 2146...</td>\n",
       "      <td>[8861, 35454, 25331, 27137, 30559, 44818, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[19354, 23134, 32666, 44207, 22210, 46426, 444...</td>\n",
       "      <td>[19354, 47026, 41368, 35250, 44492, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[19372, 29374, 36075, 3]</td>\n",
       "      <td>[19354, 29376, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[8861, 3, 42827, 4]</td>\n",
       "      <td>[8861, 44216, 23291, 8972, 42827, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  [8861, 27137, 44207, 17454, 14563, 31023, 970,...   \n",
       "1        1  [16669, 8972, 30757, 19889, 30559, 44818, 2146...   \n",
       "2        2  [19354, 23134, 32666, 44207, 22210, 46426, 444...   \n",
       "3        3                           [19372, 29374, 36075, 3]   \n",
       "4        4                                [8861, 3, 42827, 4]   \n",
       "\n",
       "                                           question2  \n",
       "0  [19400, 26642, 12141, 23989, 25254, 34103, 208...  \n",
       "1       [8861, 35454, 25331, 27137, 30559, 44818, 5]  \n",
       "2             [19354, 47026, 41368, 35250, 44492, 4]  \n",
       "3                                  [19354, 29376, 4]  \n",
       "4               [8861, 44216, 23291, 8972, 42827, 4]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = preprocess_df(df_test, enc_map)\n",
    "# df_preprocessed.to_csv('./dataset/processed/processed_testing_data.csv', index=False)\n",
    "pickle.dump(df_preprocessed, open('../dataset/processed/processed_testing_data.pkl', 'wb'))\n",
    "\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do a short experiment see if our embedding matrix works or not\n",
    "\n",
    "Does vector : \" London -> Paris \" contains semantic relationship and can approximate relationship of \" England -> France \" ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "print(enc_map['London'], enc_map['Paris'], enc_map['England'])\n",
    "\n",
    "London = embedding_matrix[enc_map['London']]\n",
    "Paris = embedding_matrix[enc_map['Paris']]\n",
    "England = embedding_matrix[enc_map['England']]\n",
    "target = Paris - London + England\n",
    "\n",
    "distances = np.zeros(embedding_matrix.shape[0])\n",
    "for i,vec in enumerate(embedding_matrix):\n",
    "    distances[i] = sqrt(np.sum(np.square(vec-target)))\n",
    "\n",
    "bests = np.argsort(distances)[:5]\n",
    "for best in bests:\n",
    "    print(dec_map[best], '  dist = ', distances[best])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this embedding matrix afterward (Example in Keras)\n",
    "\n",
    "Note:\n",
    "\n",
    "Since some weird words is not embeddable, so we random assign a random value vector to related word. Although this workaround might have some problems such as : the random assigned vector is probably being very close to other non-related word vector, which makes our model mis-understanding the meaning of such word. But at least our model has ability to identify if two words are same or not.\n",
    "\n",
    "翻譯蒟蒻 (translation) :\n",
    "\n",
    "因為我們選定的不少字不存在於 embedding file 裡面，而我們的處理方法是直接隨機給予一個 embedding vector ，這雖然會讓我們的 model 誤認為這些字與某些其他字有相關性，但至少這樣的做法讓我們的 model 有能力辨認兩個字是不是一樣的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a short example how to embed encoded text into word vector\n",
    "\n",
    "def model():\n",
    "    \n",
    "    '''This model example is for Keras'''\n",
    "\n",
    "    # hyper-parameters that should be passed as function argument\n",
    "    \n",
    "    vocab_size = len(enc_map)\n",
    "    vocab_dim = EMBEDDING_SIZE # in my case is 300\n",
    "    text_length = 1 # how many words are we going to feed in one time\n",
    "\n",
    "    \n",
    "    '----------------------------------------------------------------------------'\n",
    "\n",
    "    \n",
    "    # embed our encoded question to embedded vector\n",
    "\n",
    "    encoded_question_input = Input(shape=(text_length,))\n",
    "\n",
    "    x = Embedding(output_dim = vocab_dim, \n",
    "                  input_dim = vocab_size, \n",
    "                  init = 'glorot_uniform',       # to be honest, I don't know what is this\n",
    "                  input_length = text_length, \n",
    "                  weights = [embedding_matrix]   # our embedding_matrix\n",
    "                 )(encoded_question_input)\n",
    "\n",
    "    text_embedded = Reshape((vocab_dim*text_length,))(x)\n",
    "\n",
    "    # maybe other input source\n",
    "\n",
    "    other_input = . . . \n",
    "    other_input_dim = . . .\n",
    "\n",
    "    # feed to RNN model\n",
    "\n",
    "    x = merge([text_embedded, other_input], mode='concat', concat_axis=-1)\n",
    "    x = Reshape((1, vocab_dim*text_length + other_input_dim ))(x)\n",
    "    x = GRU(128)(x)\n",
    "    out = Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # compile the model\n",
    "    \n",
    "    model = Model(input=[encoded_question_input, other_input], output=out)\n",
    "    # choose objective and optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=1e-3, clipnorm=clipnorm))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
