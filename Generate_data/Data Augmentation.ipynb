{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Since many question pairs has same qid involve, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../dataset/raw/train.csv', delimiter=',')\n",
    "df_test = pd.read_csv('../dataset/raw/test.csv', delimiter=',')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â†“ Although I think this might be an issue, but things seems like working very well. So, I won't do this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since we are very probable to use a single question for several times, we should remove validation samples directly at this point\n",
    "# Fro example, Q_a == Q_b and Q_b == Q_c ,\n",
    "# In my method, we'll generate a new data Q_a == Q_c\n",
    "# If we move this Q_a == Q_c sample to validation set,\n",
    "#     it is very weird that our training set already has this kind of information (can be recognized from the Q_a == Q_b == Q_c relation).\n",
    "import random\n",
    "\n",
    "validation_size = 10000\n",
    "\n",
    "def split_train_val(df):\n",
    "    rnd_idxes = np.array([random.randint(0,len(df)-1) for i in range(validation_size)])\n",
    "    \n",
    "    trainin_idxes = np.repeat(True, len(df))\n",
    "    trainin_idxes[rnd_idxes] = False\n",
    "    trainin_idxes = np.where(trainin_idxes==True)\n",
    "    \n",
    "    return df.ix[trainin_idxes], df.ix[rnd_idxes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../dataset/raw/train.csv', delimiter=',')\n",
    "df_train, df_val = split_train_val(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_val, open('../dataset/processed/validation_df.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all duplicated questions\n",
    "\n",
    "We'll construct two data structures:\n",
    "1. A dictionary records { qid : question_text } pair. \n",
    "\n",
    "         The question_text is only splited as a list of words, not yet transformed into encoded form. This gives us chances to do training more flexible.\n",
    "         \n",
    "2. A list of duplicated qid pairs \n",
    "\n",
    "        Recording only qid saves us data loading time (since we'll try to augment and enumerate a huge amount of duplicated question pairs).\n",
    "\n",
    "Note:\n",
    "\n",
    "I didn't record any single non-duplicated question relation. \n",
    "I decided to random assign any question pairs to be non-duplicated question pairs. This might causes some issue that similar to too-much-upsampling case, but this extremely increases the variety of non-duplicated question samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max qid =  537932\n"
     ]
    }
   ],
   "source": [
    "def get_max_qid(df):\n",
    "    max_qid = 0\n",
    "    for idx,frame in df.iterrows():\n",
    "        qid1 = int(frame['qid1'])\n",
    "        qid2 = int(frame['qid2'])\n",
    "        if qid1>max_qid:\n",
    "            max_qid = qid1\n",
    "        elif qid2>max_qid:\n",
    "            max_qid = qid2\n",
    "    print('Max qid = ', max_qid)\n",
    "    return max_qid\n",
    "\n",
    "max_qid = get_max_qid(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming groups of duplicated question pairs.\n",
    "\n",
    "EX: \n",
    "```\n",
    "if A==B and B==C:\n",
    "    group A,B,C as a group, then we can enumerate all combinations, including A==C as a new sample\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_questions(df):\n",
    "    group_id = 0\n",
    "    group_list = np.repeat(-1, max_qid)\n",
    "    \n",
    "    for idx,frame in df.iterrows():\n",
    "        qid1 = int(frame['qid1'])\n",
    "        qid2 = int(frame['qid2'])\n",
    "        \n",
    "        if int(frame['is_duplicate'])==1:\n",
    "            # if both has no group, add new group\n",
    "            if group_list[qid1]==-1 and group_list[qid2]==-1:\n",
    "                group_list[qid1] = group_id\n",
    "                group_list[qid2] = group_id\n",
    "                group_id += 1\n",
    "\n",
    "            # if both has group, join the group \n",
    "            elif group_list[qid1]!=-1 and group_list[qid2]!=-1 :\n",
    "                idxes_to_be_joined = np.where(group_list==group_list[qid2])[0]\n",
    "                group_list[idxes_to_be_joined] = group_list[qid1]\n",
    "\n",
    "            # only q1 has group , than add q2 to q1's group\n",
    "            elif  group_list[qid1]!=-1:\n",
    "                group_list[qid2] = group_list[qid1]\n",
    "\n",
    "            # only q2 has group , than add q1 to q2's group\n",
    "            elif  group_list[qid2]!=-1:\n",
    "                group_list[qid1] = group_list[qid2]\n",
    "                \n",
    "    return group_list\n",
    "    \n",
    "group_ids = group_questions(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147086"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(group_ids!=-1) # means these questions has group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the group and store it as a dictionary\n",
    "group_dict = {}\n",
    "for i in range(np.max(group_ids)+1):\n",
    "    group_members = np.where(group_ids==i)[0]\n",
    "    if len(group_members)>0:\n",
    "        group_dict[i] = group_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def enumerate_all_positive_cases(group_dict):\n",
    "    \n",
    "    def enumerate_inside_group(group):\n",
    "        return list(itertools.combinations(group, 2))\n",
    "    \n",
    "    return np.vstack(enumerate_inside_group(group_dict[group_id]) for group_id in group_dict)\n",
    "\n",
    "def duplicate_all(df):\n",
    "    \n",
    "    def get_qid_set():\n",
    "        ids = set()\n",
    "        for i,series in df.iterrows():\n",
    "            if series['qid1'] not in ids:\n",
    "                ids.add(series['qid1'])\n",
    "            if series['qid2'] not in ids:\n",
    "                ids.add(series['qid2'])\n",
    "        return ids\n",
    "    \n",
    "    id_set = get_qid_set()\n",
    "    return [[i,i] for i in id_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enumerate all cases of duplicated question pairs from each group\n",
    "enumerate_pairs = enumerate_all_positive_cases(group_dict)\n",
    "\n",
    "# The question pairs with itself is also a sample of duplicated question pair\n",
    "duplicate_pairs = duplicate_all(df_train) \n",
    "\n",
    "all_pos_pairs = np.vstack([enumerate_pairs,duplicate_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754062\n"
     ]
    }
   ],
   "source": [
    "print(len(all_pos_pairs)) # The total duplicated samples count 766481 about 5~6 times than original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_pos_pairs, open('../dataset/processed/positive_question_id_pairs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse original training DataFrame to words list and store it\n",
    "\n",
    "Note: \n",
    "\n",
    "Not encoded yet, we need to map rare words to same `<RARE_X>` special token in each question pair. This should be done in training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_map = pickle.load(open('../dataset/processed/enc_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_wrod_list(question):\n",
    "    \n",
    "    if type(question)!=str:\n",
    "        return []\n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "    separator = '(?=[\\s\\'!\"?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]|$)'\n",
    "    single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "    return re.findall(single_word+separator, question)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_qid_question_dict(df):\n",
    "    res = {}\n",
    "    for i,frame in df.iterrows():\n",
    "        \n",
    "        qid1 = int(frame['qid1'])\n",
    "        if qid1 not in res:\n",
    "            res[qid1] = parse_wrod_list(frame['question1'])\n",
    "        \n",
    "        qid2 = int(frame['qid2'])\n",
    "        if qid2 not in res:\n",
    "            res[qid2] = parse_wrod_list(frame['question2'])\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gen_all_pos_df(qid_dict, all_pos_pairs):\n",
    "#     all_series = []\n",
    "#     column_names = ['qid1','qid2','question1','question2', 'is_duplicate']\n",
    "#     for i,pair in enumerate(all_pos_pairs):\n",
    "#         series = pd.Series([ pair[0], pair[1], qid_dict[pair[0]], qid_dict[pair[1]], 1 ], name=i)\n",
    "#         all_series.append(series)\n",
    "#     ret = pd.DataFrame(all_series)\n",
    "#     ret.columns = column_names\n",
    "#     return ret\n",
    "\n",
    "# all_pos_df = gen_all_pos_df(qid_dict, all_pos_pairs)\n",
    "# all_pos_df.head(10)\n",
    "# pickle.dump(all_pos_df, open('../dataset/processed/enumerate_all_positive_training_data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_question_dict = gen_qid_question_dict(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(training_question_dict, open('../dataset/processed/qid_question_wrods_list_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
